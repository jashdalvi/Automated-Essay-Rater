{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9839,"status":"ok","timestamp":1666029409173,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"},"user_tz":-330},"id":"1ujo2PRQEEtA","outputId":"f08ec1c2-0c00-4769-b9d3-96cf9dde385f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n","\u001b[K     |████████████████████████████████| 5.3 MB 15.1 MB/s \n","\u001b[?25hCollecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 52.8 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 66.8 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (5.0.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n","\u001b[K     |████████████████████████████████| 163 kB 94.3 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Installing collected packages: tokenizers, huggingface-hub, transformers, sentencepiece\n","Successfully installed huggingface-hub-0.10.1 sentencepiece-0.1.97 tokenizers-0.13.1 transformers-4.23.1\n"]}],"source":["!pip install transformers sentencepiece"]},{"cell_type":"code","source":["!pip install optuna"],"metadata":{"id":"6r2H4fC3ygKZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666029419560,"user_tz":-330,"elapsed":10394,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}},"outputId":"dfbf7547-5d94-4eb7-a2e6-ab4701fa5c91"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting optuna\n","  Downloading optuna-3.0.3-py3-none-any.whl (348 kB)\n","\u001b[K     |████████████████████████████████| 348 kB 16.1 MB/s \n","\u001b[?25hCollecting importlib-metadata<5.0.0\n","  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.6)\n","Collecting alembic>=1.5.0\n","  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n","\u001b[K     |████████████████████████████████| 209 kB 87.2 MB/s \n","\u001b[?25hCollecting cmaes>=0.8.2\n","  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n","Collecting cliff\n","  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n","\u001b[K     |████████████████████████████████| 81 kB 11.0 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.64.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (6.0)\n","Requirement already satisfied: scipy<1.9.0,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.7.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n","Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.41)\n","Collecting colorlog\n","  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic>=1.5.0->optuna) (5.10.0)\n","Collecting Mako\n","  Downloading Mako-1.2.3-py3-none-any.whl (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 9.0 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5.0.0->optuna) (4.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5.0.0->optuna) (3.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.3.0->optuna) (1.1.3.post0)\n","Collecting autopage>=0.4.0\n","  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n","Collecting cmd2>=1.0.0\n","  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n","\u001b[K     |████████████████████████████████| 147 kB 77.4 MB/s \n","\u001b[?25hCollecting stevedore>=2.0.1\n","  Downloading stevedore-3.5.1-py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 7.6 MB/s \n","\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n","  Downloading pbr-5.10.0-py2.py3-none-any.whl (112 kB)\n","\u001b[K     |████████████████████████████████| 112 kB 65.4 MB/s \n","\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.4.1)\n","Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (22.1.0)\n","Collecting pyperclip>=1.6\n","  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n","Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n","Building wheels for collected packages: pyperclip\n","  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=4dc10a8e0cda6edfde32649cbab2faeeb8f07e125d6abe41f4d56f7395d45453\n","  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n","Successfully built pyperclip\n","Installing collected packages: pyperclip, pbr, importlib-metadata, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n","  Attempting uninstall: importlib-metadata\n","    Found existing installation: importlib-metadata 5.0.0\n","    Uninstalling importlib-metadata-5.0.0:\n","      Successfully uninstalled importlib-metadata-5.0.0\n","Successfully installed Mako-1.2.3 alembic-1.8.1 autopage-0.5.1 cliff-3.10.1 cmaes-0.8.2 cmd2-2.4.2 colorlog-6.7.0 importlib-metadata-4.13.0 optuna-3.0.3 pbr-5.10.0 pyperclip-1.8.2 stevedore-3.5.1\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"xAya0Z97DtCR","executionInfo":{"status":"ok","timestamp":1666029423768,"user_tz":-330,"elapsed":4213,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import os\n","from transformers import AutoModel, AutoConfig, AutoTokenizer, get_cosine_schedule_with_warmup, get_polynomial_decay_schedule_with_warmup, get_linear_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup\n","import pandas as pd\n","from torch.cuda.amp import autocast, GradScaler\n","from sklearn.metrics import mean_squared_error\n","import random\n","import time\n","from torch.utils import checkpoint\n","import math\n","import gc\n","from text_unidecode import unidecode\n","from typing import Dict, List, Tuple\n","import codecs\n","import warnings\n","import torch.nn.functional as F\n","import optuna"]},{"cell_type":"code","source":["#######\n","#### BEST PARAMS FOR THE DEBERTA V3 LARGE MODEL\n","# {'learning_rate': 1.7027701828938127e-05,\n","#  'layer_wise_learning_rate_decay': 0.8780935762069765,\n","#  'learning_rate_schduler': 'polynomial',\n","#  'reinit_layers': 1}\n","#######\n","\n","\n","#####\n","#### BEST PARAMS FOR THE DEBERTA V3 BASE MODEL\n","# {'learning_rate': 0.0002634969863920811, \n","#  'layer_wise_learning_rate_decay': 0.7867664854455205, \n","#  'learning_rate_schduler': 'polynomial', \n","#  'reinit_layers': 3}\n","#####"],"metadata":{"id":"G5YXOkv0HY79","executionInfo":{"status":"ok","timestamp":1666029423768,"user_tz":-330,"elapsed":10,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"1wMfVtAqDtCT","executionInfo":{"status":"ok","timestamp":1666029423769,"user_tz":-330,"elapsed":10,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}}},"outputs":[],"source":["import transformers\n","transformers.logging.set_verbosity_error()"]},{"cell_type":"code","source":["warnings.simplefilter('ignore')"],"metadata":{"id":"PQy468aCeRZj","executionInfo":{"status":"ok","timestamp":1666029423769,"user_tz":-330,"elapsed":9,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":59602,"status":"ok","timestamp":1666029483362,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"},"user_tz":-330},"id":"j2qok14QDua8","outputId":"1256eec9-0638-4161-bbab-27147692a801"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"P6go0UzRDtCT","executionInfo":{"status":"ok","timestamp":1666029483363,"user_tz":-330,"elapsed":7,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}}},"outputs":[],"source":["def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    \n","seed_everything(seed=42)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":4480,"status":"ok","timestamp":1666029487838,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"},"user_tz":-330},"id":"_L1WdNpRDtCT","colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["64ac505a094145be9ebb4aa12c78eec0","b8debb3cad6041a1a72010bf7264553a","5683dbe83d3e4e49b58bb206bb11b805","af1fda680a094b8f885b20816d4bcc18","9e5f91b43fd54f00be684ad903d8098b","7926f47e14ea4703b87c26e4625b3082","fb5d955ec2234146b62daed64b3ae9f8","aa4a21a95e304ddaa0bc521e77073c0e","0884a1b531f9441b98834ce5a151617c","02c9b47a1dff47379e168cd2d03d107c","075967f3361c4900b722d2e472b79674","a7d364461df6495285f5a3dd0981226f","48debd180627450a8b2a050a4417ea0a","8d683771c81d490dbcff639e76d81389","5a6c1d41d81546ef8956fd71187d231c","27894b3649504ed8a459da8e5f3bf4d1","7b2e1c22e9c642cd8137facec516ef68","e4f307b3b86a447987c23b707381c694","168b3154b73a4101a68f97ba6428b5a3","b30d83cddfdf4888a0b45bf81c6e778b","4cfe4385d5e445a1a0a0f8f80a535d87","08876503e49146b4af0176f7f0b7ecbe","496bef9856734852ad77be46151e1ab6","28a18e75e04d429ab558ed5e82bda4a1","9e7adf4908394699b585bcc823fb4499","834d4c9887fa47c7a51611e9bbdcb09e","02cb6671610c4ffa999994fe9347c3e3","02a7d9b5f80a4952b8950c9ca09a497f","989bc3c3658f4714a695a16c82301886","515f1049acb84ee39a964f282360d6f3","6ea7c588729a4fd4aaf57166cedb7f91","f22846a307084a10b3209cfb4a9e7f8e","fbbe197c30504c3fb023c97b948d4edd"]},"outputId":"2bdd5831-a696-4880-e1fb-29aa5ea964c8"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64ac505a094145be9ebb4aa12c78eec0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/580 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7d364461df6495285f5a3dd0981226f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"496bef9856734852ad77be46151e1ab6"}},"metadata":{}}],"source":["class CFG:\n","    train_file = \"/content/drive/MyDrive/Kaggle/FeedbackPrize3/train_folds.csv\"\n","    fold = 0\n","    batch_size = 8\n","    num_workers = 4\n","    target_columns = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n","    hidden_dropout_prob = 0.0\n","    reinit_weights = False\n","    reinit_layers = 1\n","    lr = 2e-5\n","    llrd = 0.9\n","    warmup_ratio = 0.0\n","    use_awp = False\n","    adv_lr = 0.0002\n","    adv_eps = 0.001\n","    model_name = \"microsoft/deberta-v3-large\"\n","    gradient_accumulation_steps = 2\n","    max_grad_norm = 10\n","    print_freq = 20\n","    epochs = 3\n","    specific_max_len = 512\n","    token_dropout = False\n","    token_dropout_prob = 0.1\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    gradient_checkpointing_enable = True\n","    save_dir = \"deberta-v3-large\"\n","    save_model_name = \"deberta-v3-large\""]},{"cell_type":"code","execution_count":10,"metadata":{"id":"f2V2bCiFe0ZJ","executionInfo":{"status":"ok","timestamp":1666029487839,"user_tz":-330,"elapsed":9,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}}},"outputs":[],"source":["#Preprocessing Functions\n","\n","def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n","    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n","\n","\n","def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n","    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n","\n","# Register the encoding and decoding error handlers for `utf-8` and `cp1252`.\n","codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n","codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n","\n","def resolve_encodings_and_normalize(text: str) -> str:\n","    \"\"\"Resolve the encoding problems and normalize the abnormal characters.\"\"\"\n","    text = (\n","        text.encode(\"raw_unicode_escape\")\n","        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n","        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n","        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n","    )\n","    text = unidecode(text)\n","    return text"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"UxvMRHfKDtCU","executionInfo":{"status":"ok","timestamp":1666029487839,"user_tz":-330,"elapsed":8,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}}},"outputs":[],"source":["#Utiliy functions \n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","        \n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n","        \n","def MCRMSE(y_trues, y_preds):\n","    scores = []\n","    idxes = y_trues.shape[1]\n","    for i in range(idxes):\n","        y_true = y_trues[:,i]\n","        y_pred = y_preds[:,i]\n","        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n","        scores.append(score)\n","    mcrmse_score = np.mean(scores)\n","    return mcrmse_score, scores\n","\n","\n","def get_score(y_trues, y_preds):\n","    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n","    return mcrmse_score, scores"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"jQ_1vncGDtCU","executionInfo":{"status":"ok","timestamp":1666029487840,"user_tz":-330,"elapsed":9,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}}},"outputs":[],"source":["class Collate:\n","    def __init__(self, tokenizer, max_length = CFG.specific_max_len):\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        \n","    def __call__(self, batch):\n","        \n","        batch_len = max([len(sample[\"ids\"]) for sample in batch])\n","        \n","        output = dict()\n","        output[\"ids\"] = [sample[\"ids\"] for sample in batch]\n","        output[\"mask\"] = [sample[\"mask\"] for sample in batch]\n","        output[\"targets\"] = [sample[\"targets\"] for sample in batch]\n","        \n","        if self.tokenizer.padding_side == \"right\":\n","            output[\"ids\"] = [s + [self.tokenizer.pad_token_id] * (batch_len - len(s)) for s in output[\"ids\"]]\n","            output[\"mask\"] = [s + [0] * (batch_len - len(s)) for s in output[\"mask\"]]\n","        else:\n","            output[\"ids\"] = [[self.tokenizer.pad_token_id] * (batch_len - len(s)) + s for s in output[\"ids\"]]\n","            output[\"mask\"] = [[0] * (batch_len - len(s)) + s for s in output[\"mask\"]]\n","            \n","            \n","        output[\"ids\"] = torch.tensor(output[\"ids\"], dtype = torch.long)\n","        output[\"mask\"] = torch.tensor(output[\"mask\"], dtype = torch.long)\n","        output[\"targets\"] = torch.tensor(output[\"targets\"], dtype = torch.float32)\n","        \n","        return output"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"JghKVWQIDtCU","executionInfo":{"status":"ok","timestamp":1666029487840,"user_tz":-330,"elapsed":8,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}}},"outputs":[],"source":["class Dataset:\n","    def __init__(self, texts, targets, tokenizer, is_train = True):\n","        self.texts = texts\n","        self.targets = targets\n","        self.tokenizer = tokenizer\n","        self.is_train = is_train\n","        \n","    def __len__(self):\n","        return len(self.texts)\n","    \n","    def __getitem__(self, idx):\n","        \n","        text = self.texts[idx]\n","        targets = self.targets[idx]\n","        \n","        \n","        if CFG.specific_max_len is not None:\n","          encoding = self.tokenizer(text, add_special_tokens = True, max_length = CFG.specific_max_len, padding = False, truncation = 'longest_first')\n","        else:\n","          encoding = self.tokenizer(text, add_special_tokens = True)\n","        \n","        sample = dict()\n","\n","        if CFG.token_dropout and self.is_train:\n","          idxs = np.random.choice(np.arange(1, len(encoding[\"input_ids\"]) - 1), size = int(CFG.token_dropout_prob * len(encoding[\"input_ids\"])), replace = False)\n","          ids = np.array(encoding[\"input_ids\"])\n","          ids[idxs] = self.tokenizer.mask_token_id\n","          encoding[\"input_ids\"] = ids.tolist()\n","          \n","        \n","        sample[\"ids\"] = encoding[\"input_ids\"]\n","        sample[\"mask\"] = encoding[\"attention_mask\"]  \n","        sample[\"targets\"] = targets\n","        \n","        return sample"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"QOtpvTlNDtCV","executionInfo":{"status":"ok","timestamp":1666029487840,"user_tz":-330,"elapsed":8,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}}},"outputs":[],"source":["class RMSELoss(nn.Module):\n","    def __init__(self, reduction='mean', eps=1e-9):\n","        super().__init__()\n","        self.mse = nn.MSELoss(reduction='none')\n","        self.reduction = reduction\n","        self.eps = eps\n","\n","    def forward(self, y_pred, y_true):\n","        loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n","        if self.reduction == 'none':\n","            loss = loss\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","        elif self.reduction == 'mean':\n","            loss = loss.mean()\n","        return loss\n","\n","class SmoothRMSELoss(nn.Module):\n","    def __init__(self, reduction='mean', eps=1e-9):\n","        super().__init__()\n","        self.mse = nn.SmoothL1Loss(reduction='none')\n","        self.reduction = reduction\n","        self.eps = eps\n","\n","    def forward(self, y_pred, y_true):\n","        loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n","        if self.reduction == 'none':\n","            loss = loss\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","        elif self.reduction == 'mean':\n","            loss = loss.mean()\n","        return loss\n","\n","class SmoothRMSEComp(nn.Module):\n","  def __init__(self, reduction='mean', eps=1e-9):\n","        super().__init__()\n","        self.mse = nn.SmoothL1Loss(reduction='none')\n","        self.reduction = reduction\n","        self.eps = eps\n","\n","  def forward(self, y_pred, y_true):\n","    loss = torch.sqrt(self.mse(y_pred, y_true).mean(dim = 0)).mean(dim = 0)\n","    return loss  \n","\n","\n","class MeanPooling(nn.Module):\n","    def __init__(self):\n","        super(MeanPooling, self).__init__()\n","        \n","    def forward(self, last_hidden_state, attention_mask):\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","        sum_mask = input_mask_expanded.sum(1)\n","        sum_mask = torch.clamp(sum_mask, min=1e-9)\n","        mean_embeddings = sum_embeddings / sum_mask\n","        return mean_embeddings\n","\n","class WeightedLayerPooling(nn.Module):\n","    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n","        super(WeightedLayerPooling, self).__init__()\n","        self.layer_start = layer_start\n","        self.num_hidden_layers = num_hidden_layers\n","        self.layer_weights = layer_weights if layer_weights is not None \\\n","            else nn.Parameter(\n","                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n","            )\n","\n","    def forward(self, features):\n","\n","        all_layer_embedding = torch.stack(features)\n","        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n","\n","        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n","        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n","\n","        return weighted_average"]},{"cell_type":"code","source":["#AWP\n","class AWP:\n","    def __init__(\n","        self,\n","        model,\n","        optimizer,\n","        adv_param=\"weight\",\n","        adv_lr=1,\n","        adv_eps=0.2,\n","        start_epoch=0,\n","        adv_step=1,\n","        scaler=None\n","    ):\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.adv_param = adv_param\n","        self.adv_lr = adv_lr\n","        self.adv_eps = adv_eps\n","        self.start_epoch = start_epoch\n","        self.adv_step = adv_step\n","        self.backup = {}\n","        self.backup_eps = {}\n","        self.scaler = scaler\n","\n","    def attack_backward(self, x, y, attention_mask,epoch):\n","        if (self.adv_lr == 0) or (epoch < self.start_epoch):\n","            return None\n","\n","        self._save() \n","        for i in range(self.adv_step):\n","            self._attack_step() \n","            with torch.cuda.amp.autocast():\n","                adv_loss, tr_logits = self.model(ids=x, mask=attention_mask, targets=y)\n","                adv_loss = adv_loss.mean()\n","            self.optimizer.zero_grad()\n","            self.scaler.scale(adv_loss).backward()\n","            \n","        self._restore()\n","\n","    def _attack_step(self):\n","        e = 1e-6\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad and param.grad is not None and self.adv_param in name:\n","                norm1 = torch.norm(param.grad)\n","                norm2 = torch.norm(param.data.detach())\n","                if norm1 != 0 and not torch.isnan(norm1):\n","                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n","                    param.data.add_(r_at)\n","                    param.data = torch.min(\n","                        torch.max(param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n","                    )\n","                # param.data.clamp_(*self.backup_eps[name])\n","\n","    def _save(self):\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad and param.grad is not None and self.adv_param in name:\n","                if name not in self.backup:\n","                    self.backup[name] = param.data.clone()\n","                    grad_eps = self.adv_eps * param.abs().detach()\n","                    self.backup_eps[name] = (\n","                        self.backup[name] - grad_eps,\n","                        self.backup[name] + grad_eps,\n","                    )\n","\n","    def _restore(self,):\n","        for name, param in self.model.named_parameters():\n","            if name in self.backup:\n","                param.data = self.backup[name]\n","        self.backup = {}\n","        self.backup_eps = {}"],"metadata":{"id":"BS58rpxCbEKA","executionInfo":{"status":"ok","timestamp":1666029487841,"user_tz":-330,"elapsed":8,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","execution_count":16,"metadata":{"id":"8lkexNsJDtCV","executionInfo":{"status":"ok","timestamp":1666029488341,"user_tz":-330,"elapsed":507,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}}},"outputs":[],"source":["class Model(nn.Module):\n","    def __init__(self, model_name, params = None):\n","        super(Model, self).__init__()\n","        \n","        self.model_name = model_name\n","\n","        hidden_dropout_prob: float = CFG.hidden_dropout_prob\n","        layer_norm_eps: float = 1e-7\n","\n","        config = AutoConfig.from_pretrained(model_name)\n","\n","        config.update(\n","            {\n","                \"output_hidden_states\": True,\n","                \"hidden_dropout_prob\": hidden_dropout_prob,\n","                \"attention_probs_dropout_prob\" : hidden_dropout_prob,\n","                \"layer_norm_eps\": layer_norm_eps,\n","                \"add_pooling_layer\": False,\n","                \"num_labels\": 6,\n","            }\n","        )\n","        \n","        self.config = config\n","        self.params = params\n","        \n","        self.transformer = AutoModel.from_pretrained(model_name, config=config)\n","        if CFG.gradient_checkpointing_enable:\n","            self.transformer.gradient_checkpointing_enable()\n","        \n","        self.output = nn.Linear(config.hidden_size, 6)\n","        self.loss = nn.SmoothL1Loss(reduction = \"mean\")\n","\n","        if CFG.reinit_weights:\n","          self.init_weights_(params[\"reinit_layers\"])\n","\n","    def init_weights_(self, reinit_layers):\n","      for layer in self.transformer.encoder.layer[-reinit_layers:]:\n","        for module in layer.modules():\n","            if isinstance(module, nn.Linear):\n","                module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","                if module.bias is not None:\n","                    module.bias.data.zero_()\n","            elif isinstance(module, nn.Embedding):\n","                module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","                if module.padding_idx is not None:\n","                    module.weight.data[module.padding_idx].zero_()\n","            elif isinstance(module, nn.LayerNorm):\n","                module.bias.data.zero_()\n","                module.weight.data.fill_(1.0)\n","\n","    def get_grouped_llrd_optimizer_scheduler(self, num_train_steps, params):\n","      no_decay = [\"bias\", \"LayerNorm.weight\"]\n","      top_params = list(self.output.named_parameters())\n","      # initialize lr for task specific layer\n","      optimizer_grouped_parameters = [\n","              {\n","                  \"params\": [p for n, p in top_params if not any(nd in n for nd in no_decay)],\n","                  \"weight_decay\": 0.001,\n","                  \"lr\": params[\"lr\"],\n","              },\n","              {\n","                  \"params\": [p for n, p in top_params if any(nd in n for nd in no_decay)],\n","                  \"weight_decay\": 0.0,\n","                  \"lr\": params[\"lr\"],\n","              },\n","          ]\n","\n","      bottom_params = [(n,p) for n,p in self.named_parameters() if n in [\"transformer.encoder.rel_embeddings.weight\",\"transformer.encoder.LayerNorm.weight\",\"transformer.encoder.LayerNorm.bias\"]]\n","\n","      optimizer_grouped_parameters += [\n","              {\n","                  \"params\": [p for n, p in bottom_params if not any(nd in n for nd in no_decay)],\n","                  \"weight_decay\": 0.001,\n","                  \"lr\": params[\"lr\"],\n","              },\n","              {\n","                  \"params\": [p for n, p in bottom_params if any(nd in n for nd in no_decay)],\n","                  \"weight_decay\": 0.0,\n","                  \"lr\": params[\"lr\"],\n","              },\n","          ]\n","      # initialize lrs for every layer\n","      num_layers = self.config.num_hidden_layers\n","      layers = [self.transformer.embeddings] + list(self.transformer.encoder.layer)\n","      layers.reverse()\n","      lr = params[\"lr\"]\n","      for i,layer in enumerate(layers):\n","        lr *= params[\"llrd\"]\n","\n","        optimizer_grouped_parameters += [\n","            {\n","                \"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.001,\n","                \"lr\": lr,\n","            },\n","            {\n","                \"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","                \"lr\": lr,\n","            },\n","        ]\n","      opt = torch.optim.AdamW(optimizer_grouped_parameters)\n","      if params[\"scheduler\"] == \"polynomial\":\n","        sch = get_polynomial_decay_schedule_with_warmup(\n","            opt,\n","            num_warmup_steps=int(num_train_steps * CFG.warmup_ratio),\n","            num_training_steps=num_train_steps,\n","            last_epoch=-1,\n","        )\n","      elif params[\"scheduler\"] == \"linear\":\n","        sch = get_linear_schedule_with_warmup(\n","            opt,\n","            num_warmup_steps=int(num_train_steps * CFG.warmup_ratio),\n","            num_training_steps=num_train_steps,\n","            last_epoch=-1,\n","        )\n","      else:\n","        sch = get_cosine_schedule_with_warmup(\n","            opt,\n","            num_warmup_steps=int(num_train_steps * CFG.warmup_ratio),\n","            num_training_steps=num_train_steps,\n","            last_epoch=-1,\n","        )\n","\n","      return opt, sch\n","        \n","    def get_optimizer_scheduler(self, num_train_steps):\n","        param_optimizer = list(self.named_parameters())\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","        optimizer_parameters = [\n","            {\n","                \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.001,\n","            },\n","            {\n","                \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","            },\n","        ]\n","        opt = torch.optim.AdamW(optimizer_parameters, lr=CFG.lr)\n","        sch = get_polynomial_decay_schedule_with_warmup(\n","            opt,\n","            num_warmup_steps=int(num_train_steps * CFG.warmup_ratio),\n","            num_training_steps=num_train_steps,\n","            last_epoch=-1,\n","        )\n","        return opt, sch\n","\n","    def forward(self, ids, mask, token_type_ids=None, targets=None):\n","        if token_type_ids is not None:\n","            transformer_out = self.transformer( ids, mask, token_type_ids )\n","        else:\n","            transformer_out = self.transformer(ids, mask)\n","        sequence_output = transformer_out.last_hidden_state[:,0,:]\n","    \n","        logits = self.output(sequence_output)\n","        loss = self.loss(logits, targets)\n","\n","        return loss, logits"]},{"cell_type":"code","source":["class Model2(nn.Module):\n","    def __init__(self, model_name, params = None):\n","        super(Model2, self).__init__()\n","        \n","        self.model_name = model_name\n","\n","        hidden_dropout_prob: float = CFG.hidden_dropout_prob\n","        layer_norm_eps: float = 1e-7\n","\n","        config = AutoConfig.from_pretrained(model_name)\n","\n","        config.update(\n","            {\n","                \"output_hidden_states\": True,\n","                \"hidden_dropout_prob\": hidden_dropout_prob,\n","                \"attention_probs_dropout_prob\" : hidden_dropout_prob,\n","                \"layer_norm_eps\": layer_norm_eps,\n","                \"add_pooling_layer\": False,\n","                \"num_labels\": 6,\n","            }\n","        )\n","        \n","        self.config = config\n","        self.params = params\n","        \n","        self.transformer = AutoModel.from_pretrained(model_name, config=config)\n","        if CFG.gradient_checkpointing_enable:\n","            self.transformer.gradient_checkpointing_enable()\n","        \n","        self.output = nn.Linear(config.hidden_size * 2, 6)\n","        self.drop1 = nn.Dropout(0.1)\n","        self.drop2 = nn.Dropout(0.2)\n","        self.drop3 = nn.Dropout(0.3)\n","        self.loss = nn.SmoothL1Loss(reduction = \"mean\")\n","\n","        if self.params is not None:\n","          self.init_weights_(params[\"reinit_layers\"])\n","\n","    def init_weights_(self, reinit_layers):\n","      for layer in self.transformer.encoder.layer[-reinit_layers:]:\n","        for module in layer.modules():\n","            if isinstance(module, nn.Linear):\n","                module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","                if module.bias is not None:\n","                    module.bias.data.zero_()\n","            elif isinstance(module, nn.Embedding):\n","                module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","                if module.padding_idx is not None:\n","                    module.weight.data[module.padding_idx].zero_()\n","            elif isinstance(module, nn.LayerNorm):\n","                module.bias.data.zero_()\n","                module.weight.data.fill_(1.0)\n","\n","    def get_grouped_llrd_optimizer_scheduler(self, num_train_steps, params):\n","      \n","      \n","      no_decay = [\"bias\", \"LayerNorm.weight\"]\n","      top_params = list(self.output.named_parameters())\n","      # initialize lr for task specific layer\n","      optimizer_grouped_parameters = [\n","              {\n","                  \"params\": [p for n, p in top_params if not any(nd in n for nd in no_decay)],\n","                  \"weight_decay\": 0.001,\n","                  \"lr\": params[\"lr\"],\n","              },\n","              {\n","                  \"params\": [p for n, p in top_params if any(nd in n for nd in no_decay)],\n","                  \"weight_decay\": 0.0,\n","                  \"lr\": params[\"lr\"],\n","              },\n","          ]\n","\n","      bottom_params = [(n,p) for n,p in self.named_parameters() if n in [\"transformer.encoder.rel_embeddings.weight\",\"transformer.encoder.LayerNorm.weight\",\"transformer.encoder.LayerNorm.bias\"]]\n","\n","      optimizer_grouped_parameters += [\n","              {\n","                  \"params\": [p for n, p in bottom_params if not any(nd in n for nd in no_decay)],\n","                  \"weight_decay\": 0.001,\n","                  \"lr\": params[\"lr\"],\n","              },\n","              {\n","                  \"params\": [p for n, p in bottom_params if any(nd in n for nd in no_decay)],\n","                  \"weight_decay\": 0.0,\n","                  \"lr\": params[\"lr\"],\n","              },\n","          ]\n","      # initialize lrs for every layer\n","      num_layers = self.config.num_hidden_layers\n","      layers = [self.transformer.embeddings] + list(self.transformer.encoder.layer)\n","      layers.reverse()\n","      lr = params[\"lr\"]\n","      for i,layer in enumerate(layers):\n","        lr *= params[\"llrd\"]\n","\n","        optimizer_grouped_parameters += [\n","            {\n","                \"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.001,\n","                \"lr\": lr,\n","            },\n","            {\n","                \"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","                \"lr\": lr,\n","            },\n","        ]\n","      opt = torch.optim.AdamW(optimizer_grouped_parameters)\n","      if params[\"scheduler\"] == \"polynomial\":\n","        sch = get_polynomial_decay_schedule_with_warmup(\n","            opt,\n","            num_warmup_steps=int(num_train_steps * CFG.warmup_ratio),\n","            num_training_steps=num_train_steps,\n","            last_epoch=-1,\n","        )\n","      elif params[\"scheduler\"] == \"linear\":\n","        sch = get_linear_schedule_with_warmup(\n","            opt,\n","            num_warmup_steps=int(num_train_steps * CFG.warmup_ratio),\n","            num_training_steps=num_train_steps,\n","            last_epoch=-1,\n","        )\n","      else:\n","        sch = get_cosine_schedule_with_warmup(\n","            opt,\n","            num_warmup_steps=int(num_train_steps * CFG.warmup_ratio),\n","            num_training_steps=num_train_steps,\n","            last_epoch=-1,\n","        )\n","\n","      return opt, sch\n","        \n","    def get_optimizer_scheduler(self, num_train_steps):\n","        param_optimizer = list(self.named_parameters())\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","        optimizer_parameters = [\n","            {\n","                \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.001,\n","            },\n","            {\n","                \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","            },\n","        ]\n","        opt = torch.optim.AdamW(optimizer_parameters, lr=CFG.lr)\n","        sch = get_polynomial_decay_schedule_with_warmup(\n","            opt,\n","            num_warmup_steps=int(num_train_steps * CFG.warmup_ratio),\n","            num_training_steps=num_train_steps,\n","            last_epoch=-1,\n","        )\n","        return opt, sch\n","\n","    def forward(self, ids, mask, token_type_ids=None, targets=None):\n","        if token_type_ids is not None:\n","            transformer_out = self.transformer( ids, mask, token_type_ids )\n","        else:\n","            transformer_out = self.transformer(ids, mask)\n","\n","        last_hidden_state = transformer_out.last_hidden_state\n","        mean_pool_output = torch.mean(last_hidden_state, 1)\n","        max_pool_output = torch.max(last_hidden_state, 1)[0]\n","        sequence_output = torch.cat((mean_pool_output, max_pool_output), 1)\n","\n","\n","        logits1 = self.output(self.drop1(sequence_output))\n","        logits2 = self.output(self.drop2(sequence_output))\n","        logits3 = self.output(self.drop3(sequence_output))\n","\n","        loss = 0\n","        \n","        if targets is not None:\n","          loss += self.loss(logits1, targets)\n","          loss += self.loss(logits2, targets)\n","          loss += self.loss(logits3, targets)\n","\n","          loss /= 3\n","\n","        logits = (logits1 + logits2 + logits3) / 3\n","\n","        return loss, logits"],"metadata":{"id":"ylcx9UvwCh7f","executionInfo":{"status":"ok","timestamp":1666029490545,"user_tz":-330,"elapsed":2,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","execution_count":18,"metadata":{"id":"PojCbT3GDtCW","executionInfo":{"status":"ok","timestamp":1666029491287,"user_tz":-330,"elapsed":1,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}}},"outputs":[],"source":["def train(epoch, model, train_loader, valid_loader, optimizer, scheduler, device, awp, scaler, best_loss, fold):\n","    model.train()\n","    losses = AverageMeter()\n","    start = end = time.time()\n","    val_steps = len(train_loader) // 1\n","    for step, x in enumerate(train_loader):\n","        for k,v in x.items():\n","            x[k] = v.to(device)\n","        \n","        with autocast():    \n","            loss, logits = model(**x)\n","            \n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        \n","        losses.update(loss.item() * CFG.gradient_accumulation_steps , CFG.batch_size)\n","        scaler.scale(loss).backward()\n","        if CFG.use_awp:\n","          awp.attack_backward(x[\"ids\"],x[\"targets\"],x[\"mask\"],epoch) \n","        \n","        \n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.unscale_(optimizer)\n","            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","            if scheduler is not None:\n","                scheduler.step()\n","        end = time.time()\n","        \n","        if ((step + 1) % CFG.print_freq == 0) or (step == (len(train_loader)-1)):\n","            print('Epoch: [{0}][{1}/{2}] '\n","                  'Elapsed {remain:s} '\n","                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                  'Grad: {grad_norm:.4f}  '\n","                  'LR: {lr:.8f}  '\n","                  .format(epoch+1, step + 1, len(train_loader), \n","                          remain=timeSince(start, float(step+1)/len(train_loader)),\n","                          loss=losses,\n","                          grad_norm= grad_norm,\n","                          lr=scheduler.get_lr()[0]))\n","        \n","        if ((step + 1) % val_steps == 0) or ((step + 1) == len(train_loader)):\n","          print(\"\\nVALID LOOP\\n\")\n","          valid_loss = valid(epoch, model, valid_loader, device)\n","          print(f\"\\nThe valid loss for the current epoch is {valid_loss}\\n\")\n","          torch.cuda.empty_cache()\n","          gc.collect()\n","          if valid_loss < best_loss:\n","              best_loss = valid_loss\n","              if CFG.save_dir is not None:\n","                if not os.path.exists(CFG.save_dir):\n","                  os.mkdir(CFG.save_dir)\n","                save_path = os.path.join(CFG.save_dir, f\"{CFG.save_model_name}_fold_{fold}.pth\")\n","              else:\n","                save_path = f\"{CFG.save_model_name}_fold_{fold}.pth\"\n","              torch.save(model.state_dict(), save_path)\n","        model.train()\n","    return best_loss\n","\n","def valid(epoch, model, valid_loader, device):\n","    model.eval()\n","    all_targets = []\n","    all_outputs = []\n","    losses = AverageMeter()\n","    with torch.no_grad():\n","        for step, x in enumerate(valid_loader):\n","\n","            for k, v in x.items():\n","                x[k] = v.to(device)\n","            with autocast():\n","                loss, logits = model(**x)\n","\n","            \n","            losses.update(loss.item(), CFG.batch_size)\n","            targets = x[\"targets\"].cpu().numpy()\n","            outputs = logits.cpu().numpy()\n","\n","            all_targets.append(targets)\n","            all_outputs.append(np.clip(outputs, 1.0, 5.0))\n","\n","            if ((step + 1) % CFG.print_freq == 0) or (step == (len(valid_loader)-1)):\n","                print('Epoch: [{0}][{1}/{2}] '\n","                      'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                      .format(epoch+1, step + 1, len(valid_loader), loss=losses))\n","        \n","    \n","    all_targets = np.vstack(all_targets)\n","    all_outputs = np.vstack(all_outputs)\n","    loss = get_score(all_targets, all_outputs)[0]\n","    \n","    del all_targets, all_outputs;\n","    return loss"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"zovUkUouDtCW","executionInfo":{"status":"ok","timestamp":1666029494201,"user_tz":-330,"elapsed":5,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}}},"outputs":[],"source":["def main(fold, params):\n","    torch.cuda.empty_cache()\n","    df = pd.read_csv(CFG.train_file)\n","    \n","    train_df = df.loc[df.kfold != fold]\n","    valid_df = df.loc[df.kfold == fold]\n","    \n","    train_texts = train_df[\"full_text\"].apply(resolve_encodings_and_normalize).to_list()\n","    valid_texts = valid_df[\"full_text\"].apply(resolve_encodings_and_normalize).to_list()\n","    \n","    train_targets = train_df[CFG.target_columns].values.tolist()\n","    \n","    valid_targets = valid_df[CFG.target_columns].values.tolist()\n","    \n","    train_ds = Dataset(train_texts, train_targets, CFG.tokenizer)\n","    valid_ds = Dataset(valid_texts, valid_targets, CFG.tokenizer, is_train = False)\n","\n","    collate_fn = Collate(CFG.tokenizer)\n","    train_loader = torch.utils.data.DataLoader(train_ds, batch_size = CFG.batch_size, shuffle = True, collate_fn = collate_fn, num_workers = CFG.num_workers)\n","    valid_loader = torch.utils.data.DataLoader(valid_ds, batch_size = CFG.batch_size, shuffle = False, collate_fn = collate_fn, num_workers = CFG.num_workers)\n","    \n","    model = Model(CFG.model_name, params)\n","\n","    ## Adding noisy tune -> NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better\n","    # noise_lambda = 0.15\n","    # sd = model.state_dict()\n","    # for name, param in model.named_parameters():\n","    #   sd[name][:] += (torch.rand(param.size()) - 0.5) * noise_lambda * torch.std(param)\n","\n","    \n","    # model.load_state_dict(sd)\n","\n","    \n","    num_train_steps = int(len(train_ds) / CFG.batch_size / CFG.gradient_accumulation_steps * CFG.epochs)\n","    optimizer, scheduler = model.get_grouped_llrd_optimizer_scheduler(num_train_steps, params)\n","    \n","    model = model.to(CFG.device)\n","    best_loss = np.inf\n","    scaler = GradScaler()\n","    if CFG.use_awp:\n","      print('Enable AWP')\n","      awp = AWP(model,\n","          optimizer,\n","          adv_lr=CFG.adv_lr,\n","          adv_eps=CFG.adv_eps,\n","          start_epoch=2,\n","          scaler=scaler)\n","    else:\n","      awp = None\n","    for epoch in range(CFG.epochs):\n","        print(\"\\nTRAIN LOOP\\n\")\n","        best_loss = train(epoch, model, train_loader, valid_loader, optimizer, scheduler, CFG.device, awp, scaler, best_loss, fold)\n","        # print(\"\\nVALID LOOP\\n\")\n","        # valid_loss = valid(epoch, model, valid_loader, CFG.device)\n","        \n","        # print(f\"\\nThe valid loss for the current epoch is {valid_loss}\\n\")\n","        # torch.cuda.empty_cache()\n","        # gc.collect()\n","        # if valid_loss < best_loss:\n","        #     best_loss = valid_loss\n","        #     if CFG.save_dir is not None:\n","        #       if not os.path.exists(CFG.save_dir):\n","        #         os.mkdir(CFG.save_dir)\n","        #       save_path = os.path.join(CFG.save_dir, f\"{CFG.save_model_name}_fold_{fold}.pth\")\n","        #     else:\n","        #       save_path = f\"{CFG.save_model_name}_fold_{fold}.pth\"\n","        #     torch.save(model.state_dict(), save_path)\n","            \n","    del model, optimizer, scheduler, train_loader, valid_loader, train_df, valid_df;\n","    gc.collect()\n","\n","    return best_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["5d238877715d4e639711071b267d8eda","83187bb5894c4b51b1f4e7a17021d2ac","446e382e36f248d49244151c4708a5be","287b1ed5cb9f4baa8a7f84ddba8c6933","d8e3ff80d4534a289a7f3e30b023bff1","8cbe137e722c4134ab58da91c6abf901","128f6533639e406cb0d8c678e8d2cee5","57d23e066ffe45368a7d34842d903511","97744e52494e44b0992268782a7659d2","40021c5428884ccb9b4e2382b29713b3","6b4312bb28e94c4cbdf4f35ac01e69a8"]},"id":"ndUmqkakDtCW","outputId":"8ecdfaf0-e1f1-49e0-f3a7-10dabbd754cf"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","RUNNING FOLD 0\n","\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5d238877715d4e639711071b267d8eda","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/874M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"output_type":"stream","name":"stdout","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/392] Elapsed 0m 36s (remain 11m 21s) Loss: 0.4230(1.3046) Grad: 2.6483  LR: 0.00004915  \n","Epoch: [1][40/392] Elapsed 1m 10s (remain 10m 22s) Loss: 0.2401(0.7478) Grad: 8.4716  LR: 0.00004829  \n","Epoch: [1][60/392] Elapsed 1m 45s (remain 9m 45s) Loss: 0.1414(0.5591) Grad: 3.5440  LR: 0.00004744  \n","Epoch: [1][80/392] Elapsed 2m 21s (remain 9m 13s) Loss: 0.0770(0.4489) Grad: 1.5898  LR: 0.00004659  \n","Epoch: [1][100/392] Elapsed 2m 57s (remain 8m 38s) Loss: 0.1291(0.3855) Grad: 6.1771  LR: 0.00004573  \n","Epoch: [1][120/392] Elapsed 3m 33s (remain 8m 2s) Loss: 0.0924(0.3399) Grad: 3.5196  LR: 0.00004488  \n","Epoch: [1][140/392] Elapsed 4m 9s (remain 7m 28s) Loss: 0.1289(0.3123) Grad: 4.9873  LR: 0.00004403  \n","Epoch: [1][160/392] Elapsed 4m 44s (remain 6m 53s) Loss: 0.1364(0.2887) Grad: 1.3655  LR: 0.00004317  \n","Epoch: [1][180/392] Elapsed 5m 20s (remain 6m 17s) Loss: 0.0897(0.2695) Grad: 1.4337  LR: 0.00004232  \n","Epoch: [1][200/392] Elapsed 5m 56s (remain 5m 41s) Loss: 0.1186(0.2540) Grad: 1.3590  LR: 0.00004147  \n","Epoch: [1][220/392] Elapsed 6m 31s (remain 5m 6s) Loss: 0.1177(0.2420) Grad: 2.4430  LR: 0.00004062  \n","Epoch: [1][240/392] Elapsed 7m 7s (remain 4m 30s) Loss: 0.0674(0.2316) Grad: 2.3965  LR: 0.00003976  \n","Epoch: [1][260/392] Elapsed 7m 43s (remain 3m 55s) Loss: 0.0992(0.2220) Grad: 3.0277  LR: 0.00003891  \n","Epoch: [1][280/392] Elapsed 8m 18s (remain 3m 19s) Loss: 0.0838(0.2152) Grad: 1.9543  LR: 0.00003806  \n","Epoch: [1][300/392] Elapsed 8m 54s (remain 2m 43s) Loss: 0.1397(0.2091) Grad: 2.6999  LR: 0.00003720  \n","Epoch: [1][320/392] Elapsed 9m 30s (remain 2m 8s) Loss: 0.1138(0.2035) Grad: 4.1779  LR: 0.00003635  \n","Epoch: [1][340/392] Elapsed 10m 5s (remain 1m 32s) Loss: 0.1260(0.1976) Grad: 1.7884  LR: 0.00003550  \n","Epoch: [1][360/392] Elapsed 10m 41s (remain 0m 57s) Loss: 0.0981(0.1924) Grad: 1.5215  LR: 0.00003464  \n","Epoch: [1][380/392] Elapsed 11m 17s (remain 0m 21s) Loss: 0.1412(0.1885) Grad: 3.2013  LR: 0.00003379  \n","Epoch: [1][392/392] Elapsed 11m 36s (remain 0m 0s) Loss: 0.0858(0.1865) Grad: 3.5974  LR: 0.00003328  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/98] Loss: 0.0828(0.1098) \n","Epoch: [1][40/98] Loss: 0.1149(0.1097) \n","Epoch: [1][60/98] Loss: 0.0525(0.1078) \n","Epoch: [1][80/98] Loss: 0.1167(0.1093) \n","Epoch: [1][98/98] Loss: 0.1112(0.1098) \n","\n","The valid loss for the current epoch is 0.46942976117134094\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/392] Elapsed 0m 36s (remain 11m 10s) Loss: 0.0755(0.1010) Grad: 1.6280  LR: 0.00003242  \n","Epoch: [2][40/392] Elapsed 1m 11s (remain 10m 30s) Loss: 0.0703(0.1006) Grad: 2.9258  LR: 0.00003157  \n","Epoch: [2][60/392] Elapsed 1m 47s (remain 9m 56s) Loss: 0.0621(0.0989) Grad: 1.4675  LR: 0.00003072  \n","Epoch: [2][80/392] Elapsed 2m 23s (remain 9m 19s) Loss: 0.1528(0.1005) Grad: 2.8612  LR: 0.00002987  \n","Epoch: [2][100/392] Elapsed 2m 59s (remain 8m 43s) Loss: 0.0918(0.0993) Grad: 1.8942  LR: 0.00002901  \n","Epoch: [2][120/392] Elapsed 3m 34s (remain 8m 5s) Loss: 0.0780(0.0989) Grad: 1.3115  LR: 0.00002816  \n","Epoch: [2][140/392] Elapsed 4m 9s (remain 7m 29s) Loss: 0.0870(0.0997) Grad: 1.6225  LR: 0.00002731  \n","Epoch: [2][160/392] Elapsed 4m 45s (remain 6m 54s) Loss: 0.0643(0.0984) Grad: 1.2813  LR: 0.00002645  \n","Epoch: [2][180/392] Elapsed 5m 21s (remain 6m 18s) Loss: 0.0994(0.0985) Grad: 1.3211  LR: 0.00002560  \n","Epoch: [2][200/392] Elapsed 5m 56s (remain 5m 42s) Loss: 0.0736(0.0996) Grad: 2.3781  LR: 0.00002475  \n","Epoch: [2][220/392] Elapsed 6m 33s (remain 5m 7s) Loss: 0.1300(0.0999) Grad: 3.5492  LR: 0.00002389  \n","Epoch: [2][240/392] Elapsed 7m 8s (remain 4m 31s) Loss: 0.0726(0.0992) Grad: 1.4971  LR: 0.00002304  \n","Epoch: [2][260/392] Elapsed 7m 44s (remain 3m 55s) Loss: 0.0754(0.0986) Grad: 1.3153  LR: 0.00002219  \n","Epoch: [2][280/392] Elapsed 8m 19s (remain 3m 19s) Loss: 0.0620(0.0988) Grad: 2.6187  LR: 0.00002133  \n","Epoch: [2][300/392] Elapsed 8m 55s (remain 2m 44s) Loss: 0.0669(0.0980) Grad: 1.5040  LR: 0.00002048  \n","Epoch: [2][320/392] Elapsed 9m 31s (remain 2m 8s) Loss: 0.0611(0.0978) Grad: 1.4178  LR: 0.00001963  \n","Epoch: [2][340/392] Elapsed 10m 6s (remain 1m 32s) Loss: 0.0656(0.0973) Grad: 3.0230  LR: 0.00001877  \n","Epoch: [2][360/392] Elapsed 10m 42s (remain 0m 57s) Loss: 0.0859(0.0975) Grad: 2.3472  LR: 0.00001792  \n","Epoch: [2][380/392] Elapsed 11m 18s (remain 0m 21s) Loss: 0.0991(0.0973) Grad: 1.9243  LR: 0.00001707  \n","Epoch: [2][392/392] Elapsed 11m 38s (remain 0m 0s) Loss: 0.0319(0.0971) Grad: 2.1035  LR: 0.00001656  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/98] Loss: 0.0734(0.1004) \n","Epoch: [2][40/98] Loss: 0.0811(0.0988) \n","Epoch: [2][60/98] Loss: 0.0554(0.0986) \n","Epoch: [2][80/98] Loss: 0.1403(0.1010) \n","Epoch: [2][98/98] Loss: 0.0974(0.1013) \n","\n","The valid loss for the current epoch is 0.45063579082489014\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/392] Elapsed 0m 35s (remain 11m 8s) Loss: 0.0687(0.0806) Grad: 1.2036  LR: 0.00001570  \n","Epoch: [3][40/392] Elapsed 1m 11s (remain 10m 29s) Loss: 0.0921(0.0824) Grad: 1.2076  LR: 0.00001485  \n","Epoch: [3][60/392] Elapsed 1m 47s (remain 9m 53s) Loss: 0.0894(0.0797) Grad: 1.9151  LR: 0.00001400  \n","Epoch: [3][80/392] Elapsed 2m 23s (remain 9m 19s) Loss: 0.0683(0.0810) Grad: 1.8112  LR: 0.00001314  \n","Epoch: [3][100/392] Elapsed 2m 58s (remain 8m 41s) Loss: 0.0684(0.0816) Grad: 1.2828  LR: 0.00001229  \n","Epoch: [3][120/392] Elapsed 3m 34s (remain 8m 5s) Loss: 0.0934(0.0801) Grad: 1.9232  LR: 0.00001144  \n","Epoch: [3][140/392] Elapsed 4m 9s (remain 7m 29s) Loss: 0.0925(0.0803) Grad: 1.8397  LR: 0.00001058  \n","Epoch: [3][160/392] Elapsed 4m 45s (remain 6m 54s) Loss: 0.0946(0.0801) Grad: 1.6056  LR: 0.00000973  \n","Epoch: [3][180/392] Elapsed 5m 20s (remain 6m 18s) Loss: 0.0443(0.0795) Grad: 1.0283  LR: 0.00000888  \n","Epoch: [3][200/392] Elapsed 5m 56s (remain 5m 42s) Loss: 0.1021(0.0795) Grad: 2.9321  LR: 0.00000802  \n","Epoch: [3][220/392] Elapsed 6m 32s (remain 5m 6s) Loss: 0.1290(0.0795) Grad: 2.3937  LR: 0.00000717  \n","Epoch: [3][240/392] Elapsed 7m 7s (remain 4m 30s) Loss: 0.0812(0.0792) Grad: 1.3654  LR: 0.00000632  \n","Epoch: [3][260/392] Elapsed 7m 43s (remain 3m 55s) Loss: 0.0927(0.0792) Grad: 2.1328  LR: 0.00000547  \n","Epoch: [3][280/392] Elapsed 8m 18s (remain 3m 19s) Loss: 0.0777(0.0790) Grad: 1.5895  LR: 0.00000461  \n","Epoch: [3][300/392] Elapsed 8m 54s (remain 2m 43s) Loss: 0.0765(0.0793) Grad: 0.9356  LR: 0.00000376  \n","Epoch: [3][320/392] Elapsed 9m 29s (remain 2m 8s) Loss: 0.1154(0.0793) Grad: 2.5427  LR: 0.00000291  \n","Epoch: [3][340/392] Elapsed 10m 5s (remain 1m 32s) Loss: 0.0712(0.0790) Grad: 1.6981  LR: 0.00000205  \n","Epoch: [3][360/392] Elapsed 10m 41s (remain 0m 56s) Loss: 0.0786(0.0793) Grad: 2.1444  LR: 0.00000120  \n","Epoch: [3][380/392] Elapsed 11m 16s (remain 0m 21s) Loss: 0.0782(0.0788) Grad: 2.6730  LR: 0.00000035  \n","Epoch: [3][392/392] Elapsed 11m 36s (remain 0m 0s) Loss: 0.0370(0.0786) Grad: 1.7681  LR: 0.00000001  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/98] Loss: 0.0793(0.0993) \n","Epoch: [3][40/98] Loss: 0.0893(0.0975) \n","Epoch: [3][60/98] Loss: 0.0563(0.0977) \n","Epoch: [3][80/98] Loss: 0.1430(0.1001) \n","Epoch: [3][98/98] Loss: 0.0932(0.1005) \n","\n","The valid loss for the current epoch is 0.4488475024700165\n","\n","######\n","The best loss for fold 0 is 0.4488475024700165\n","######\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","RUNNING FOLD 1\n","\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","TRAIN LOOP\n","\n","Epoch: [1][20/391] Elapsed 0m 36s (remain 11m 9s) Loss: 0.1478(1.1803) Grad: 9.1442  LR: 0.00004915  \n","Epoch: [1][40/391] Elapsed 1m 12s (remain 10m 34s) Loss: 0.2075(0.6944) Grad: 3.3051  LR: 0.00004829  \n","Epoch: [1][60/391] Elapsed 1m 47s (remain 9m 55s) Loss: 0.1394(0.5339) Grad: 2.7766  LR: 0.00004744  \n","Epoch: [1][80/391] Elapsed 2m 23s (remain 9m 18s) Loss: 0.1325(0.4343) Grad: 2.6547  LR: 0.00004659  \n","Epoch: [1][100/391] Elapsed 2m 58s (remain 8m 40s) Loss: 0.3165(0.3748) Grad: 8.4911  LR: 0.00004573  \n","Epoch: [1][120/391] Elapsed 3m 34s (remain 8m 4s) Loss: 0.1295(0.3403) Grad: 4.4224  LR: 0.00004488  \n","Epoch: [1][140/391] Elapsed 4m 10s (remain 7m 28s) Loss: 0.1057(0.3124) Grad: 1.8753  LR: 0.00004403  \n","Epoch: [1][160/391] Elapsed 4m 45s (remain 6m 52s) Loss: 0.0765(0.2881) Grad: 1.9436  LR: 0.00004317  \n","Epoch: [1][180/391] Elapsed 5m 21s (remain 6m 17s) Loss: 0.1643(0.2710) Grad: 0.9747  LR: 0.00004232  \n","Epoch: [1][200/391] Elapsed 5m 57s (remain 5m 41s) Loss: 0.0911(0.2561) Grad: 1.7179  LR: 0.00004147  \n","Epoch: [1][220/391] Elapsed 6m 33s (remain 5m 5s) Loss: 0.1053(0.2423) Grad: 3.3035  LR: 0.00004062  \n","Epoch: [1][240/391] Elapsed 7m 9s (remain 4m 30s) Loss: 0.0489(0.2325) Grad: 1.4533  LR: 0.00003976  \n","Epoch: [1][260/391] Elapsed 7m 44s (remain 3m 54s) Loss: 0.0773(0.2237) Grad: 3.4430  LR: 0.00003891  \n","Epoch: [1][280/391] Elapsed 8m 20s (remain 3m 18s) Loss: 0.1207(0.2157) Grad: 2.3781  LR: 0.00003806  \n","Epoch: [1][300/391] Elapsed 8m 56s (remain 2m 42s) Loss: 0.1205(0.2087) Grad: 2.9069  LR: 0.00003720  \n","Epoch: [1][320/391] Elapsed 9m 32s (remain 2m 7s) Loss: 0.1122(0.2019) Grad: 2.3559  LR: 0.00003635  \n","Epoch: [1][340/391] Elapsed 10m 8s (remain 1m 31s) Loss: 0.0788(0.1958) Grad: 1.9793  LR: 0.00003550  \n","Epoch: [1][360/391] Elapsed 10m 43s (remain 0m 55s) Loss: 0.0974(0.1915) Grad: 1.5152  LR: 0.00003464  \n","Epoch: [1][380/391] Elapsed 11m 19s (remain 0m 19s) Loss: 0.1052(0.1876) Grad: 1.1355  LR: 0.00003379  \n","Epoch: [1][391/391] Elapsed 11m 39s (remain 0m 0s) Loss: 0.1012(0.1857) Grad: 3.0350  LR: 0.00003336  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/98] Loss: 0.0722(0.1082) \n","Epoch: [1][40/98] Loss: 0.1138(0.1115) \n","Epoch: [1][60/98] Loss: 0.0946(0.1099) \n","Epoch: [1][80/98] Loss: 0.0654(0.1099) \n","Epoch: [1][98/98] Loss: 0.1176(0.1101) \n","\n","The valid loss for the current epoch is 0.4703044891357422\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/391] Elapsed 0m 36s (remain 11m 7s) Loss: 0.1102(0.0921) Grad: 1.1496  LR: 0.00003251  \n","Epoch: [2][40/391] Elapsed 1m 11s (remain 10m 28s) Loss: 0.0815(0.0977) Grad: 1.9647  LR: 0.00003166  \n","Epoch: [2][60/391] Elapsed 1m 47s (remain 9m 51s) Loss: 0.0685(0.0992) Grad: 1.3789  LR: 0.00003080  \n","Epoch: [2][80/391] Elapsed 2m 23s (remain 9m 17s) Loss: 0.0764(0.0973) Grad: 1.6048  LR: 0.00002995  \n","Epoch: [2][100/391] Elapsed 2m 59s (remain 8m 41s) Loss: 0.0624(0.0971) Grad: 1.5813  LR: 0.00002910  \n","Epoch: [2][120/391] Elapsed 3m 34s (remain 8m 4s) Loss: 0.1232(0.0971) Grad: 1.9427  LR: 0.00002824  \n","Epoch: [2][140/391] Elapsed 4m 10s (remain 7m 28s) Loss: 0.0791(0.0969) Grad: 3.9120  LR: 0.00002739  \n","Epoch: [2][160/391] Elapsed 4m 45s (remain 6m 52s) Loss: 0.0802(0.0966) Grad: 0.9585  LR: 0.00002654  \n","Epoch: [2][180/391] Elapsed 5m 22s (remain 6m 17s) Loss: 0.0881(0.0970) Grad: 1.8100  LR: 0.00002569  \n","Epoch: [2][200/391] Elapsed 5m 58s (remain 5m 42s) Loss: 0.0603(0.0965) Grad: 1.0741  LR: 0.00002483  \n","Epoch: [2][220/391] Elapsed 6m 34s (remain 5m 6s) Loss: 0.0913(0.0969) Grad: 2.5607  LR: 0.00002398  \n","Epoch: [2][240/391] Elapsed 7m 9s (remain 4m 30s) Loss: 0.0552(0.0972) Grad: 1.4076  LR: 0.00002313  \n","Epoch: [2][260/391] Elapsed 7m 45s (remain 3m 54s) Loss: 0.0844(0.0965) Grad: 2.2450  LR: 0.00002227  \n","Epoch: [2][280/391] Elapsed 8m 20s (remain 3m 18s) Loss: 0.0783(0.0963) Grad: 0.9730  LR: 0.00002142  \n","Epoch: [2][300/391] Elapsed 8m 57s (remain 2m 42s) Loss: 0.1198(0.0957) Grad: 2.3629  LR: 0.00002057  \n","Epoch: [2][320/391] Elapsed 9m 32s (remain 2m 7s) Loss: 0.0801(0.0953) Grad: 1.3297  LR: 0.00001971  \n","Epoch: [2][340/391] Elapsed 10m 8s (remain 1m 31s) Loss: 0.0916(0.0949) Grad: 1.6333  LR: 0.00001886  \n","Epoch: [2][360/391] Elapsed 10m 44s (remain 0m 55s) Loss: 0.0997(0.0952) Grad: 1.5060  LR: 0.00001801  \n","Epoch: [2][380/391] Elapsed 11m 20s (remain 0m 19s) Loss: 0.1061(0.0951) Grad: 1.6592  LR: 0.00001715  \n","Epoch: [2][391/391] Elapsed 11m 39s (remain 0m 0s) Loss: 0.0961(0.0951) Grad: 2.8638  LR: 0.00001673  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/98] Loss: 0.0758(0.1024) \n","Epoch: [2][40/98] Loss: 0.0965(0.1024) \n","Epoch: [2][60/98] Loss: 0.0868(0.1032) \n","Epoch: [2][80/98] Loss: 0.0879(0.1035) \n","Epoch: [2][98/98] Loss: 0.1171(0.1048) \n","\n","The valid loss for the current epoch is 0.458465576171875\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/391] Elapsed 0m 36s (remain 11m 8s) Loss: 0.0887(0.0746) Grad: 2.1579  LR: 0.00001587  \n","Epoch: [3][40/391] Elapsed 1m 11s (remain 10m 26s) Loss: 0.0519(0.0754) Grad: 1.3938  LR: 0.00001502  \n","Epoch: [3][60/391] Elapsed 1m 46s (remain 9m 50s) Loss: 0.0846(0.0758) Grad: 2.3870  LR: 0.00001417  \n","Epoch: [3][80/391] Elapsed 2m 22s (remain 9m 14s) Loss: 0.0838(0.0757) Grad: 2.4773  LR: 0.00001331  \n","Epoch: [3][100/391] Elapsed 2m 58s (remain 8m 38s) Loss: 0.0987(0.0764) Grad: 1.3200  LR: 0.00001246  \n","Epoch: [3][120/391] Elapsed 3m 33s (remain 8m 3s) Loss: 0.0667(0.0758) Grad: 1.9006  LR: 0.00001161  \n","Epoch: [3][140/391] Elapsed 4m 9s (remain 7m 27s) Loss: 0.0881(0.0756) Grad: 2.2050  LR: 0.00001075  \n","Epoch: [3][160/391] Elapsed 4m 45s (remain 6m 51s) Loss: 0.1027(0.0752) Grad: 1.6926  LR: 0.00000990  \n","Epoch: [3][180/391] Elapsed 5m 21s (remain 6m 16s) Loss: 0.0384(0.0741) Grad: 0.6651  LR: 0.00000905  \n","Epoch: [3][200/391] Elapsed 5m 56s (remain 5m 40s) Loss: 0.0711(0.0740) Grad: 2.1388  LR: 0.00000820  \n","Epoch: [3][220/391] Elapsed 6m 32s (remain 5m 4s) Loss: 0.0795(0.0737) Grad: 1.6548  LR: 0.00000734  \n","Epoch: [3][240/391] Elapsed 7m 7s (remain 4m 29s) Loss: 0.0580(0.0737) Grad: 0.8989  LR: 0.00000649  \n","Epoch: [3][260/391] Elapsed 7m 43s (remain 3m 53s) Loss: 0.0926(0.0740) Grad: 2.3539  LR: 0.00000564  \n","Epoch: [3][280/391] Elapsed 8m 19s (remain 3m 17s) Loss: 0.0802(0.0743) Grad: 1.0606  LR: 0.00000478  \n","Epoch: [3][300/391] Elapsed 8m 54s (remain 2m 42s) Loss: 0.0890(0.0738) Grad: 1.4619  LR: 0.00000393  \n","Epoch: [3][320/391] Elapsed 9m 30s (remain 2m 6s) Loss: 0.0398(0.0736) Grad: 0.8769  LR: 0.00000308  \n","Epoch: [3][340/391] Elapsed 10m 5s (remain 1m 30s) Loss: 0.0706(0.0735) Grad: 1.6290  LR: 0.00000222  \n","Epoch: [3][360/391] Elapsed 10m 41s (remain 0m 55s) Loss: 0.0756(0.0732) Grad: 0.9692  LR: 0.00000137  \n","Epoch: [3][380/391] Elapsed 11m 16s (remain 0m 19s) Loss: 0.0809(0.0732) Grad: 0.9956  LR: 0.00000052  \n","Epoch: [3][391/391] Elapsed 11m 36s (remain 0m 0s) Loss: 0.0540(0.0732) Grad: 1.8156  LR: 0.00000009  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/98] Loss: 0.0794(0.1037) \n","Epoch: [3][40/98] Loss: 0.0954(0.1034) \n","Epoch: [3][60/98] Loss: 0.0845(0.1046) \n","Epoch: [3][80/98] Loss: 0.0966(0.1056) \n","Epoch: [3][98/98] Loss: 0.1093(0.1066) \n","\n","The valid loss for the current epoch is 0.4624464809894562\n","\n","######\n","The best loss for fold 1 is 0.458465576171875\n","######\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","RUNNING FOLD 2\n","\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","TRAIN LOOP\n","\n","Epoch: [1][20/392] Elapsed 0m 35s (remain 11m 6s) Loss: 0.2319(1.6840) Grad: 4.9597  LR: 0.00004915  \n","Epoch: [1][40/392] Elapsed 1m 11s (remain 10m 28s) Loss: 0.0792(0.9242) Grad: 4.6766  LR: 0.00004829  \n","Epoch: [1][60/392] Elapsed 1m 47s (remain 9m 53s) Loss: 0.1990(0.6945) Grad: 5.4045  LR: 0.00004744  \n","Epoch: [1][80/392] Elapsed 2m 22s (remain 9m 17s) Loss: 0.0871(0.5514) Grad: 1.2798  LR: 0.00004659  \n","Epoch: [1][100/392] Elapsed 2m 58s (remain 8m 40s) Loss: 0.1478(0.4652) Grad: 2.0915  LR: 0.00004573  \n","Epoch: [1][120/392] Elapsed 3m 34s (remain 8m 5s) Loss: 0.1154(0.4086) Grad: 1.5047  LR: 0.00004488  \n","Epoch: [1][140/392] Elapsed 4m 9s (remain 7m 29s) Loss: 0.0970(0.3679) Grad: 3.8922  LR: 0.00004403  \n","Epoch: [1][160/392] Elapsed 4m 45s (remain 6m 53s) Loss: 0.0867(0.3374) Grad: 3.1206  LR: 0.00004317  \n","Epoch: [1][180/392] Elapsed 5m 20s (remain 6m 17s) Loss: 0.1430(0.3120) Grad: 1.9883  LR: 0.00004232  \n","Epoch: [1][200/392] Elapsed 5m 56s (remain 5m 42s) Loss: 0.1364(0.2925) Grad: 2.8042  LR: 0.00004147  \n","Epoch: [1][220/392] Elapsed 6m 31s (remain 5m 6s) Loss: 0.0943(0.2763) Grad: 1.3878  LR: 0.00004062  \n","Epoch: [1][240/392] Elapsed 7m 7s (remain 4m 30s) Loss: 0.1278(0.2633) Grad: 2.1182  LR: 0.00003976  \n","Epoch: [1][260/392] Elapsed 7m 43s (remain 3m 55s) Loss: 0.0817(0.2517) Grad: 1.9373  LR: 0.00003891  \n","Epoch: [1][280/392] Elapsed 8m 18s (remain 3m 19s) Loss: 0.1769(0.2423) Grad: 2.3718  LR: 0.00003806  \n","Epoch: [1][300/392] Elapsed 8m 54s (remain 2m 43s) Loss: 0.1041(0.2340) Grad: 2.9586  LR: 0.00003720  \n","Epoch: [1][320/392] Elapsed 9m 29s (remain 2m 8s) Loss: 0.0730(0.2263) Grad: 2.4004  LR: 0.00003635  \n","Epoch: [1][340/392] Elapsed 10m 5s (remain 1m 32s) Loss: 0.0641(0.2190) Grad: 2.0477  LR: 0.00003550  \n","Epoch: [1][360/392] Elapsed 10m 40s (remain 0m 56s) Loss: 0.1238(0.2127) Grad: 3.2344  LR: 0.00003464  \n","Epoch: [1][380/392] Elapsed 11m 16s (remain 0m 21s) Loss: 0.0987(0.2071) Grad: 1.1274  LR: 0.00003379  \n","Epoch: [1][392/392] Elapsed 11m 36s (remain 0m 0s) Loss: 0.0751(0.2040) Grad: 3.1311  LR: 0.00003328  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/98] Loss: 0.1015(0.1085) \n","Epoch: [1][40/98] Loss: 0.1176(0.1076) \n","Epoch: [1][60/98] Loss: 0.0858(0.1113) \n","Epoch: [1][80/98] Loss: 0.0994(0.1087) \n","Epoch: [1][98/98] Loss: 0.0798(0.1112) \n","\n","The valid loss for the current epoch is 0.4729728698730469\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/392] Elapsed 0m 36s (remain 11m 10s) Loss: 0.0578(0.0967) Grad: 1.1116  LR: 0.00003242  \n","Epoch: [2][40/392] Elapsed 1m 11s (remain 10m 30s) Loss: 0.0629(0.0991) Grad: 2.3413  LR: 0.00003157  \n","Epoch: [2][60/392] Elapsed 1m 47s (remain 9m 53s) Loss: 0.0989(0.0999) Grad: 1.4781  LR: 0.00003072  \n","Epoch: [2][80/392] Elapsed 2m 22s (remain 9m 17s) Loss: 0.0959(0.0952) Grad: 2.3517  LR: 0.00002987  \n","Epoch: [2][100/392] Elapsed 2m 58s (remain 8m 41s) Loss: 0.0961(0.0960) Grad: 1.6584  LR: 0.00002901  \n","Epoch: [2][120/392] Elapsed 3m 33s (remain 8m 5s) Loss: 0.1020(0.0959) Grad: 1.7205  LR: 0.00002816  \n","Epoch: [2][140/392] Elapsed 4m 9s (remain 7m 28s) Loss: 0.0828(0.0954) Grad: 1.9280  LR: 0.00002731  \n","Epoch: [2][160/392] Elapsed 4m 45s (remain 6m 53s) Loss: 0.0786(0.0951) Grad: 1.0012  LR: 0.00002645  \n","Epoch: [2][180/392] Elapsed 5m 20s (remain 6m 17s) Loss: 0.0803(0.0961) Grad: 1.9993  LR: 0.00002560  \n","Epoch: [2][200/392] Elapsed 5m 56s (remain 5m 41s) Loss: 0.0530(0.0959) Grad: 1.7269  LR: 0.00002475  \n","Epoch: [2][220/392] Elapsed 6m 31s (remain 5m 6s) Loss: 0.1467(0.0964) Grad: 2.2892  LR: 0.00002389  \n","Epoch: [2][240/392] Elapsed 7m 7s (remain 4m 30s) Loss: 0.0957(0.0969) Grad: 1.3512  LR: 0.00002304  \n","Epoch: [2][260/392] Elapsed 7m 42s (remain 3m 55s) Loss: 0.1046(0.0973) Grad: 3.9985  LR: 0.00002219  \n","Epoch: [2][280/392] Elapsed 8m 18s (remain 3m 19s) Loss: 0.1395(0.0976) Grad: 2.5892  LR: 0.00002133  \n","Epoch: [2][300/392] Elapsed 8m 54s (remain 2m 43s) Loss: 0.1673(0.0976) Grad: 2.3944  LR: 0.00002048  \n","Epoch: [2][320/392] Elapsed 9m 29s (remain 2m 8s) Loss: 0.1088(0.0974) Grad: 1.6517  LR: 0.00001963  \n","Epoch: [2][340/392] Elapsed 10m 5s (remain 1m 32s) Loss: 0.0537(0.0972) Grad: 2.0366  LR: 0.00001877  \n","Epoch: [2][360/392] Elapsed 10m 41s (remain 0m 56s) Loss: 0.0856(0.0969) Grad: 1.6528  LR: 0.00001792  \n","Epoch: [2][380/392] Elapsed 11m 16s (remain 0m 21s) Loss: 0.0980(0.0972) Grad: 1.6428  LR: 0.00001707  \n","Epoch: [2][392/392] Elapsed 11m 36s (remain 0m 0s) Loss: 0.0888(0.0967) Grad: 3.1991  LR: 0.00001656  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/98] Loss: 0.0943(0.1018) \n","Epoch: [2][40/98] Loss: 0.1175(0.1040) \n","Epoch: [2][60/98] Loss: 0.0933(0.1069) \n","Epoch: [2][80/98] Loss: 0.0932(0.1051) \n","Epoch: [2][98/98] Loss: 0.0796(0.1075) \n","\n","The valid loss for the current epoch is 0.46460196375846863\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/392] Elapsed 0m 35s (remain 11m 9s) Loss: 0.0692(0.0757) Grad: 1.4620  LR: 0.00001570  \n","Epoch: [3][40/392] Elapsed 1m 11s (remain 10m 29s) Loss: 0.0485(0.0771) Grad: 1.2125  LR: 0.00001485  \n","Epoch: [3][60/392] Elapsed 1m 47s (remain 9m 52s) Loss: 0.0619(0.0792) Grad: 1.9911  LR: 0.00001400  \n","Epoch: [3][80/392] Elapsed 2m 22s (remain 9m 15s) Loss: 0.1150(0.0794) Grad: 2.4207  LR: 0.00001314  \n","Epoch: [3][100/392] Elapsed 2m 58s (remain 8m 40s) Loss: 0.0951(0.0785) Grad: 1.4100  LR: 0.00001229  \n","Epoch: [3][120/392] Elapsed 3m 33s (remain 8m 4s) Loss: 0.0603(0.0784) Grad: 1.1655  LR: 0.00001144  \n","Epoch: [3][140/392] Elapsed 4m 9s (remain 7m 29s) Loss: 0.0634(0.0781) Grad: 2.1771  LR: 0.00001058  \n","Epoch: [3][160/392] Elapsed 4m 45s (remain 6m 53s) Loss: 0.0560(0.0792) Grad: 1.8400  LR: 0.00000973  \n","Epoch: [3][180/392] Elapsed 5m 20s (remain 6m 17s) Loss: 0.0478(0.0799) Grad: 2.2604  LR: 0.00000888  \n","Epoch: [3][200/392] Elapsed 5m 56s (remain 5m 41s) Loss: 0.0653(0.0796) Grad: 1.7045  LR: 0.00000802  \n","Epoch: [3][220/392] Elapsed 6m 31s (remain 5m 6s) Loss: 0.0798(0.0789) Grad: 2.1088  LR: 0.00000717  \n","Epoch: [3][240/392] Elapsed 7m 7s (remain 4m 30s) Loss: 0.0767(0.0785) Grad: 1.0078  LR: 0.00000632  \n","Epoch: [3][260/392] Elapsed 7m 43s (remain 3m 55s) Loss: 0.0597(0.0782) Grad: 2.3023  LR: 0.00000547  \n","Epoch: [3][280/392] Elapsed 8m 18s (remain 3m 19s) Loss: 0.1109(0.0779) Grad: 2.7755  LR: 0.00000461  \n","Epoch: [3][300/392] Elapsed 8m 54s (remain 2m 43s) Loss: 0.0696(0.0773) Grad: 1.0462  LR: 0.00000376  \n","Epoch: [3][320/392] Elapsed 9m 29s (remain 2m 8s) Loss: 0.0702(0.0770) Grad: 1.2861  LR: 0.00000291  \n","Epoch: [3][340/392] Elapsed 10m 5s (remain 1m 32s) Loss: 0.0677(0.0769) Grad: 1.0670  LR: 0.00000205  \n","Epoch: [3][360/392] Elapsed 10m 41s (remain 0m 57s) Loss: 0.0723(0.0765) Grad: 1.9810  LR: 0.00000120  \n","Epoch: [3][380/392] Elapsed 11m 17s (remain 0m 21s) Loss: 0.0615(0.0763) Grad: 1.0906  LR: 0.00000035  \n","Epoch: [3][392/392] Elapsed 11m 37s (remain 0m 0s) Loss: 0.0682(0.0763) Grad: 3.7950  LR: 0.00000001  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/98] Loss: 0.0923(0.1062) \n","Epoch: [3][40/98] Loss: 0.1208(0.1040) \n","Epoch: [3][60/98] Loss: 0.1007(0.1070) \n","Epoch: [3][80/98] Loss: 0.1098(0.1055) \n","Epoch: [3][98/98] Loss: 0.0915(0.1081) \n","\n","The valid loss for the current epoch is 0.46583881974220276\n","\n","######\n","The best loss for fold 2 is 0.46460196375846863\n","######\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","RUNNING FOLD 3\n","\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","TRAIN LOOP\n","\n","Epoch: [1][20/392] Elapsed 0m 35s (remain 11m 6s) Loss: 0.1531(1.3639) Grad: 4.6411  LR: 0.00004915  \n","Epoch: [1][40/392] Elapsed 1m 11s (remain 10m 29s) Loss: 0.1890(0.7763) Grad: 3.7094  LR: 0.00004829  \n","Epoch: [1][60/392] Elapsed 1m 47s (remain 9m 52s) Loss: 0.1346(0.5711) Grad: 5.3653  LR: 0.00004744  \n","Epoch: [1][80/392] Elapsed 2m 22s (remain 9m 15s) Loss: 0.1676(0.4609) Grad: 6.7591  LR: 0.00004659  \n","Epoch: [1][100/392] Elapsed 2m 58s (remain 8m 40s) Loss: 0.0859(0.3931) Grad: 0.8854  LR: 0.00004573  \n","Epoch: [1][120/392] Elapsed 3m 33s (remain 8m 4s) Loss: 0.1027(0.3488) Grad: 3.6138  LR: 0.00004488  \n","Epoch: [1][140/392] Elapsed 4m 9s (remain 7m 28s) Loss: 0.0754(0.3165) Grad: 1.6651  LR: 0.00004403  \n","Epoch: [1][160/392] Elapsed 4m 44s (remain 6m 53s) Loss: 0.1048(0.2937) Grad: 2.4503  LR: 0.00004317  \n","Epoch: [1][180/392] Elapsed 5m 21s (remain 6m 18s) Loss: 0.0952(0.2742) Grad: 2.0699  LR: 0.00004232  \n","Epoch: [1][200/392] Elapsed 5m 56s (remain 5m 42s) Loss: 0.0897(0.2588) Grad: 1.2587  LR: 0.00004147  \n","Epoch: [1][220/392] Elapsed 6m 32s (remain 5m 6s) Loss: 0.1276(0.2452) Grad: 2.6185  LR: 0.00004062  \n","Epoch: [1][240/392] Elapsed 7m 8s (remain 4m 31s) Loss: 0.0857(0.2336) Grad: 2.4759  LR: 0.00003976  \n","Epoch: [1][260/392] Elapsed 7m 43s (remain 3m 55s) Loss: 0.1654(0.2234) Grad: 1.5425  LR: 0.00003891  \n","Epoch: [1][280/392] Elapsed 8m 19s (remain 3m 19s) Loss: 0.0562(0.2161) Grad: 1.8470  LR: 0.00003806  \n","Epoch: [1][300/392] Elapsed 8m 55s (remain 2m 44s) Loss: 0.0913(0.2093) Grad: 1.5352  LR: 0.00003720  \n","Epoch: [1][320/392] Elapsed 9m 30s (remain 2m 8s) Loss: 0.2100(0.2057) Grad: 5.6680  LR: 0.00003635  \n","Epoch: [1][340/392] Elapsed 10m 6s (remain 1m 32s) Loss: 0.1029(0.2010) Grad: 2.5182  LR: 0.00003550  \n","Epoch: [1][360/392] Elapsed 10m 42s (remain 0m 57s) Loss: 0.1061(0.1960) Grad: 3.2904  LR: 0.00003464  \n","Epoch: [1][380/392] Elapsed 11m 18s (remain 0m 21s) Loss: 0.1166(0.1909) Grad: 1.6601  LR: 0.00003379  \n","Epoch: [1][392/392] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1990(0.1891) Grad: 4.5848  LR: 0.00003328  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/98] Loss: 0.0923(0.0981) \n","Epoch: [1][40/98] Loss: 0.0994(0.1033) \n","Epoch: [1][60/98] Loss: 0.1020(0.1030) \n","Epoch: [1][80/98] Loss: 0.1013(0.1055) \n","Epoch: [1][98/98] Loss: 0.0785(0.1044) \n","\n","The valid loss for the current epoch is 0.4575028121471405\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/392] Elapsed 0m 36s (remain 11m 11s) Loss: 0.1596(0.1157) Grad: 1.8557  LR: 0.00003242  \n","Epoch: [2][40/392] Elapsed 1m 12s (remain 10m 35s) Loss: 0.1175(0.1105) Grad: 1.6370  LR: 0.00003157  \n","Epoch: [2][60/392] Elapsed 1m 48s (remain 10m 2s) Loss: 0.1118(0.1063) Grad: 1.7575  LR: 0.00003072  \n","Epoch: [2][80/392] Elapsed 2m 24s (remain 9m 23s) Loss: 0.1177(0.1033) Grad: 3.1777  LR: 0.00002987  \n","Epoch: [2][100/392] Elapsed 3m 0s (remain 8m 46s) Loss: 0.0740(0.1034) Grad: 2.0096  LR: 0.00002901  \n","Epoch: [2][120/392] Elapsed 3m 35s (remain 8m 8s) Loss: 0.1162(0.1034) Grad: 1.1216  LR: 0.00002816  \n","Epoch: [2][140/392] Elapsed 4m 11s (remain 7m 32s) Loss: 0.0933(0.1031) Grad: 1.4952  LR: 0.00002731  \n","Epoch: [2][160/392] Elapsed 4m 46s (remain 6m 55s) Loss: 0.1397(0.1019) Grad: 2.4503  LR: 0.00002645  \n","Epoch: [2][180/392] Elapsed 5m 22s (remain 6m 19s) Loss: 0.0764(0.1004) Grad: 1.0601  LR: 0.00002560  \n"]}],"source":["for fold in range(5):\n","  print(\"-----\"*30)\n","  print(f\"\\nRUNNING FOLD {fold}\\n\")\n","  print(\"-----\"*30)\n","  params = {'lr': 5e-5,\n","            'llrd': 0.9,\n","            'scheduler': 'polynomial',\n","            'reinit_layers': 4}\n","  best_loss = main(fold, params)\n","  print(\"######\")\n","  print(f\"The best loss for fold {fold} is {best_loss}\")\n","  print(\"######\")\n","  gc.collect()"]},{"cell_type":"code","source":["#running hyperparameter tuning on first fold with optuna\n","def objective(trial):\n","  params = {\n","      \"lr\": trial.suggest_loguniform(\"learning_rate\", 1e-7, 1e-4),\n","      \"llrd\" : trial.suggest_uniform(\"layer_wise_learning_rate_decay\", 0.7, 1.0),\n","      \"scheduler\" : trial.suggest_categorical(\"learning_rate_schduler\",[\"polynomial\", \"cosine\",\"linear\"]),\n","      \"reinit_layers\": trial.suggest_int(\"reinit_layers\",0, 4)\n","  }\n","  best_loss= main(1,params)\n","  return best_loss\n","\n","\n","study = optuna.create_study(direction='minimize')\n","study.optimize(objective, n_trials=20)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vTU6r_sovsBJ","executionInfo":{"status":"ok","timestamp":1663311812699,"user_tz":-330,"elapsed":12567757,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}},"outputId":"5229c918-bbbb-48d6-c1d3-7784cf0e3b14"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-09-15 18:26:18,743]\u001b[0m A new study created in memory with name: no-name-d9193ad8-4b94-4d31-a28c-de8c5b753dcc\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 12s (remain 10m 33s) Loss: 1.3281(2.1507) Grad: 16.2040  LR: 0.00000612  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.2657(1.2626) Grad: 3.9198  LR: 0.00001225  \n","Epoch: [1][60/196] Elapsed 3m 35s (remain 8m 7s) Loss: 0.1242(0.8913) Grad: 3.8266  LR: 0.00001837  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 0.1186(0.6998) Grad: 4.7077  LR: 0.00002450  \n","Epoch: [1][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.0962(0.5867) Grad: 1.9993  LR: 0.00003062  \n","Epoch: [1][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 0.1393(0.5106) Grad: 2.8769  LR: 0.00003583  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1014(0.4560) Grad: 3.6234  LR: 0.00003562  \n","Epoch: [1][160/196] Elapsed 9m 32s (remain 2m 8s) Loss: 0.0952(0.4140) Grad: 2.8452  LR: 0.00003509  \n","Epoch: [1][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.1466(0.3822) Grad: 3.5936  LR: 0.00003426  \n","Epoch: [1][196/196] Elapsed 11m 39s (remain 0m 0s) Loss: 0.1468(0.3617) Grad: 1.6981  LR: 0.00003338  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.48067936301231384\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 12s (remain 10m 34s) Loss: 0.1198(0.1124) Grad: 4.5297  LR: 0.00003203  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.0867(0.1098) Grad: 2.9946  LR: 0.00003043  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.0721(0.1071) Grad: 1.0160  LR: 0.00002861  \n","Epoch: [2][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.1238(0.1053) Grad: 1.7685  LR: 0.00002659  \n","Epoch: [2][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.0893(0.1032) Grad: 2.4408  LR: 0.00002442  \n","Epoch: [2][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 0.1051(0.1027) Grad: 1.9590  LR: 0.00002213  \n","Epoch: [2][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1304(0.1035) Grad: 3.9332  LR: 0.00001977  \n","Epoch: [2][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.0849(0.1025) Grad: 2.3551  LR: 0.00001737  \n","Epoch: [2][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.0684(0.1009) Grad: 0.9413  LR: 0.00001499  \n","Epoch: [2][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.0733(0.1001) Grad: 1.6976  LR: 0.00001311  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.4583507478237152\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 12s (remain 10m 34s) Loss: 0.1027(0.0779) Grad: 2.0285  LR: 0.00001085  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.0564(0.0764) Grad: 2.0738  LR: 0.00000872  \n","Epoch: [3][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.0568(0.0725) Grad: 1.0257  LR: 0.00000674  \n","Epoch: [3][80/196] Elapsed 4m 45s (remain 6m 54s) Loss: 0.0621(0.0709) Grad: 1.6748  LR: 0.00000497  \n","Epoch: [3][100/196] Elapsed 5m 57s (remain 5m 42s) Loss: 0.0883(0.0699) Grad: 1.3882  LR: 0.00000344  \n","Epoch: [3][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.0591(0.0698) Grad: 1.6332  LR: 0.00000216  \n","Epoch: [3][140/196] Elapsed 8m 19s (remain 3m 19s) Loss: 0.0601(0.0694) Grad: 1.6902  LR: 0.00000116  \n","Epoch: [3][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.0631(0.0693) Grad: 0.7188  LR: 0.00000046  \n","Epoch: [3][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.0716(0.0694) Grad: 1.0313  LR: 0.00000008  \n","Epoch: [3][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.0824(0.0692) Grad: 2.5153  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.45831298828125\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-09-15 19:04:12,212]\u001b[0m Trial 0 finished with value: 0.45831298828125 and parameters: {'learning_rate': 3.5828994394331454e-05, 'layer_wise_learning_rate_decay': 0.9767672591137755, 'learning_rate_schduler': 'cosine', 'reinit_layers': 1}. Best is trial 0 with value: 0.45831298828125.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 11s (remain 10m 32s) Loss: 0.6627(1.9961) Grad: 12.6625  LR: 0.00001022  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 18s) Loss: 0.1617(1.1272) Grad: 5.9817  LR: 0.00002043  \n","Epoch: [1][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.1523(0.8059) Grad: 6.1189  LR: 0.00003065  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.1614(0.6430) Grad: 4.9825  LR: 0.00004086  \n","Epoch: [1][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.0882(0.5389) Grad: 2.2667  LR: 0.00005108  \n","Epoch: [1][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 0.1492(0.4709) Grad: 5.0348  LR: 0.00005975  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.0770(0.4227) Grad: 1.9766  LR: 0.00005941  \n","Epoch: [1][160/196] Elapsed 9m 32s (remain 2m 8s) Loss: 0.1981(0.3885) Grad: 4.4461  LR: 0.00005853  \n","Epoch: [1][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.1491(0.3605) Grad: 3.6420  LR: 0.00005714  \n","Epoch: [1][196/196] Elapsed 11m 39s (remain 0m 0s) Loss: 0.1232(0.3415) Grad: 4.7468  LR: 0.00005567  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.5392002463340759\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 11s (remain 10m 29s) Loss: 0.1147(0.1274) Grad: 3.8033  LR: 0.00005343  \n","Epoch: [2][40/196] Elapsed 2m 22s (remain 9m 17s) Loss: 0.1100(0.1139) Grad: 3.7908  LR: 0.00005076  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 5s) Loss: 0.0869(0.1118) Grad: 1.9651  LR: 0.00004772  \n","Epoch: [2][80/196] Elapsed 4m 45s (remain 6m 54s) Loss: 0.0801(0.1090) Grad: 2.1392  LR: 0.00004435  \n","Epoch: [2][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.0924(0.1070) Grad: 2.8385  LR: 0.00004073  \n","Epoch: [2][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1136(0.1055) Grad: 2.7469  LR: 0.00003692  \n","Epoch: [2][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.0665(0.1039) Grad: 1.7560  LR: 0.00003298  \n","Epoch: [2][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.0733(0.1047) Grad: 2.2175  LR: 0.00002898  \n","Epoch: [2][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.1089(0.1048) Grad: 3.1740  LR: 0.00002500  \n","Epoch: [2][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.0935(0.1038) Grad: 3.1952  LR: 0.00002187  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.4593140780925751\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 12s (remain 10m 33s) Loss: 0.0643(0.0777) Grad: 1.6479  LR: 0.00001810  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.0808(0.0774) Grad: 1.9345  LR: 0.00001454  \n","Epoch: [3][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.0554(0.0755) Grad: 1.2811  LR: 0.00001125  \n","Epoch: [3][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.0652(0.0734) Grad: 0.8362  LR: 0.00000830  \n","Epoch: [3][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.0702(0.0730) Grad: 2.1292  LR: 0.00000573  \n","Epoch: [3][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 0.0739(0.0727) Grad: 1.9861  LR: 0.00000360  \n","Epoch: [3][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.0500(0.0715) Grad: 1.7373  LR: 0.00000193  \n","Epoch: [3][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.0495(0.0710) Grad: 1.5459  LR: 0.00000077  \n","Epoch: [3][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.0791(0.0711) Grad: 0.7775  LR: 0.00000013  \n","Epoch: [3][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.0489(0.0703) Grad: 1.7691  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.45570382475852966\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-09-15 19:42:05,320]\u001b[0m Trial 1 finished with value: 0.45570382475852966 and parameters: {'learning_rate': 5.975913614980313e-05, 'layer_wise_learning_rate_decay': 0.9419119594261866, 'learning_rate_schduler': 'cosine', 'reinit_layers': 1}. Best is trial 1 with value: 0.45570382475852966.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 11s (remain 10m 26s) Loss: 3.0075(2.8926) Grad: 45.9391  LR: 0.00000002  \n","Epoch: [1][40/196] Elapsed 2m 22s (remain 9m 15s) Loss: 2.9578(2.8690) Grad: 46.0174  LR: 0.00000004  \n","Epoch: [1][60/196] Elapsed 3m 33s (remain 8m 4s) Loss: 2.8359(2.8478) Grad: 45.5045  LR: 0.00000007  \n","Epoch: [1][80/196] Elapsed 4m 45s (remain 6m 53s) Loss: 2.4535(2.8280) Grad: 45.7833  LR: 0.00000009  \n","Epoch: [1][100/196] Elapsed 5m 56s (remain 5m 42s) Loss: 2.6267(2.7785) Grad: 45.9548  LR: 0.00000011  \n","Epoch: [1][120/196] Elapsed 7m 7s (remain 4m 30s) Loss: 2.3203(2.7316) Grad: 45.5919  LR: 0.00000013  \n","Epoch: [1][140/196] Elapsed 8m 18s (remain 3m 19s) Loss: 2.2931(2.6779) Grad: 45.9473  LR: 0.00000013  \n","Epoch: [1][160/196] Elapsed 9m 30s (remain 2m 8s) Loss: 2.2342(2.6223) Grad: 45.3126  LR: 0.00000013  \n","Epoch: [1][180/196] Elapsed 10m 41s (remain 0m 57s) Loss: 1.8138(2.5486) Grad: 44.5934  LR: 0.00000013  \n","Epoch: [1][196/196] Elapsed 11m 37s (remain 0m 0s) Loss: 2.0883(2.4950) Grad: 45.1421  LR: 0.00000012  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 2.0875608921051025\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 11s (remain 10m 32s) Loss: 1.7726(1.6957) Grad: 45.7035  LR: 0.00000012  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 1.4059(1.6226) Grad: 44.1594  LR: 0.00000011  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 1.2418(1.5850) Grad: 42.4413  LR: 0.00000010  \n","Epoch: [2][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 1.3995(1.5321) Grad: 43.4033  LR: 0.00000010  \n","Epoch: [2][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 1.3439(1.4697) Grad: 43.2231  LR: 0.00000009  \n","Epoch: [2][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 1.1205(1.4049) Grad: 40.3951  LR: 0.00000008  \n","Epoch: [2][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 1.1597(1.3597) Grad: 40.4753  LR: 0.00000007  \n","Epoch: [2][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.9332(1.3135) Grad: 37.0475  LR: 0.00000006  \n","Epoch: [2][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.9381(1.2679) Grad: 37.3355  LR: 0.00000005  \n","Epoch: [2][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.9709(1.2376) Grad: 37.4341  LR: 0.00000005  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 1.4343982934951782\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 11s (remain 10m 26s) Loss: 0.5848(0.8464) Grad: 31.5245  LR: 0.00000004  \n","Epoch: [3][40/196] Elapsed 2m 22s (remain 9m 14s) Loss: 0.7681(0.7997) Grad: 36.3528  LR: 0.00000003  \n","Epoch: [3][60/196] Elapsed 3m 33s (remain 8m 3s) Loss: 1.0294(0.7980) Grad: 37.7590  LR: 0.00000002  \n","Epoch: [3][80/196] Elapsed 4m 44s (remain 6m 52s) Loss: 0.8957(0.7915) Grad: 34.5998  LR: 0.00000002  \n","Epoch: [3][100/196] Elapsed 5m 56s (remain 5m 41s) Loss: 0.7333(0.7815) Grad: 33.4935  LR: 0.00000001  \n","Epoch: [3][120/196] Elapsed 7m 7s (remain 4m 30s) Loss: 0.8519(0.7728) Grad: 34.2606  LR: 0.00000001  \n","Epoch: [3][140/196] Elapsed 8m 19s (remain 3m 19s) Loss: 0.5827(0.7666) Grad: 33.9058  LR: 0.00000000  \n","Epoch: [3][160/196] Elapsed 9m 30s (remain 2m 8s) Loss: 0.7709(0.7612) Grad: 33.7145  LR: 0.00000000  \n","Epoch: [3][180/196] Elapsed 10m 41s (remain 0m 57s) Loss: 0.7607(0.7557) Grad: 35.3653  LR: 0.00000000  \n","Epoch: [3][196/196] Elapsed 11m 37s (remain 0m 0s) Loss: 0.3462(0.7537) Grad: 19.5632  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 1.2976852655410767\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-09-15 20:19:57,130]\u001b[0m Trial 2 finished with value: 1.2976852655410767 and parameters: {'learning_rate': 1.3139112976803736e-07, 'layer_wise_learning_rate_decay': 0.7741365547424885, 'learning_rate_schduler': 'cosine', 'reinit_layers': 0}. Best is trial 1 with value: 0.45570382475852966.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 12s (remain 10m 36s) Loss: 1.6320(2.3574) Grad: 42.3980  LR: 0.00000258  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 20s) Loss: 0.2700(1.5041) Grad: 10.6155  LR: 0.00000515  \n","Epoch: [1][60/196] Elapsed 3m 35s (remain 8m 8s) Loss: 0.2152(1.0883) Grad: 5.6037  LR: 0.00000773  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 0.2636(0.8729) Grad: 4.9757  LR: 0.00001031  \n","Epoch: [1][100/196] Elapsed 5m 58s (remain 5m 43s) Loss: 0.2555(0.7395) Grad: 7.7593  LR: 0.00001288  \n","Epoch: [1][120/196] Elapsed 7m 9s (remain 4m 32s) Loss: 0.2169(0.6504) Grad: 7.0170  LR: 0.00001498  \n","Epoch: [1][140/196] Elapsed 8m 21s (remain 3m 20s) Loss: 0.3085(0.5887) Grad: 5.4635  LR: 0.00001433  \n","Epoch: [1][160/196] Elapsed 9m 32s (remain 2m 8s) Loss: 0.1893(0.5397) Grad: 5.4492  LR: 0.00001369  \n","Epoch: [1][180/196] Elapsed 10m 44s (remain 0m 57s) Loss: 0.1760(0.5016) Grad: 8.2680  LR: 0.00001305  \n","Epoch: [1][196/196] Elapsed 11m 39s (remain 0m 0s) Loss: 0.1654(0.4768) Grad: 14.1178  LR: 0.00001253  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.6211233139038086\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 11s (remain 10m 26s) Loss: 0.1472(0.1683) Grad: 5.9022  LR: 0.00001189  \n","Epoch: [2][40/196] Elapsed 2m 22s (remain 9m 14s) Loss: 0.1583(0.1824) Grad: 7.5168  LR: 0.00001125  \n","Epoch: [2][60/196] Elapsed 3m 32s (remain 8m 2s) Loss: 0.2062(0.1811) Grad: 5.8140  LR: 0.00001061  \n","Epoch: [2][80/196] Elapsed 4m 44s (remain 6m 51s) Loss: 0.1843(0.1825) Grad: 14.7255  LR: 0.00000996  \n","Epoch: [2][100/196] Elapsed 5m 55s (remain 5m 40s) Loss: 0.1779(0.1835) Grad: 11.2766  LR: 0.00000932  \n","Epoch: [2][120/196] Elapsed 7m 6s (remain 4m 29s) Loss: 0.2112(0.1822) Grad: 5.5069  LR: 0.00000868  \n","Epoch: [2][140/196] Elapsed 8m 17s (remain 3m 18s) Loss: 0.1693(0.1801) Grad: 14.3329  LR: 0.00000804  \n","Epoch: [2][160/196] Elapsed 9m 28s (remain 2m 7s) Loss: 0.1334(0.1775) Grad: 3.9538  LR: 0.00000739  \n","Epoch: [2][180/196] Elapsed 10m 38s (remain 0m 56s) Loss: 0.1755(0.1754) Grad: 7.8093  LR: 0.00000675  \n","Epoch: [2][196/196] Elapsed 11m 34s (remain 0m 0s) Loss: 0.1406(0.1746) Grad: 16.7026  LR: 0.00000624  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.5681373476982117\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 10s (remain 10m 24s) Loss: 0.1389(0.1561) Grad: 8.5847  LR: 0.00000559  \n","Epoch: [3][40/196] Elapsed 2m 21s (remain 9m 12s) Loss: 0.1095(0.1567) Grad: 21.5962  LR: 0.00000495  \n","Epoch: [3][60/196] Elapsed 3m 32s (remain 8m 1s) Loss: 0.1568(0.1540) Grad: 19.9403  LR: 0.00000431  \n","Epoch: [3][80/196] Elapsed 4m 43s (remain 6m 50s) Loss: 0.1314(0.1581) Grad: 17.7191  LR: 0.00000366  \n","Epoch: [3][100/196] Elapsed 5m 54s (remain 5m 40s) Loss: 0.2093(0.1606) Grad: 21.8581  LR: 0.00000302  \n","Epoch: [3][120/196] Elapsed 7m 5s (remain 4m 29s) Loss: 0.0953(0.1585) Grad: 6.4657  LR: 0.00000238  \n","Epoch: [3][140/196] Elapsed 8m 16s (remain 3m 18s) Loss: 0.2058(0.1565) Grad: 8.6993  LR: 0.00000174  \n","Epoch: [3][160/196] Elapsed 9m 28s (remain 2m 7s) Loss: 0.1531(0.1564) Grad: 13.7082  LR: 0.00000109  \n","Epoch: [3][180/196] Elapsed 10m 39s (remain 0m 56s) Loss: 0.1662(0.1559) Grad: 22.7087  LR: 0.00000045  \n","Epoch: [3][196/196] Elapsed 11m 35s (remain 0m 0s) Loss: 0.1353(0.1553) Grad: 9.6201  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.5547553300857544\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-09-15 20:57:45,530]\u001b[0m Trial 3 finished with value: 0.5547553300857544 and parameters: {'learning_rate': 1.5072591229583353e-05, 'layer_wise_learning_rate_decay': 0.735996067546315, 'learning_rate_schduler': 'polynomial', 'reinit_layers': 0}. Best is trial 1 with value: 0.45570382475852966.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 12s (remain 10m 34s) Loss: 2.9079(2.8963) Grad: 19.5235  LR: 0.00000007  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 3.0929(2.9001) Grad: 20.1187  LR: 0.00000013  \n","Epoch: [1][60/196] Elapsed 3m 34s (remain 8m 7s) Loss: 2.9493(2.8764) Grad: 19.9003  LR: 0.00000020  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 2.6498(2.8356) Grad: 19.7370  LR: 0.00000026  \n","Epoch: [1][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 2.4036(2.7994) Grad: 19.7950  LR: 0.00000033  \n","Epoch: [1][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 2.2564(2.7526) Grad: 20.0148  LR: 0.00000038  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 2.3973(2.6889) Grad: 19.9308  LR: 0.00000037  \n","Epoch: [1][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 2.2436(2.6279) Grad: 20.1031  LR: 0.00000035  \n","Epoch: [1][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 1.9258(2.5645) Grad: 19.6313  LR: 0.00000033  \n","Epoch: [1][196/196] Elapsed 11m 37s (remain 0m 0s) Loss: 1.9134(2.5094) Grad: 20.2492  LR: 0.00000032  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 2.106431245803833\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 11s (remain 10m 29s) Loss: 1.6808(1.7781) Grad: 19.8798  LR: 0.00000030  \n","Epoch: [2][40/196] Elapsed 2m 22s (remain 9m 14s) Loss: 1.7265(1.7097) Grad: 19.6421  LR: 0.00000029  \n","Epoch: [2][60/196] Elapsed 3m 32s (remain 8m 2s) Loss: 1.5776(1.6278) Grad: 19.8334  LR: 0.00000027  \n","Epoch: [2][80/196] Elapsed 4m 43s (remain 6m 51s) Loss: 1.2760(1.5567) Grad: 19.0275  LR: 0.00000025  \n","Epoch: [2][100/196] Elapsed 5m 54s (remain 5m 40s) Loss: 1.2371(1.4803) Grad: 18.6687  LR: 0.00000024  \n","Epoch: [2][120/196] Elapsed 7m 5s (remain 4m 29s) Loss: 1.0437(1.4296) Grad: 17.7560  LR: 0.00000022  \n","Epoch: [2][140/196] Elapsed 8m 16s (remain 3m 18s) Loss: 1.0396(1.3702) Grad: 18.1523  LR: 0.00000021  \n","Epoch: [2][160/196] Elapsed 9m 27s (remain 2m 7s) Loss: 1.0987(1.3190) Grad: 17.9505  LR: 0.00000019  \n","Epoch: [2][180/196] Elapsed 10m 38s (remain 0m 56s) Loss: 0.8633(1.2749) Grad: 17.9608  LR: 0.00000017  \n","Epoch: [2][196/196] Elapsed 11m 33s (remain 0m 0s) Loss: 0.9846(1.2453) Grad: 18.0435  LR: 0.00000016  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 1.4301539659500122\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 10s (remain 10m 23s) Loss: 0.7749(0.7907) Grad: 16.1755  LR: 0.00000014  \n","Epoch: [3][40/196] Elapsed 2m 21s (remain 9m 13s) Loss: 0.8135(0.7792) Grad: 17.3570  LR: 0.00000013  \n","Epoch: [3][60/196] Elapsed 3m 33s (remain 8m 3s) Loss: 0.6348(0.7681) Grad: 14.9275  LR: 0.00000011  \n","Epoch: [3][80/196] Elapsed 4m 44s (remain 6m 52s) Loss: 0.8876(0.7524) Grad: 17.1809  LR: 0.00000009  \n","Epoch: [3][100/196] Elapsed 5m 55s (remain 5m 41s) Loss: 0.5833(0.7282) Grad: 14.1860  LR: 0.00000008  \n","Epoch: [3][120/196] Elapsed 7m 6s (remain 4m 30s) Loss: 0.7840(0.7091) Grad: 16.1348  LR: 0.00000006  \n","Epoch: [3][140/196] Elapsed 8m 17s (remain 3m 19s) Loss: 0.5202(0.7009) Grad: 13.6946  LR: 0.00000004  \n","Epoch: [3][160/196] Elapsed 9m 29s (remain 2m 8s) Loss: 0.5489(0.6882) Grad: 13.4129  LR: 0.00000003  \n","Epoch: [3][180/196] Elapsed 10m 40s (remain 0m 56s) Loss: 0.5672(0.6770) Grad: 14.9448  LR: 0.00000001  \n","Epoch: [3][196/196] Elapsed 11m 36s (remain 0m 0s) Loss: 0.6055(0.6716) Grad: 15.2379  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 1.177595853805542\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-09-15 21:35:29,369]\u001b[0m Trial 4 finished with value: 1.177595853805542 and parameters: {'learning_rate': 3.8498404377033904e-07, 'layer_wise_learning_rate_decay': 0.7221476129529402, 'learning_rate_schduler': 'linear', 'reinit_layers': 2}. Best is trial 1 with value: 0.45570382475852966.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 12s (remain 10m 35s) Loss: 2.2792(2.4612) Grad: 21.3833  LR: 0.00000020  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 20s) Loss: 2.5460(2.4187) Grad: 21.7220  LR: 0.00000041  \n","Epoch: [1][60/196] Elapsed 3m 34s (remain 8m 7s) Loss: 1.9550(2.3407) Grad: 21.4315  LR: 0.00000061  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 1.6546(2.2281) Grad: 21.5187  LR: 0.00000081  \n","Epoch: [1][100/196] Elapsed 5m 58s (remain 5m 43s) Loss: 1.3316(2.0808) Grad: 20.3413  LR: 0.00000102  \n","Epoch: [1][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 0.8517(1.9025) Grad: 18.6450  LR: 0.00000119  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.4417(1.7069) Grad: 11.7520  LR: 0.00000118  \n","Epoch: [1][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.3019(1.5293) Grad: 8.5100  LR: 0.00000117  \n","Epoch: [1][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.2264(1.3873) Grad: 3.2246  LR: 0.00000114  \n","Epoch: [1][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1931(1.2923) Grad: 4.1717  LR: 0.00000111  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.6539306640625\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 11s (remain 10m 33s) Loss: 0.1993(0.2188) Grad: 2.1517  LR: 0.00000106  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 18s) Loss: 0.2048(0.2127) Grad: 2.7212  LR: 0.00000101  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.2201(0.2081) Grad: 4.2012  LR: 0.00000095  \n","Epoch: [2][80/196] Elapsed 4m 45s (remain 6m 54s) Loss: 0.1910(0.2014) Grad: 2.0642  LR: 0.00000088  \n","Epoch: [2][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1305(0.2016) Grad: 2.5815  LR: 0.00000081  \n","Epoch: [2][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.2225(0.1983) Grad: 4.5001  LR: 0.00000073  \n","Epoch: [2][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1875(0.1964) Grad: 4.1894  LR: 0.00000066  \n","Epoch: [2][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1341(0.1949) Grad: 3.0573  LR: 0.00000058  \n","Epoch: [2][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.1318(0.1931) Grad: 2.4290  LR: 0.00000050  \n","Epoch: [2][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1550(0.1929) Grad: 5.3703  LR: 0.00000044  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.605730414390564\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 12s (remain 10m 33s) Loss: 0.2002(0.1866) Grad: 4.3901  LR: 0.00000036  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.1505(0.1791) Grad: 1.7839  LR: 0.00000029  \n","Epoch: [3][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.1634(0.1839) Grad: 5.0811  LR: 0.00000022  \n","Epoch: [3][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.2117(0.1822) Grad: 3.3866  LR: 0.00000017  \n","Epoch: [3][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1285(0.1809) Grad: 2.6786  LR: 0.00000011  \n","Epoch: [3][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1569(0.1792) Grad: 1.8177  LR: 0.00000007  \n","Epoch: [3][140/196] Elapsed 8m 19s (remain 3m 19s) Loss: 0.1682(0.1791) Grad: 4.3127  LR: 0.00000004  \n","Epoch: [3][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.2089(0.1793) Grad: 3.6126  LR: 0.00000002  \n","Epoch: [3][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.2151(0.1797) Grad: 3.8528  LR: 0.00000000  \n","Epoch: [3][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1538(0.1791) Grad: 2.6972  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.5984465479850769\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-09-15 22:13:21,780]\u001b[0m Trial 5 finished with value: 0.5984465479850769 and parameters: {'learning_rate': 1.1896075168370157e-06, 'layer_wise_learning_rate_decay': 0.7212806415658086, 'learning_rate_schduler': 'cosine', 'reinit_layers': 3}. Best is trial 1 with value: 0.45570382475852966.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 12s (remain 10m 35s) Loss: 2.4248(2.5588) Grad: 22.4338  LR: 0.00000004  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 2.7910(2.5485) Grad: 22.1282  LR: 0.00000008  \n","Epoch: [1][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 2.4471(2.5460) Grad: 22.0588  LR: 0.00000012  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 2.1733(2.5048) Grad: 22.1435  LR: 0.00000016  \n","Epoch: [1][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 2.3596(2.4730) Grad: 22.2139  LR: 0.00000020  \n","Epoch: [1][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 2.0947(2.4229) Grad: 22.2017  LR: 0.00000023  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 1.9021(2.3670) Grad: 22.2261  LR: 0.00000023  \n","Epoch: [1][160/196] Elapsed 9m 32s (remain 2m 8s) Loss: 1.6768(2.3123) Grad: 21.6222  LR: 0.00000022  \n","Epoch: [1][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 1.4424(2.2496) Grad: 21.6740  LR: 0.00000022  \n","Epoch: [1][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 1.5421(2.2009) Grad: 22.2576  LR: 0.00000021  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 2.0373640060424805\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 12s (remain 10m 34s) Loss: 1.4612(1.5167) Grad: 21.3867  LR: 0.00000020  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 1.2499(1.4624) Grad: 21.7104  LR: 0.00000019  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 1.3014(1.3889) Grad: 21.0756  LR: 0.00000018  \n","Epoch: [2][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.9356(1.3179) Grad: 20.7848  LR: 0.00000017  \n","Epoch: [2][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 1.2706(1.2550) Grad: 21.6890  LR: 0.00000016  \n","Epoch: [2][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.8575(1.1991) Grad: 19.7470  LR: 0.00000014  \n","Epoch: [2][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.7635(1.1436) Grad: 17.6699  LR: 0.00000013  \n","Epoch: [2][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.7791(1.0976) Grad: 18.5902  LR: 0.00000011  \n","Epoch: [2][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.5726(1.0531) Grad: 15.3509  LR: 0.00000010  \n","Epoch: [2][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.6864(1.0204) Grad: 16.8994  LR: 0.00000008  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 1.2470184564590454\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 11s (remain 10m 33s) Loss: 0.6546(0.6254) Grad: 17.6682  LR: 0.00000007  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 18s) Loss: 0.4922(0.6111) Grad: 16.3427  LR: 0.00000006  \n","Epoch: [3][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.4986(0.5993) Grad: 14.5627  LR: 0.00000004  \n","Epoch: [3][80/196] Elapsed 4m 45s (remain 6m 54s) Loss: 0.5609(0.5900) Grad: 14.9508  LR: 0.00000003  \n","Epoch: [3][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.4689(0.5943) Grad: 14.2525  LR: 0.00000002  \n","Epoch: [3][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.5776(0.5850) Grad: 14.7592  LR: 0.00000001  \n","Epoch: [3][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.5983(0.5814) Grad: 15.3451  LR: 0.00000001  \n","Epoch: [3][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.4190(0.5756) Grad: 11.9156  LR: 0.00000000  \n","Epoch: [3][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.6357(0.5761) Grad: 14.9253  LR: 0.00000000  \n","Epoch: [3][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.3736(0.5732) Grad: 11.7782  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 1.1141407489776611\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-09-15 22:51:14,205]\u001b[0m Trial 6 finished with value: 1.1141407489776611 and parameters: {'learning_rate': 2.2922957603103856e-07, 'layer_wise_learning_rate_decay': 0.7872546732613366, 'learning_rate_schduler': 'cosine', 'reinit_layers': 3}. Best is trial 1 with value: 0.45570382475852966.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 12s (remain 10m 39s) Loss: 2.2866(2.2442) Grad: 25.0934  LR: 0.00000055  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 21s) Loss: 1.6449(2.0800) Grad: 24.1665  LR: 0.00000110  \n","Epoch: [1][60/196] Elapsed 3m 35s (remain 8m 7s) Loss: 0.8250(1.7940) Grad: 20.3056  LR: 0.00000166  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 0.2524(1.4649) Grad: 7.4933  LR: 0.00000221  \n","Epoch: [1][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1931(1.2125) Grad: 3.9911  LR: 0.00000276  \n","Epoch: [1][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 0.2164(1.0484) Grad: 2.5684  LR: 0.00000321  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.3320(0.9262) Grad: 3.9656  LR: 0.00000307  \n","Epoch: [1][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.2017(0.8324) Grad: 4.7449  LR: 0.00000293  \n","Epoch: [1][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.1928(0.7586) Grad: 3.0671  LR: 0.00000280  \n","Epoch: [1][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1260(0.7098) Grad: 4.6098  LR: 0.00000269  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.5778746008872986\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 12s (remain 10m 34s) Loss: 0.1259(0.1569) Grad: 2.6451  LR: 0.00000255  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 20s) Loss: 0.1729(0.1579) Grad: 5.7194  LR: 0.00000241  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 7s) Loss: 0.1050(0.1548) Grad: 2.0682  LR: 0.00000227  \n","Epoch: [2][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 0.1220(0.1517) Grad: 2.6985  LR: 0.00000213  \n","Epoch: [2][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1979(0.1542) Grad: 6.6124  LR: 0.00000200  \n","Epoch: [2][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1475(0.1518) Grad: 3.0242  LR: 0.00000186  \n","Epoch: [2][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1040(0.1514) Grad: 3.8692  LR: 0.00000172  \n","Epoch: [2][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1177(0.1508) Grad: 3.2345  LR: 0.00000158  \n","Epoch: [2][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.1099(0.1481) Grad: 4.0045  LR: 0.00000145  \n","Epoch: [2][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.0910(0.1469) Grad: 5.6647  LR: 0.00000134  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.5204089879989624\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 12s (remain 10m 34s) Loss: 0.1290(0.1317) Grad: 3.6755  LR: 0.00000120  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.1475(0.1279) Grad: 3.1740  LR: 0.00000106  \n","Epoch: [3][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.1996(0.1286) Grad: 2.5732  LR: 0.00000092  \n","Epoch: [3][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.1540(0.1282) Grad: 2.5498  LR: 0.00000078  \n","Epoch: [3][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.0768(0.1265) Grad: 1.9484  LR: 0.00000065  \n","Epoch: [3][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1413(0.1274) Grad: 6.7915  LR: 0.00000051  \n","Epoch: [3][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.2095(0.1267) Grad: 3.3926  LR: 0.00000037  \n","Epoch: [3][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1584(0.1272) Grad: 3.9916  LR: 0.00000023  \n","Epoch: [3][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.0828(0.1275) Grad: 3.9993  LR: 0.00000010  \n","Epoch: [3][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1093(0.1275) Grad: 4.5207  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.5056667923927307\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-09-15 23:29:06,692]\u001b[0m Trial 7 finished with value: 0.5056667923927307 and parameters: {'learning_rate': 3.2291716682826884e-06, 'layer_wise_learning_rate_decay': 0.7547056904289499, 'learning_rate_schduler': 'linear', 'reinit_layers': 4}. Best is trial 1 with value: 0.45570382475852966.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 12s (remain 10m 37s) Loss: 0.3207(1.3670) Grad: 10.5144  LR: 0.00000891  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 20s) Loss: 0.2283(0.8161) Grad: 2.8694  LR: 0.00001781  \n","Epoch: [1][60/196] Elapsed 3m 34s (remain 8m 7s) Loss: 0.2030(0.6093) Grad: 4.4690  LR: 0.00002672  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 0.1216(0.4948) Grad: 3.6565  LR: 0.00003563  \n","Epoch: [1][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1262(0.4214) Grad: 4.3316  LR: 0.00004453  \n","Epoch: [1][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 0.1467(0.3738) Grad: 3.4700  LR: 0.00005177  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1391(0.3393) Grad: 4.8795  LR: 0.00004955  \n","Epoch: [1][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1094(0.3154) Grad: 3.3148  LR: 0.00004733  \n","Epoch: [1][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.1142(0.2938) Grad: 2.3177  LR: 0.00004511  \n","Epoch: [1][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1448(0.2790) Grad: 2.5184  LR: 0.00004333  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.4658414423465729\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 12s (remain 10m 35s) Loss: 0.1412(0.1119) Grad: 2.9718  LR: 0.00004111  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.1538(0.1058) Grad: 5.0827  LR: 0.00003888  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.0673(0.1027) Grad: 1.0732  LR: 0.00003666  \n","Epoch: [2][80/196] Elapsed 4m 45s (remain 6m 54s) Loss: 0.1454(0.1046) Grad: 5.7957  LR: 0.00003444  \n","Epoch: [2][100/196] Elapsed 5m 56s (remain 5m 42s) Loss: 0.0906(0.1037) Grad: 2.1349  LR: 0.00003222  \n","Epoch: [2][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1177(0.1031) Grad: 2.8419  LR: 0.00003000  \n","Epoch: [2][140/196] Elapsed 8m 19s (remain 3m 19s) Loss: 0.0918(0.1032) Grad: 2.5247  LR: 0.00002777  \n","Epoch: [2][160/196] Elapsed 9m 30s (remain 2m 8s) Loss: 0.0955(0.1017) Grad: 2.9787  LR: 0.00002555  \n","Epoch: [2][180/196] Elapsed 10m 41s (remain 0m 57s) Loss: 0.1386(0.1013) Grad: 3.7996  LR: 0.00002333  \n","Epoch: [2][196/196] Elapsed 11m 37s (remain 0m 0s) Loss: 0.0761(0.1007) Grad: 2.3506  LR: 0.00002155  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.4809825122356415\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 11s (remain 10m 27s) Loss: 0.1238(0.0825) Grad: 3.0641  LR: 0.00001933  \n","Epoch: [3][40/196] Elapsed 2m 22s (remain 9m 16s) Loss: 0.0481(0.0750) Grad: 1.0917  LR: 0.00001711  \n","Epoch: [3][60/196] Elapsed 3m 33s (remain 8m 4s) Loss: 0.0764(0.0723) Grad: 13.2708  LR: 0.00001489  \n","Epoch: [3][80/196] Elapsed 4m 45s (remain 6m 53s) Loss: 0.0667(0.0711) Grad: 1.3312  LR: 0.00001267  \n","Epoch: [3][100/196] Elapsed 5m 56s (remain 5m 42s) Loss: 0.0566(0.0704) Grad: 1.6026  LR: 0.00001044  \n","Epoch: [3][120/196] Elapsed 7m 7s (remain 4m 30s) Loss: 0.0678(0.0705) Grad: 1.9761  LR: 0.00000822  \n","Epoch: [3][140/196] Elapsed 8m 19s (remain 3m 19s) Loss: 0.0629(0.0704) Grad: 1.0430  LR: 0.00000600  \n","Epoch: [3][160/196] Elapsed 9m 30s (remain 2m 8s) Loss: 0.0634(0.0694) Grad: 2.0001  LR: 0.00000378  \n","Epoch: [3][180/196] Elapsed 10m 41s (remain 0m 57s) Loss: 0.0580(0.0689) Grad: 1.1229  LR: 0.00000156  \n","Epoch: [3][196/196] Elapsed 11m 37s (remain 0m 0s) Loss: 0.0636(0.0685) Grad: 1.6951  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.46920451521873474\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-09-16 00:06:42,044]\u001b[0m Trial 8 finished with value: 0.4658414423465729 and parameters: {'learning_rate': 5.210470632097099e-05, 'layer_wise_learning_rate_decay': 0.9815766262345241, 'learning_rate_schduler': 'linear', 'reinit_layers': 3}. Best is trial 1 with value: 0.45570382475852966.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 11s (remain 10m 33s) Loss: 2.4134(2.3962) Grad: 42.6502  LR: 0.00000033  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 1.9811(2.2472) Grad: 41.9844  LR: 0.00000065  \n","Epoch: [1][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 1.2317(2.0337) Grad: 38.3061  LR: 0.00000098  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.4153(1.7435) Grad: 20.5296  LR: 0.00000130  \n","Epoch: [1][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.2168(1.4496) Grad: 4.8794  LR: 0.00000163  \n","Epoch: [1][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.2251(1.2432) Grad: 7.4450  LR: 0.00000189  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.3085(1.0970) Grad: 5.9484  LR: 0.00000181  \n","Epoch: [1][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1537(0.9886) Grad: 4.0654  LR: 0.00000173  \n","Epoch: [1][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.1554(0.8985) Grad: 4.1655  LR: 0.00000165  \n","Epoch: [1][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1577(0.8407) Grad: 9.5372  LR: 0.00000159  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.6422947645187378\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 12s (remain 10m 35s) Loss: 0.2311(0.2132) Grad: 8.4851  LR: 0.00000150  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 20s) Loss: 0.1534(0.2032) Grad: 6.1575  LR: 0.00000142  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 7s) Loss: 0.1873(0.2005) Grad: 3.4002  LR: 0.00000134  \n","Epoch: [2][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.1891(0.1982) Grad: 4.0219  LR: 0.00000126  \n","Epoch: [2][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1995(0.1948) Grad: 7.3894  LR: 0.00000118  \n","Epoch: [2][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 0.2233(0.1967) Grad: 6.1966  LR: 0.00000110  \n","Epoch: [2][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.2168(0.1968) Grad: 4.5611  LR: 0.00000102  \n","Epoch: [2][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.3456(0.1959) Grad: 6.2934  LR: 0.00000094  \n","Epoch: [2][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.1441(0.1973) Grad: 3.0419  LR: 0.00000085  \n","Epoch: [2][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1834(0.1985) Grad: 6.6247  LR: 0.00000079  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.6282711625099182\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 12s (remain 10m 35s) Loss: 0.1987(0.1992) Grad: 5.6614  LR: 0.00000071  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 20s) Loss: 0.1882(0.1972) Grad: 8.9606  LR: 0.00000063  \n","Epoch: [3][60/196] Elapsed 3m 35s (remain 8m 7s) Loss: 0.1728(0.1959) Grad: 4.6090  LR: 0.00000054  \n","Epoch: [3][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 0.1726(0.1911) Grad: 4.2487  LR: 0.00000046  \n","Epoch: [3][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.2255(0.1929) Grad: 10.1701  LR: 0.00000038  \n","Epoch: [3][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1595(0.1918) Grad: 5.8957  LR: 0.00000030  \n","Epoch: [3][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1906(0.1923) Grad: 3.3299  LR: 0.00000022  \n","Epoch: [3][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.2909(0.1951) Grad: 5.8572  LR: 0.00000014  \n","Epoch: [3][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.1992(0.1937) Grad: 7.1414  LR: 0.00000006  \n","Epoch: [3][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1868(0.1925) Grad: 4.9999  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.6222785711288452\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-09-16 00:44:35,900]\u001b[0m Trial 9 finished with value: 0.6222785711288452 and parameters: {'learning_rate': 1.906783363311194e-06, 'layer_wise_learning_rate_decay': 0.7418116385385011, 'learning_rate_schduler': 'linear', 'reinit_layers': 0}. Best is trial 1 with value: 0.45570382475852966.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 11s (remain 10m 33s) Loss: 2.6041(2.7005) Grad: 17.3163  LR: 0.00000175  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 18s) Loss: 1.3273(2.3319) Grad: 16.5500  LR: 0.00000350  \n","Epoch: [1][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.2629(1.7385) Grad: 4.1191  LR: 0.00000524  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.1850(1.3510) Grad: 2.8408  LR: 0.00000699  \n","Epoch: [1][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1194(1.1085) Grad: 4.0295  LR: 0.00000874  \n","Epoch: [1][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1170(0.9457) Grad: 4.0105  LR: 0.00001016  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.0956(0.8292) Grad: 1.9154  LR: 0.00000972  \n","Epoch: [1][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1318(0.7405) Grad: 3.9856  LR: 0.00000929  \n","Epoch: [1][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.0797(0.6709) Grad: 1.9032  LR: 0.00000885  \n","Epoch: [1][196/196] Elapsed 11m 37s (remain 0m 0s) Loss: 0.1035(0.6258) Grad: 3.6445  LR: 0.00000850  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.46816107630729675\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 11s (remain 10m 32s) Loss: 0.1045(0.1084) Grad: 2.9906  LR: 0.00000807  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 18s) Loss: 0.0956(0.1069) Grad: 2.4060  LR: 0.00000763  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 5s) Loss: 0.0850(0.1065) Grad: 2.0395  LR: 0.00000719  \n","Epoch: [2][80/196] Elapsed 4m 45s (remain 6m 54s) Loss: 0.0731(0.1066) Grad: 1.9639  LR: 0.00000676  \n","Epoch: [2][100/196] Elapsed 5m 57s (remain 5m 42s) Loss: 0.1191(0.1053) Grad: 3.0544  LR: 0.00000632  \n","Epoch: [2][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1163(0.1051) Grad: 5.3515  LR: 0.00000589  \n","Epoch: [2][140/196] Elapsed 8m 19s (remain 3m 19s) Loss: 0.0725(0.1045) Grad: 2.5590  LR: 0.00000545  \n","Epoch: [2][160/196] Elapsed 9m 30s (remain 2m 8s) Loss: 0.1170(0.1056) Grad: 4.4162  LR: 0.00000501  \n","Epoch: [2][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.1289(0.1050) Grad: 3.6293  LR: 0.00000458  \n","Epoch: [2][196/196] Elapsed 11m 37s (remain 0m 0s) Loss: 0.0936(0.1050) Grad: 3.2016  LR: 0.00000423  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.4653651714324951\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 11s (remain 10m 33s) Loss: 0.0870(0.0940) Grad: 3.3620  LR: 0.00000379  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 18s) Loss: 0.0854(0.0942) Grad: 1.7604  LR: 0.00000336  \n","Epoch: [3][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.0796(0.0952) Grad: 1.6269  LR: 0.00000292  \n","Epoch: [3][80/196] Elapsed 4m 45s (remain 6m 54s) Loss: 0.0869(0.0944) Grad: 6.0490  LR: 0.00000249  \n","Epoch: [3][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.0958(0.0936) Grad: 1.7152  LR: 0.00000205  \n","Epoch: [3][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1004(0.0949) Grad: 2.7248  LR: 0.00000161  \n","Epoch: [3][140/196] Elapsed 8m 19s (remain 3m 19s) Loss: 0.1171(0.0954) Grad: 2.8445  LR: 0.00000118  \n","Epoch: [3][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.0734(0.0950) Grad: 1.3139  LR: 0.00000074  \n","Epoch: [3][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.1056(0.0954) Grad: 1.4715  LR: 0.00000031  \n","Epoch: [3][196/196] Elapsed 11m 37s (remain 0m 0s) Loss: 0.0711(0.0946) Grad: 4.7139  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.4555629789829254\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-09-16 01:22:27,296]\u001b[0m Trial 10 finished with value: 0.4555629789829254 and parameters: {'learning_rate': 1.0223764573961562e-05, 'layer_wise_learning_rate_decay': 0.9089745817743605, 'learning_rate_schduler': 'polynomial', 'reinit_layers': 1}. Best is trial 10 with value: 0.4555629789829254.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 12s (remain 10m 34s) Loss: 2.3743(2.8769) Grad: 17.4244  LR: 0.00000177  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 1.2638(2.4915) Grad: 16.5317  LR: 0.00000354  \n","Epoch: [1][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.3078(1.8601) Grad: 5.0017  LR: 0.00000531  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 0.1489(1.4458) Grad: 5.7934  LR: 0.00000708  \n","Epoch: [1][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1392(1.1840) Grad: 3.4416  LR: 0.00000884  \n","Epoch: [1][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1509(1.0075) Grad: 6.5264  LR: 0.00001028  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1047(0.8811) Grad: 4.3910  LR: 0.00000984  \n","Epoch: [1][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1322(0.7849) Grad: 2.5689  LR: 0.00000940  \n","Epoch: [1][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.1091(0.7113) Grad: 2.3364  LR: 0.00000896  \n","Epoch: [1][196/196] Elapsed 11m 37s (remain 0m 0s) Loss: 0.1441(0.6635) Grad: 5.3982  LR: 0.00000861  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.47643914818763733\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 11s (remain 10m 32s) Loss: 0.1012(0.1081) Grad: 2.5777  LR: 0.00000816  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 18s) Loss: 0.0912(0.1041) Grad: 2.1873  LR: 0.00000772  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.0968(0.1061) Grad: 2.3527  LR: 0.00000728  \n","Epoch: [2][80/196] Elapsed 4m 45s (remain 6m 54s) Loss: 0.0991(0.1091) Grad: 6.3720  LR: 0.00000684  \n","Epoch: [2][100/196] Elapsed 5m 56s (remain 5m 42s) Loss: 0.1096(0.1072) Grad: 3.5011  LR: 0.00000640  \n","Epoch: [2][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1002(0.1060) Grad: 1.8169  LR: 0.00000596  \n","Epoch: [2][140/196] Elapsed 8m 19s (remain 3m 19s) Loss: 0.1067(0.1046) Grad: 3.3242  LR: 0.00000552  \n","Epoch: [2][160/196] Elapsed 9m 30s (remain 2m 8s) Loss: 0.1196(0.1045) Grad: 3.9553  LR: 0.00000508  \n","Epoch: [2][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.0960(0.1040) Grad: 2.9201  LR: 0.00000463  \n","Epoch: [2][196/196] Elapsed 11m 37s (remain 0m 0s) Loss: 0.1449(0.1046) Grad: 2.5175  LR: 0.00000428  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.46374818682670593\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 11s (remain 10m 33s) Loss: 0.0976(0.0933) Grad: 2.5091  LR: 0.00000384  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 18s) Loss: 0.0717(0.0953) Grad: 1.6668  LR: 0.00000340  \n","Epoch: [3][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.0905(0.0952) Grad: 2.0780  LR: 0.00000296  \n","Epoch: [3][80/196] Elapsed 4m 45s (remain 6m 54s) Loss: 0.0993(0.0943) Grad: 2.4717  LR: 0.00000252  \n","Epoch: [3][100/196] Elapsed 5m 57s (remain 5m 42s) Loss: 0.0463(0.0944) Grad: 1.1180  LR: 0.00000207  \n","Epoch: [3][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1152(0.0943) Grad: 2.5202  LR: 0.00000163  \n","Epoch: [3][140/196] Elapsed 8m 19s (remain 3m 19s) Loss: 0.1185(0.0944) Grad: 2.8980  LR: 0.00000119  \n","Epoch: [3][160/196] Elapsed 9m 30s (remain 2m 8s) Loss: 0.0831(0.0939) Grad: 1.7900  LR: 0.00000075  \n","Epoch: [3][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.0975(0.0935) Grad: 2.4463  LR: 0.00000031  \n","Epoch: [3][196/196] Elapsed 11m 37s (remain 0m 0s) Loss: 0.0905(0.0933) Grad: 3.1266  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.4552156627178192\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-09-16 02:00:17,411]\u001b[0m Trial 11 finished with value: 0.4552156627178192 and parameters: {'learning_rate': 1.0348080335764067e-05, 'layer_wise_learning_rate_decay': 0.9154759345739282, 'learning_rate_schduler': 'polynomial', 'reinit_layers': 1}. Best is trial 11 with value: 0.4552156627178192.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 11s (remain 10m 33s) Loss: 2.2095(2.6464) Grad: 17.1960  LR: 0.00000144  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 1.7338(2.3753) Grad: 16.3909  LR: 0.00000288  \n","Epoch: [1][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.2608(1.8709) Grad: 3.4001  LR: 0.00000432  \n","Epoch: [1][80/196] Elapsed 4m 45s (remain 6m 54s) Loss: 0.1422(1.4588) Grad: 2.9088  LR: 0.00000576  \n","Epoch: [1][100/196] Elapsed 5m 57s (remain 5m 42s) Loss: 0.1322(1.2014) Grad: 3.4412  LR: 0.00000721  \n","Epoch: [1][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1136(1.0239) Grad: 3.8666  LR: 0.00000838  \n","Epoch: [1][140/196] Elapsed 8m 19s (remain 3m 19s) Loss: 0.1123(0.8947) Grad: 2.4485  LR: 0.00000802  \n","Epoch: [1][160/196] Elapsed 9m 30s (remain 2m 8s) Loss: 0.0813(0.7970) Grad: 1.4254  LR: 0.00000766  \n","Epoch: [1][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.1009(0.7210) Grad: 2.8761  LR: 0.00000730  \n","Epoch: [1][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1665(0.6724) Grad: 5.7341  LR: 0.00000701  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.48826348781585693\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 12s (remain 10m 34s) Loss: 0.1128(0.1127) Grad: 4.0129  LR: 0.00000665  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.0990(0.1113) Grad: 2.1175  LR: 0.00000629  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.0855(0.1094) Grad: 4.3974  LR: 0.00000593  \n","Epoch: [2][80/196] Elapsed 4m 45s (remain 6m 54s) Loss: 0.1590(0.1092) Grad: 5.3067  LR: 0.00000557  \n","Epoch: [2][100/196] Elapsed 5m 56s (remain 5m 42s) Loss: 0.0753(0.1080) Grad: 3.6286  LR: 0.00000521  \n","Epoch: [2][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1043(0.1071) Grad: 1.5519  LR: 0.00000485  \n","Epoch: [2][140/196] Elapsed 8m 19s (remain 3m 19s) Loss: 0.1082(0.1069) Grad: 1.6616  LR: 0.00000449  \n","Epoch: [2][160/196] Elapsed 9m 30s (remain 2m 8s) Loss: 0.1240(0.1063) Grad: 2.0138  LR: 0.00000413  \n","Epoch: [2][180/196] Elapsed 10m 41s (remain 0m 57s) Loss: 0.1217(0.1055) Grad: 3.4128  LR: 0.00000378  \n","Epoch: [2][196/196] Elapsed 11m 37s (remain 0m 0s) Loss: 0.0975(0.1059) Grad: 4.5224  LR: 0.00000349  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.46488532423973083\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 11s (remain 10m 33s) Loss: 0.1062(0.0942) Grad: 2.8301  LR: 0.00000313  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 18s) Loss: 0.0912(0.0947) Grad: 1.8062  LR: 0.00000277  \n","Epoch: [3][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.0850(0.0965) Grad: 1.6934  LR: 0.00000241  \n","Epoch: [3][80/196] Elapsed 4m 45s (remain 6m 54s) Loss: 0.0704(0.0971) Grad: 2.5374  LR: 0.00000205  \n","Epoch: [3][100/196] Elapsed 5m 57s (remain 5m 42s) Loss: 0.1115(0.0989) Grad: 3.8503  LR: 0.00000169  \n","Epoch: [3][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1151(0.0976) Grad: 3.7103  LR: 0.00000133  \n","Epoch: [3][140/196] Elapsed 8m 19s (remain 3m 19s) Loss: 0.1007(0.0976) Grad: 3.6571  LR: 0.00000097  \n","Epoch: [3][160/196] Elapsed 9m 30s (remain 2m 8s) Loss: 0.0786(0.0969) Grad: 1.6071  LR: 0.00000061  \n","Epoch: [3][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.0859(0.0975) Grad: 2.3498  LR: 0.00000025  \n","Epoch: [3][196/196] Elapsed 11m 37s (remain 0m 0s) Loss: 0.1140(0.0972) Grad: 2.4598  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.4534367322921753\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-09-16 02:38:07,724]\u001b[0m Trial 12 finished with value: 0.4534367322921753 and parameters: {'learning_rate': 8.430514132964566e-06, 'layer_wise_learning_rate_decay': 0.8931288973170695, 'learning_rate_schduler': 'polynomial', 'reinit_layers': 1}. Best is trial 12 with value: 0.4534367322921753.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 12s (remain 10m 35s) Loss: 2.2832(2.5220) Grad: 19.9522  LR: 0.00000120  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.9825(2.1572) Grad: 18.0501  LR: 0.00000240  \n","Epoch: [1][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.2484(1.6441) Grad: 6.3193  LR: 0.00000360  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.2434(1.2891) Grad: 7.0590  LR: 0.00000480  \n","Epoch: [1][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1364(1.0664) Grad: 1.7462  LR: 0.00000600  \n","Epoch: [1][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1184(0.9148) Grad: 2.1593  LR: 0.00000698  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1196(0.8032) Grad: 2.8526  LR: 0.00000668  \n","Epoch: [1][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1685(0.7180) Grad: 2.9121  LR: 0.00000638  \n","Epoch: [1][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.1005(0.6521) Grad: 2.3067  LR: 0.00000608  \n","Epoch: [1][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.0842(0.6079) Grad: 2.0194  LR: 0.00000584  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.47588440775871277\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 12s (remain 10m 34s) Loss: 0.1036(0.1088) Grad: 2.2729  LR: 0.00000554  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.1125(0.1112) Grad: 2.4259  LR: 0.00000524  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.1104(0.1098) Grad: 3.5255  LR: 0.00000494  \n","Epoch: [2][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.1016(0.1099) Grad: 2.0838  LR: 0.00000464  \n","Epoch: [2][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.0914(0.1091) Grad: 3.1226  LR: 0.00000434  \n","Epoch: [2][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1214(0.1096) Grad: 3.7896  LR: 0.00000404  \n","Epoch: [2][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1371(0.1100) Grad: 3.1533  LR: 0.00000374  \n","Epoch: [2][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1407(0.1101) Grad: 5.1700  LR: 0.00000345  \n","Epoch: [2][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.1098(0.1097) Grad: 3.6464  LR: 0.00000315  \n","Epoch: [2][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.0810(0.1093) Grad: 3.0437  LR: 0.00000291  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.4614075720310211\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 12s (remain 10m 36s) Loss: 0.0832(0.0988) Grad: 1.7497  LR: 0.00000261  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.0825(0.1018) Grad: 2.8538  LR: 0.00000231  \n","Epoch: [3][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.0850(0.1013) Grad: 2.5458  LR: 0.00000201  \n","Epoch: [3][80/196] Elapsed 4m 45s (remain 6m 54s) Loss: 0.1082(0.1031) Grad: 3.0985  LR: 0.00000171  \n","Epoch: [3][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.0800(0.1014) Grad: 2.0545  LR: 0.00000141  \n","Epoch: [3][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.0955(0.1010) Grad: 2.0914  LR: 0.00000111  \n","Epoch: [3][140/196] Elapsed 8m 19s (remain 3m 19s) Loss: 0.1327(0.1007) Grad: 3.5045  LR: 0.00000081  \n","Epoch: [3][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.0836(0.1007) Grad: 2.7945  LR: 0.00000051  \n","Epoch: [3][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.0766(0.1009) Grad: 1.3957  LR: 0.00000021  \n","Epoch: [3][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1292(0.1011) Grad: 3.2420  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.4558442533016205\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-09-16 03:15:59,833]\u001b[0m Trial 13 finished with value: 0.4558442533016205 and parameters: {'learning_rate': 7.0242596937025054e-06, 'layer_wise_learning_rate_decay': 0.8529699795029209, 'learning_rate_schduler': 'polynomial', 'reinit_layers': 2}. Best is trial 12 with value: 0.4534367322921753.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 11s (remain 10m 33s) Loss: 2.1400(2.5713) Grad: 17.4885  LR: 0.00000291  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 18s) Loss: 0.6804(1.9668) Grad: 13.8776  LR: 0.00000582  \n","Epoch: [1][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.1551(1.3973) Grad: 3.0829  LR: 0.00000873  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.1315(1.0872) Grad: 2.0704  LR: 0.00001164  \n","Epoch: [1][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1088(0.8941) Grad: 3.2032  LR: 0.00001455  \n","Epoch: [1][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 0.1265(0.7645) Grad: 3.1294  LR: 0.00001692  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1299(0.6719) Grad: 3.3602  LR: 0.00001619  \n","Epoch: [1][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1288(0.6026) Grad: 5.2275  LR: 0.00001547  \n","Epoch: [1][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.1380(0.5489) Grad: 3.1907  LR: 0.00001474  \n","Epoch: [1][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1286(0.5138) Grad: 4.7090  LR: 0.00001416  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.4781307876110077\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 12s (remain 10m 34s) Loss: 0.1133(0.1156) Grad: 2.7888  LR: 0.00001343  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.1179(0.1092) Grad: 4.2042  LR: 0.00001271  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.1016(0.1091) Grad: 2.3963  LR: 0.00001198  \n","Epoch: [2][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.1279(0.1059) Grad: 4.1085  LR: 0.00001126  \n","Epoch: [2][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.0824(0.1051) Grad: 1.9376  LR: 0.00001053  \n","Epoch: [2][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.0853(0.1038) Grad: 2.1230  LR: 0.00000980  \n","Epoch: [2][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1026(0.1038) Grad: 2.5393  LR: 0.00000908  \n","Epoch: [2][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1151(0.1032) Grad: 1.9860  LR: 0.00000835  \n","Epoch: [2][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.1054(0.1032) Grad: 2.0212  LR: 0.00000763  \n","Epoch: [2][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1110(0.1031) Grad: 4.0881  LR: 0.00000704  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.4505164325237274\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 12s (remain 10m 33s) Loss: 0.0883(0.0911) Grad: 2.5097  LR: 0.00000632  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.0936(0.0934) Grad: 2.3094  LR: 0.00000559  \n","Epoch: [3][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.0893(0.0932) Grad: 2.4045  LR: 0.00000487  \n","Epoch: [3][80/196] Elapsed 4m 45s (remain 6m 54s) Loss: 0.0827(0.0925) Grad: 1.8601  LR: 0.00000414  \n","Epoch: [3][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.0927(0.0915) Grad: 2.7630  LR: 0.00000341  \n","Epoch: [3][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.0770(0.0912) Grad: 1.7004  LR: 0.00000269  \n","Epoch: [3][140/196] Elapsed 8m 19s (remain 3m 19s) Loss: 0.1044(0.0930) Grad: 1.9994  LR: 0.00000196  \n","Epoch: [3][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.0978(0.0923) Grad: 1.8730  LR: 0.00000124  \n","Epoch: [3][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.0600(0.0916) Grad: 2.0002  LR: 0.00000051  \n","Epoch: [3][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1216(0.0914) Grad: 2.3942  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.4489534795284271\n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-09-16 03:53:53,804]\u001b[0m Trial 14 finished with value: 0.4489534795284271 and parameters: {'learning_rate': 1.7027701828938127e-05, 'layer_wise_learning_rate_decay': 0.8780935762069765, 'learning_rate_schduler': 'polynomial', 'reinit_layers': 1}. Best is trial 14 with value: 0.4489534795284271.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 11s (remain 10m 33s) Loss: 1.9800(2.4633) Grad: 18.8299  LR: 0.00000371  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 18s) Loss: 0.2941(1.6175) Grad: 4.2475  LR: 0.00000741  \n","Epoch: [1][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.2280(1.1579) Grad: 6.2789  LR: 0.00001112  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.1413(0.9121) Grad: 2.9273  LR: 0.00001483  \n","Epoch: [1][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1081(0.7558) Grad: 4.2234  LR: 0.00001853  \n","Epoch: [1][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1060(0.6517) Grad: 1.9142  LR: 0.00002154  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1474(0.5752) Grad: 4.3052  LR: 0.00002062  \n","Epoch: [1][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1363(0.5175) Grad: 2.5704  LR: 0.00001970  \n","Epoch: [1][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.1022(0.4732) Grad: 2.9929  LR: 0.00001877  \n","Epoch: [1][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1170(0.4437) Grad: 3.3068  LR: 0.00001803  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.46865782141685486\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 11s (remain 10m 33s) Loss: 0.1177(0.1095) Grad: 2.9268  LR: 0.00001711  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.1114(0.1127) Grad: 3.5254  LR: 0.00001618  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.1159(0.1092) Grad: 4.7581  LR: 0.00001526  \n","Epoch: [2][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.1247(0.1060) Grad: 4.7952  LR: 0.00001433  \n","Epoch: [2][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.0920(0.1061) Grad: 3.2497  LR: 0.00001341  \n","Epoch: [2][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.0972(0.1048) Grad: 3.6228  LR: 0.00001248  \n","Epoch: [2][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.0883(0.1045) Grad: 4.4634  LR: 0.00001156  \n","Epoch: [2][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.0866(0.1044) Grad: 2.3811  LR: 0.00001063  \n","Epoch: [2][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.0850(0.1043) Grad: 3.1639  LR: 0.00000971  \n","Epoch: [2][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1086(0.1040) Grad: 4.3189  LR: 0.00000897  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.457609087228775\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 12s (remain 10m 33s) Loss: 0.0938(0.0909) Grad: 1.4267  LR: 0.00000805  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 18s) Loss: 0.1166(0.0944) Grad: 2.4728  LR: 0.00000712  \n","Epoch: [3][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.1084(0.0940) Grad: 2.5473  LR: 0.00000620  \n","Epoch: [3][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.1015(0.0940) Grad: 1.9624  LR: 0.00000527  \n","Epoch: [3][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1003(0.0935) Grad: 3.2686  LR: 0.00000435  \n","Epoch: [3][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 0.0894(0.0935) Grad: 3.6684  LR: 0.00000342  \n","Epoch: [3][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.0790(0.0925) Grad: 1.7721  LR: 0.00000250  \n","Epoch: [3][160/196] Elapsed 9m 32s (remain 2m 8s) Loss: 0.0855(0.0923) Grad: 1.9995  LR: 0.00000157  \n","Epoch: [3][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.0809(0.0919) Grad: 2.2571  LR: 0.00000065  \n","Epoch: [3][196/196] Elapsed 11m 39s (remain 0m 0s) Loss: 0.0944(0.0914) Grad: 1.9904  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.45275118947029114\n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-09-16 04:31:47,643]\u001b[0m Trial 15 finished with value: 0.45275118947029114 and parameters: {'learning_rate': 2.168343678854504e-05, 'layer_wise_learning_rate_decay': 0.8571961513207124, 'learning_rate_schduler': 'polynomial', 'reinit_layers': 2}. Best is trial 14 with value: 0.4489534795284271.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 11s (remain 10m 33s) Loss: 1.6403(2.2064) Grad: 18.6384  LR: 0.00000407  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.3045(1.4024) Grad: 6.8586  LR: 0.00000814  \n","Epoch: [1][60/196] Elapsed 3m 34s (remain 8m 7s) Loss: 0.2466(1.0238) Grad: 7.4947  LR: 0.00001221  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 0.1050(0.8146) Grad: 3.3463  LR: 0.00001627  \n","Epoch: [1][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.0963(0.6806) Grad: 2.7758  LR: 0.00002034  \n","Epoch: [1][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1422(0.5868) Grad: 7.0914  LR: 0.00002365  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.0890(0.5190) Grad: 2.9379  LR: 0.00002263  \n","Epoch: [1][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1512(0.4687) Grad: 6.9918  LR: 0.00002162  \n","Epoch: [1][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.1182(0.4297) Grad: 2.8159  LR: 0.00002060  \n","Epoch: [1][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1106(0.4046) Grad: 5.8287  LR: 0.00001979  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.4756709635257721\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 12s (remain 10m 33s) Loss: 0.1288(0.1025) Grad: 4.7647  LR: 0.00001878  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.0967(0.1105) Grad: 2.5080  LR: 0.00001776  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.1079(0.1107) Grad: 4.6046  LR: 0.00001675  \n","Epoch: [2][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.0812(0.1079) Grad: 2.7773  LR: 0.00001573  \n","Epoch: [2][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1002(0.1083) Grad: 2.1494  LR: 0.00001472  \n","Epoch: [2][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.0918(0.1081) Grad: 1.3823  LR: 0.00001370  \n","Epoch: [2][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.0927(0.1066) Grad: 1.7549  LR: 0.00001269  \n","Epoch: [2][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1210(0.1064) Grad: 2.9008  LR: 0.00001167  \n","Epoch: [2][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.0890(0.1057) Grad: 2.0320  LR: 0.00001066  \n","Epoch: [2][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.0949(0.1054) Grad: 2.4464  LR: 0.00000985  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.45744648575782776\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 12s (remain 10m 34s) Loss: 0.1002(0.0947) Grad: 4.4672  LR: 0.00000883  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.0852(0.0946) Grad: 1.9895  LR: 0.00000782  \n","Epoch: [3][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.0751(0.0922) Grad: 2.7943  LR: 0.00000680  \n","Epoch: [3][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.1031(0.0936) Grad: 3.1522  LR: 0.00000579  \n","Epoch: [3][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1003(0.0944) Grad: 2.0783  LR: 0.00000477  \n","Epoch: [3][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1017(0.0933) Grad: 2.4107  LR: 0.00000376  \n","Epoch: [3][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1102(0.0947) Grad: 1.6564  LR: 0.00000274  \n","Epoch: [3][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1093(0.0943) Grad: 3.4920  LR: 0.00000173  \n","Epoch: [3][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.0728(0.0944) Grad: 1.9756  LR: 0.00000071  \n","Epoch: [3][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.0853(0.0940) Grad: 2.7310  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.454280823469162\n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-09-16 05:09:42,784]\u001b[0m Trial 16 finished with value: 0.454280823469162 and parameters: {'learning_rate': 2.3800759968097607e-05, 'layer_wise_learning_rate_decay': 0.8363549913652786, 'learning_rate_schduler': 'polynomial', 'reinit_layers': 2}. Best is trial 14 with value: 0.4489534795284271.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 12s (remain 10m 37s) Loss: 0.3605(1.4878) Grad: 9.8687  LR: 0.00001317  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 21s) Loss: 0.2062(0.9130) Grad: 3.3334  LR: 0.00002635  \n","Epoch: [1][60/196] Elapsed 3m 35s (remain 8m 8s) Loss: 0.1867(0.6777) Grad: 5.4035  LR: 0.00003952  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 0.1797(0.5494) Grad: 4.0744  LR: 0.00005270  \n","Epoch: [1][100/196] Elapsed 5m 58s (remain 5m 43s) Loss: 0.1112(0.4646) Grad: 3.0191  LR: 0.00006587  \n","Epoch: [1][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 0.0880(0.4108) Grad: 4.0185  LR: 0.00007658  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1163(0.3732) Grad: 3.6453  LR: 0.00007329  \n","Epoch: [1][160/196] Elapsed 9m 32s (remain 2m 8s) Loss: 0.1144(0.3429) Grad: 3.1968  LR: 0.00007001  \n","Epoch: [1][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.1454(0.3208) Grad: 3.1167  LR: 0.00006672  \n","Epoch: [1][196/196] Elapsed 11m 39s (remain 0m 0s) Loss: 0.0859(0.3040) Grad: 2.0352  LR: 0.00006409  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.47545692324638367\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 12s (remain 10m 34s) Loss: 0.1142(0.1098) Grad: 2.4458  LR: 0.00006081  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.1124(0.1048) Grad: 2.4328  LR: 0.00005752  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 7s) Loss: 0.1339(0.1068) Grad: 2.4571  LR: 0.00005423  \n","Epoch: [2][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 0.1097(0.1059) Grad: 4.3693  LR: 0.00005095  \n","Epoch: [2][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1115(0.1057) Grad: 2.6906  LR: 0.00004766  \n","Epoch: [2][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1089(0.1064) Grad: 1.5993  LR: 0.00004437  \n","Epoch: [2][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.0943(0.1061) Grad: 2.8761  LR: 0.00004109  \n","Epoch: [2][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1160(0.1071) Grad: 3.1131  LR: 0.00003780  \n","Epoch: [2][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.1117(0.1069) Grad: 4.1096  LR: 0.00003451  \n","Epoch: [2][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.0799(0.1060) Grad: 4.3520  LR: 0.00003189  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.47311124205589294\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 12s (remain 10m 34s) Loss: 0.0595(0.0928) Grad: 1.4375  LR: 0.00002860  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 20s) Loss: 0.0717(0.0908) Grad: 1.8806  LR: 0.00002531  \n","Epoch: [3][60/196] Elapsed 3m 35s (remain 8m 7s) Loss: 0.0850(0.0928) Grad: 1.1531  LR: 0.00002203  \n","Epoch: [3][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 0.0896(0.0922) Grad: 3.4903  LR: 0.00001874  \n","Epoch: [3][100/196] Elapsed 5m 58s (remain 5m 43s) Loss: 0.1080(0.0908) Grad: 2.9320  LR: 0.00001545  \n","Epoch: [3][120/196] Elapsed 7m 9s (remain 4m 32s) Loss: 0.0659(0.0899) Grad: 1.8172  LR: 0.00001217  \n","Epoch: [3][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.0794(0.0897) Grad: 1.8653  LR: 0.00000888  \n","Epoch: [3][160/196] Elapsed 9m 32s (remain 2m 8s) Loss: 0.0748(0.0893) Grad: 0.9444  LR: 0.00000559  \n","Epoch: [3][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.0754(0.0884) Grad: 1.9118  LR: 0.00000231  \n","Epoch: [3][196/196] Elapsed 11m 39s (remain 0m 0s) Loss: 0.0706(0.0877) Grad: 1.3742  LR: 0.00000001  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.45891880989074707\n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-09-16 05:47:37,855]\u001b[0m Trial 17 finished with value: 0.45891880989074707 and parameters: {'learning_rate': 7.707324684449108e-05, 'layer_wise_learning_rate_decay': 0.8339476407176097, 'learning_rate_schduler': 'polynomial', 'reinit_layers': 4}. Best is trial 14 with value: 0.4489534795284271.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 12s (remain 10m 36s) Loss: 2.6135(2.7590) Grad: 19.8303  LR: 0.00000063  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 20s) Loss: 2.0224(2.5872) Grad: 19.9114  LR: 0.00000127  \n","Epoch: [1][60/196] Elapsed 3m 34s (remain 8m 7s) Loss: 1.1023(2.2502) Grad: 18.6274  LR: 0.00000190  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 0.2280(1.8308) Grad: 6.7808  LR: 0.00000254  \n","Epoch: [1][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.2152(1.5102) Grad: 3.8052  LR: 0.00000317  \n","Epoch: [1][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 0.2582(1.2887) Grad: 2.1512  LR: 0.00000369  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1642(1.1284) Grad: 2.3194  LR: 0.00000353  \n","Epoch: [1][160/196] Elapsed 9m 32s (remain 2m 8s) Loss: 0.1995(1.0067) Grad: 3.5046  LR: 0.00000337  \n","Epoch: [1][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.1004(0.9093) Grad: 2.2256  LR: 0.00000321  \n","Epoch: [1][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1101(0.8450) Grad: 3.2673  LR: 0.00000309  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.5040845274925232\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 11s (remain 10m 32s) Loss: 0.1302(0.1370) Grad: 4.6800  LR: 0.00000293  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 18s) Loss: 0.1029(0.1281) Grad: 2.6356  LR: 0.00000277  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.1094(0.1236) Grad: 3.4926  LR: 0.00000261  \n","Epoch: [2][80/196] Elapsed 4m 45s (remain 6m 54s) Loss: 0.1026(0.1220) Grad: 2.7578  LR: 0.00000245  \n","Epoch: [2][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1127(0.1193) Grad: 1.9870  LR: 0.00000230  \n","Epoch: [2][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1337(0.1185) Grad: 1.4891  LR: 0.00000214  \n","Epoch: [2][140/196] Elapsed 8m 19s (remain 3m 19s) Loss: 0.0926(0.1165) Grad: 1.7667  LR: 0.00000198  \n","Epoch: [2][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.0872(0.1157) Grad: 2.6406  LR: 0.00000182  \n","Epoch: [2][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.1168(0.1156) Grad: 3.2060  LR: 0.00000166  \n","Epoch: [2][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1321(0.1152) Grad: 3.3444  LR: 0.00000154  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.47307947278022766\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 12s (remain 10m 35s) Loss: 0.1289(0.1152) Grad: 2.1874  LR: 0.00000138  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 20s) Loss: 0.1038(0.1116) Grad: 2.9944  LR: 0.00000122  \n","Epoch: [3][60/196] Elapsed 3m 35s (remain 8m 7s) Loss: 0.0955(0.1102) Grad: 2.1210  LR: 0.00000106  \n","Epoch: [3][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 0.1150(0.1095) Grad: 2.8214  LR: 0.00000090  \n","Epoch: [3][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.0709(0.1080) Grad: 2.8964  LR: 0.00000074  \n","Epoch: [3][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 0.0707(0.1064) Grad: 1.8118  LR: 0.00000059  \n","Epoch: [3][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1164(0.1070) Grad: 3.5446  LR: 0.00000043  \n","Epoch: [3][160/196] Elapsed 9m 32s (remain 2m 8s) Loss: 0.1275(0.1066) Grad: 3.0602  LR: 0.00000027  \n","Epoch: [3][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.1059(0.1067) Grad: 3.2053  LR: 0.00000011  \n","Epoch: [3][196/196] Elapsed 11m 39s (remain 0m 0s) Loss: 0.1225(0.1070) Grad: 3.1221  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.46429410576820374\n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-09-16 06:25:32,764]\u001b[0m Trial 18 finished with value: 0.46429410576820374 and parameters: {'learning_rate': 3.712200194342814e-06, 'layer_wise_learning_rate_decay': 0.873797238234208, 'learning_rate_schduler': 'polynomial', 'reinit_layers': 2}. Best is trial 14 with value: 0.4489534795284271.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 12s (remain 10m 36s) Loss: 1.6407(2.3579) Grad: 19.6849  LR: 0.00000342  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 21s) Loss: 0.1692(1.6092) Grad: 2.6283  LR: 0.00000683  \n","Epoch: [1][60/196] Elapsed 3m 35s (remain 8m 8s) Loss: 0.1660(1.1529) Grad: 1.6743  LR: 0.00001025  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 56s) Loss: 0.1442(0.9106) Grad: 2.9029  LR: 0.00001366  \n","Epoch: [1][100/196] Elapsed 5m 58s (remain 5m 44s) Loss: 0.1284(0.7553) Grad: 5.7241  LR: 0.00001708  \n","Epoch: [1][120/196] Elapsed 7m 10s (remain 4m 32s) Loss: 0.0961(0.6498) Grad: 6.2203  LR: 0.00001985  \n","Epoch: [1][140/196] Elapsed 8m 21s (remain 3m 20s) Loss: 0.0909(0.5737) Grad: 2.3681  LR: 0.00001900  \n","Epoch: [1][160/196] Elapsed 9m 33s (remain 2m 8s) Loss: 0.0931(0.5165) Grad: 2.3086  LR: 0.00001815  \n","Epoch: [1][180/196] Elapsed 10m 44s (remain 0m 57s) Loss: 0.1277(0.4712) Grad: 2.2002  LR: 0.00001730  \n","Epoch: [1][196/196] Elapsed 11m 40s (remain 0m 0s) Loss: 0.1350(0.4426) Grad: 7.0962  LR: 0.00001661  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.4734232425689697\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 12s (remain 10m 34s) Loss: 0.1437(0.1073) Grad: 5.1518  LR: 0.00001576  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.0989(0.1068) Grad: 2.0834  LR: 0.00001491  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 7s) Loss: 0.1130(0.1084) Grad: 1.8458  LR: 0.00001406  \n","Epoch: [2][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 0.0601(0.1073) Grad: 2.7524  LR: 0.00001321  \n","Epoch: [2][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.0799(0.1081) Grad: 2.2620  LR: 0.00001235  \n","Epoch: [2][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 0.1161(0.1072) Grad: 3.9139  LR: 0.00001150  \n","Epoch: [2][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.0785(0.1064) Grad: 2.0786  LR: 0.00001065  \n","Epoch: [2][160/196] Elapsed 9m 32s (remain 2m 8s) Loss: 0.0992(0.1067) Grad: 1.8319  LR: 0.00000980  \n","Epoch: [2][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.1497(0.1064) Grad: 3.3159  LR: 0.00000895  \n","Epoch: [2][196/196] Elapsed 11m 39s (remain 0m 0s) Loss: 0.0971(0.1060) Grad: 5.0950  LR: 0.00000827  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.4604106843471527\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 12s (remain 10m 37s) Loss: 0.0755(0.0964) Grad: 1.3559  LR: 0.00000741  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 21s) Loss: 0.0718(0.0967) Grad: 2.7638  LR: 0.00000656  \n","Epoch: [3][60/196] Elapsed 3m 35s (remain 8m 8s) Loss: 0.1429(0.1007) Grad: 2.4926  LR: 0.00000571  \n","Epoch: [3][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 0.0777(0.1008) Grad: 1.8732  LR: 0.00000486  \n","Epoch: [3][100/196] Elapsed 5m 58s (remain 5m 44s) Loss: 0.0910(0.0993) Grad: 2.6933  LR: 0.00000401  \n","Epoch: [3][120/196] Elapsed 7m 9s (remain 4m 32s) Loss: 0.0817(0.0985) Grad: 1.7557  LR: 0.00000315  \n","Epoch: [3][140/196] Elapsed 8m 21s (remain 3m 20s) Loss: 0.0957(0.0987) Grad: 1.7157  LR: 0.00000230  \n","Epoch: [3][160/196] Elapsed 9m 32s (remain 2m 8s) Loss: 0.0693(0.0981) Grad: 1.8602  LR: 0.00000145  \n","Epoch: [3][180/196] Elapsed 10m 44s (remain 0m 57s) Loss: 0.0841(0.0967) Grad: 2.0103  LR: 0.00000060  \n","Epoch: [3][196/196] Elapsed 11m 39s (remain 0m 0s) Loss: 0.0785(0.0962) Grad: 2.0554  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.45386430621147156\n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-09-16 07:03:31,355]\u001b[0m Trial 19 finished with value: 0.45386430621147156 and parameters: {'learning_rate': 1.9979077386798717e-05, 'layer_wise_learning_rate_decay': 0.8086618256858717, 'learning_rate_schduler': 'polynomial', 'reinit_layers': 2}. Best is trial 14 with value: 0.4489534795284271.\u001b[0m\n"]}]},{"cell_type":"code","source":["study.best_trial.params"],"metadata":{"id":"DJAJ-NHo2pB6","executionInfo":{"status":"ok","timestamp":1663311812700,"user_tz":-330,"elapsed":4,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"48b3c9ea-8def-4030-a3f6-15dd3a45bd4a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'learning_rate': 1.7027701828938127e-05,\n"," 'layer_wise_learning_rate_decay': 0.8780935762069765,\n"," 'learning_rate_schduler': 'polynomial',\n"," 'reinit_layers': 1}"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dpq4wadpEuWB"},"outputs":[],"source":["!cp -r deberta-v3-large/ /content/drive/MyDrive/Kaggle/FeedbackPrize3/"]},{"cell_type":"code","source":[],"metadata":{"id":"4ipyl8HZ1_QD"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"collapsed_sections":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"64ac505a094145be9ebb4aa12c78eec0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b8debb3cad6041a1a72010bf7264553a","IPY_MODEL_5683dbe83d3e4e49b58bb206bb11b805","IPY_MODEL_af1fda680a094b8f885b20816d4bcc18"],"layout":"IPY_MODEL_9e5f91b43fd54f00be684ad903d8098b"}},"b8debb3cad6041a1a72010bf7264553a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7926f47e14ea4703b87c26e4625b3082","placeholder":"​","style":"IPY_MODEL_fb5d955ec2234146b62daed64b3ae9f8","value":"Downloading: 100%"}},"5683dbe83d3e4e49b58bb206bb11b805":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa4a21a95e304ddaa0bc521e77073c0e","max":52,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0884a1b531f9441b98834ce5a151617c","value":52}},"af1fda680a094b8f885b20816d4bcc18":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_02c9b47a1dff47379e168cd2d03d107c","placeholder":"​","style":"IPY_MODEL_075967f3361c4900b722d2e472b79674","value":" 52.0/52.0 [00:00&lt;00:00, 1.77kB/s]"}},"9e5f91b43fd54f00be684ad903d8098b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7926f47e14ea4703b87c26e4625b3082":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb5d955ec2234146b62daed64b3ae9f8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aa4a21a95e304ddaa0bc521e77073c0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0884a1b531f9441b98834ce5a151617c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"02c9b47a1dff47379e168cd2d03d107c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"075967f3361c4900b722d2e472b79674":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a7d364461df6495285f5a3dd0981226f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_48debd180627450a8b2a050a4417ea0a","IPY_MODEL_8d683771c81d490dbcff639e76d81389","IPY_MODEL_5a6c1d41d81546ef8956fd71187d231c"],"layout":"IPY_MODEL_27894b3649504ed8a459da8e5f3bf4d1"}},"48debd180627450a8b2a050a4417ea0a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b2e1c22e9c642cd8137facec516ef68","placeholder":"​","style":"IPY_MODEL_e4f307b3b86a447987c23b707381c694","value":"Downloading: 100%"}},"8d683771c81d490dbcff639e76d81389":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_168b3154b73a4101a68f97ba6428b5a3","max":580,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b30d83cddfdf4888a0b45bf81c6e778b","value":580}},"5a6c1d41d81546ef8956fd71187d231c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4cfe4385d5e445a1a0a0f8f80a535d87","placeholder":"​","style":"IPY_MODEL_08876503e49146b4af0176f7f0b7ecbe","value":" 580/580 [00:00&lt;00:00, 22.5kB/s]"}},"27894b3649504ed8a459da8e5f3bf4d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b2e1c22e9c642cd8137facec516ef68":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4f307b3b86a447987c23b707381c694":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"168b3154b73a4101a68f97ba6428b5a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b30d83cddfdf4888a0b45bf81c6e778b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4cfe4385d5e445a1a0a0f8f80a535d87":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08876503e49146b4af0176f7f0b7ecbe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"496bef9856734852ad77be46151e1ab6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_28a18e75e04d429ab558ed5e82bda4a1","IPY_MODEL_9e7adf4908394699b585bcc823fb4499","IPY_MODEL_834d4c9887fa47c7a51611e9bbdcb09e"],"layout":"IPY_MODEL_02cb6671610c4ffa999994fe9347c3e3"}},"28a18e75e04d429ab558ed5e82bda4a1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_02a7d9b5f80a4952b8950c9ca09a497f","placeholder":"​","style":"IPY_MODEL_989bc3c3658f4714a695a16c82301886","value":"Downloading: 100%"}},"9e7adf4908394699b585bcc823fb4499":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_515f1049acb84ee39a964f282360d6f3","max":2464616,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6ea7c588729a4fd4aaf57166cedb7f91","value":2464616}},"834d4c9887fa47c7a51611e9bbdcb09e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f22846a307084a10b3209cfb4a9e7f8e","placeholder":"​","style":"IPY_MODEL_fbbe197c30504c3fb023c97b948d4edd","value":" 2.46M/2.46M [00:00&lt;00:00, 32.8MB/s]"}},"02cb6671610c4ffa999994fe9347c3e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02a7d9b5f80a4952b8950c9ca09a497f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"989bc3c3658f4714a695a16c82301886":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"515f1049acb84ee39a964f282360d6f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ea7c588729a4fd4aaf57166cedb7f91":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f22846a307084a10b3209cfb4a9e7f8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbbe197c30504c3fb023c97b948d4edd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5d238877715d4e639711071b267d8eda":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_83187bb5894c4b51b1f4e7a17021d2ac","IPY_MODEL_446e382e36f248d49244151c4708a5be","IPY_MODEL_287b1ed5cb9f4baa8a7f84ddba8c6933"],"layout":"IPY_MODEL_d8e3ff80d4534a289a7f3e30b023bff1"}},"83187bb5894c4b51b1f4e7a17021d2ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8cbe137e722c4134ab58da91c6abf901","placeholder":"​","style":"IPY_MODEL_128f6533639e406cb0d8c678e8d2cee5","value":"Downloading: 100%"}},"446e382e36f248d49244151c4708a5be":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_57d23e066ffe45368a7d34842d903511","max":873673253,"min":0,"orientation":"horizontal","style":"IPY_MODEL_97744e52494e44b0992268782a7659d2","value":873673253}},"287b1ed5cb9f4baa8a7f84ddba8c6933":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40021c5428884ccb9b4e2382b29713b3","placeholder":"​","style":"IPY_MODEL_6b4312bb28e94c4cbdf4f35ac01e69a8","value":" 874M/874M [00:13&lt;00:00, 68.4MB/s]"}},"d8e3ff80d4534a289a7f3e30b023bff1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8cbe137e722c4134ab58da91c6abf901":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"128f6533639e406cb0d8c678e8d2cee5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"57d23e066ffe45368a7d34842d903511":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97744e52494e44b0992268782a7659d2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"40021c5428884ccb9b4e2382b29713b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b4312bb28e94c4cbdf4f35ac01e69a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}