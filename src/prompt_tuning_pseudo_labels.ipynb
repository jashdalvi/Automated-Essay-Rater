{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-09T07:54:42.699874Z","iopub.status.busy":"2022-10-09T07:54:42.699271Z","iopub.status.idle":"2022-10-09T07:54:49.452812Z","shell.execute_reply":"2022-10-09T07:54:49.451741Z","shell.execute_reply.started":"2022-10-09T07:54:42.699781Z"},"id":"xAya0Z97DtCR","trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import os\n","from transformers import AutoModel, AutoConfig, AutoTokenizer, get_polynomial_decay_schedule_with_warmup, get_linear_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup\n","import pandas as pd\n","from transformers.optimization import Adafactor, AdafactorSchedule\n","from torch.cuda.amp import autocast, GradScaler\n","from sklearn.metrics import mean_squared_error\n","import random\n","import time\n","from torch.utils import checkpoint\n","import math\n","import gc\n","from text_unidecode import unidecode\n","from typing import Dict, List, Tuple\n","import codecs\n","import warnings\n","import torch.nn.functional as F\n","import joblib"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-09T07:54:49.458861Z","iopub.status.busy":"2022-10-09T07:54:49.457536Z","iopub.status.idle":"2022-10-09T07:54:49.463723Z","shell.execute_reply":"2022-10-09T07:54:49.462735Z","shell.execute_reply.started":"2022-10-09T07:54:49.458830Z"},"id":"1wMfVtAqDtCT","trusted":true},"outputs":[],"source":["import transformers\n","transformers.logging.set_verbosity_error()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-09T07:54:49.466984Z","iopub.status.busy":"2022-10-09T07:54:49.465899Z","iopub.status.idle":"2022-10-09T07:54:49.484212Z","shell.execute_reply":"2022-10-09T07:54:49.483193Z","shell.execute_reply.started":"2022-10-09T07:54:49.466930Z"},"id":"PQy468aCeRZj","trusted":true},"outputs":[],"source":["warnings.simplefilter('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-09T07:54:49.486196Z","iopub.status.busy":"2022-10-09T07:54:49.485756Z","iopub.status.idle":"2022-10-09T07:54:49.497310Z","shell.execute_reply":"2022-10-09T07:54:49.496255Z","shell.execute_reply.started":"2022-10-09T07:54:49.486151Z"},"id":"P6go0UzRDtCT","trusted":true},"outputs":[],"source":["def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    \n","seed_everything(seed=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-09T07:54:49.501183Z","iopub.status.busy":"2022-10-09T07:54:49.500780Z","iopub.status.idle":"2022-10-09T07:54:49.506053Z","shell.execute_reply":"2022-10-09T07:54:49.505012Z","shell.execute_reply.started":"2022-10-09T07:54:49.501148Z"},"id":"m7F59_xBA8lY","trusted":true},"outputs":[],"source":["#####\n","#### BEST PARAMS FOR THE DEBERTA V3 BASE MODEL\n","# params = {'learning_rate': 0.0002634969863920811, \n","#             'layer_wise_learning_rate_decay': 0.7867664854455205, \n","#             'learning_rate_schduler': 'polynomial', \n","#             'reinit_layers': 3}\n","#####"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-09T07:54:49.509808Z","iopub.status.busy":"2022-10-09T07:54:49.507411Z","iopub.status.idle":"2022-10-09T07:54:57.726382Z","shell.execute_reply":"2022-10-09T07:54:57.725359Z","shell.execute_reply.started":"2022-10-09T07:54:49.509774Z"},"id":"_L1WdNpRDtCT","trusted":true},"outputs":[],"source":["class CFG:\n","    train_file = \"../input/multi-label-stratified-folds/train_folds.csv\"\n","    fold = 0\n","    batch_size = 8\n","    num_workers = 4\n","    target_columns = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n","    hidden_dropout_prob = 0.0\n","    reinit_weights = False\n","    reinit_layers = 1\n","    lr = 0.001\n","    llrd = 0.9\n","    warmup_ratio = 0\n","    use_awp = False\n","    adv_lr = 0.0002\n","    adv_eps = 0.001\n","    model_name = \"microsoft/deberta-large\"\n","    gradient_accumulation_steps = 2\n","    max_grad_norm = 10\n","    print_freq = 20\n","    epochs = 3\n","    n_tokens = 40\n","    specific_max_len = 768 - n_tokens\n","    token_dropout = False\n","    token_dropout_prob = 0.15\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    gradient_checkpointing_enable = True\n","    save_dir = \"deberta-large\"\n","    save_model_name = \"deberta-large\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-09T07:54:57.730450Z","iopub.status.busy":"2022-10-09T07:54:57.729839Z","iopub.status.idle":"2022-10-09T07:54:57.738391Z","shell.execute_reply":"2022-10-09T07:54:57.737322Z","shell.execute_reply.started":"2022-10-09T07:54:57.730419Z"},"id":"f2V2bCiFe0ZJ","trusted":true},"outputs":[],"source":["#Preprocessing Functions\n","\n","def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n","    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n","\n","\n","def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n","    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n","\n","# Register the encoding and decoding error handlers for `utf-8` and `cp1252`.\n","codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n","codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n","\n","def resolve_encodings_and_normalize(text: str) -> str:\n","    \"\"\"Resolve the encoding problems and normalize the abnormal characters.\"\"\"\n","    text = (\n","        text.encode(\"raw_unicode_escape\")\n","        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n","        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n","        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n","    )\n","    text = unidecode(text)\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-09T07:54:57.740718Z","iopub.status.busy":"2022-10-09T07:54:57.739769Z","iopub.status.idle":"2022-10-09T07:54:57.752256Z","shell.execute_reply":"2022-10-09T07:54:57.751178Z","shell.execute_reply.started":"2022-10-09T07:54:57.740680Z"},"id":"UxvMRHfKDtCU","trusted":true},"outputs":[],"source":["#Utiliy functions \n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","        \n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n","        \n","def MCRMSE(y_trues, y_preds):\n","    scores = []\n","    idxes = y_trues.shape[1]\n","    for i in range(idxes):\n","        y_true = y_trues[:,i]\n","        y_pred = y_preds[:,i]\n","        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n","        scores.append(score)\n","    mcrmse_score = np.mean(scores)\n","    return mcrmse_score, scores\n","\n","\n","def get_score(y_trues, y_preds):\n","    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n","    return mcrmse_score, scores"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-09T07:54:57.756471Z","iopub.status.busy":"2022-10-09T07:54:57.755618Z","iopub.status.idle":"2022-10-09T07:54:57.767792Z","shell.execute_reply":"2022-10-09T07:54:57.766821Z","shell.execute_reply.started":"2022-10-09T07:54:57.756440Z"},"id":"jQ_1vncGDtCU","trusted":true},"outputs":[],"source":["class Collate:\n","    def __init__(self, tokenizer, max_length = CFG.specific_max_len):\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        \n","    def __call__(self, batch):\n","        \n","        batch_len = max([len(sample[\"ids\"]) for sample in batch])\n","        \n","        output = dict()\n","        output[\"ids\"] = [sample[\"ids\"] for sample in batch]\n","        output[\"mask\"] = [sample[\"mask\"] for sample in batch]\n","        output[\"targets\"] = [sample[\"targets\"] for sample in batch]\n","        \n","        if self.tokenizer.padding_side == \"right\":\n","            output[\"ids\"] = [s + [self.tokenizer.pad_token_id] * (batch_len - len(s)) for s in output[\"ids\"]]\n","            output[\"mask\"] = [s + [0] * (batch_len - len(s)) for s in output[\"mask\"]]\n","        else:\n","            output[\"ids\"] = [[self.tokenizer.pad_token_id] * (batch_len - len(s)) + s for s in output[\"ids\"]]\n","            output[\"mask\"] = [[0] * (batch_len - len(s)) + s for s in output[\"mask\"]]\n","            \n","            \n","        output[\"ids\"] = torch.tensor(output[\"ids\"], dtype = torch.long)\n","        output[\"mask\"] = torch.tensor(output[\"mask\"], dtype = torch.long)\n","        output[\"targets\"] = torch.tensor(output[\"targets\"], dtype = torch.float32)\n","        \n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-09T07:54:57.771348Z","iopub.status.busy":"2022-10-09T07:54:57.771074Z","iopub.status.idle":"2022-10-09T07:54:57.783387Z","shell.execute_reply":"2022-10-09T07:54:57.782348Z","shell.execute_reply.started":"2022-10-09T07:54:57.771306Z"},"id":"JghKVWQIDtCU","trusted":true},"outputs":[],"source":["class Dataset:\n","    def __init__(self, texts, targets, tokenizer, is_train = True):\n","        self.texts = texts\n","        self.targets = targets\n","        self.tokenizer = tokenizer\n","        self.is_train = is_train\n","        \n","    def __len__(self):\n","        return len(self.texts)\n","    \n","    def __getitem__(self, idx):\n","        \n","        text = self.texts[idx]\n","        targets = self.targets[idx]\n","        \n","        \n","        if CFG.specific_max_len is not None:\n","            encoding = self.tokenizer(text, add_special_tokens = True, max_length = CFG.specific_max_len, padding = False, truncation = 'longest_first')\n","        else:\n","            encoding = self.tokenizer(text, add_special_tokens = True)\n","        \n","        sample = dict()\n","\n","        if CFG.token_dropout and self.is_train:\n","            print(\"Running token dropout\")\n","            idxs = np.random.choice(np.arange(1, len(encoding[\"input_ids\"]) - 1), size = int(CFG.token_dropout_prob * len(encoding[\"input_ids\"])), replace = False)\n","            ids = np.array(encoding[\"input_ids\"])\n","            ids[idxs] = self.tokenizer.mask_token_id\n","            encoding[\"input_ids\"] = ids.tolist()\n","          \n","        sample[\"ids\"] = [50256] * CFG.n_tokens + encoding[\"input_ids\"]\n","        sample[\"mask\"] = [1] * CFG.n_tokens + encoding[\"attention_mask\"]  \n","        sample[\"targets\"] = targets\n","        \n","        return sample"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-09T07:54:57.785827Z","iopub.status.busy":"2022-10-09T07:54:57.785161Z","iopub.status.idle":"2022-10-09T07:54:57.805793Z","shell.execute_reply":"2022-10-09T07:54:57.804722Z","shell.execute_reply.started":"2022-10-09T07:54:57.785792Z"},"id":"QOtpvTlNDtCV","trusted":true},"outputs":[],"source":["class RMSELoss(nn.Module):\n","    def __init__(self, reduction='mean', eps=1e-9):\n","        super().__init__()\n","        self.mse = nn.MSELoss(reduction='none')\n","        self.reduction = reduction\n","        self.eps = eps\n","\n","    def forward(self, y_pred, y_true):\n","        loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n","        if self.reduction == 'none':\n","            loss = loss\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","        elif self.reduction == 'mean':\n","            loss = loss.mean()\n","        return loss\n","\n","class SmoothRMSELoss(nn.Module):\n","    def __init__(self, reduction='mean', eps=1e-9):\n","        super().__init__()\n","        self.mse = nn.SmoothL1Loss(reduction='none')\n","        self.reduction = reduction\n","        self.eps = eps\n","\n","    def forward(self, y_pred, y_true):\n","        loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n","        if self.reduction == 'none':\n","            loss = loss\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","        elif self.reduction == 'mean':\n","            loss = loss.mean()\n","        return loss\n","\n","class SmoothRMSEComp(nn.Module):\n","    def __init__(self, reduction='mean', eps=1e-9):\n","        super().__init__()\n","        self.mse = nn.SmoothL1Loss(reduction='none')\n","        self.reduction = reduction\n","        self.eps = eps\n","\n","    def forward(self, y_pred, y_true):\n","        loss = torch.sqrt(self.mse(y_pred, y_true).mean(dim = 0)).mean(dim = 0)\n","        return loss  \n","\n","\n","class MeanPooling(nn.Module):\n","    def __init__(self):\n","        super(MeanPooling, self).__init__()\n","        \n","    def forward(self, last_hidden_state, attention_mask):\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","        sum_mask = input_mask_expanded.sum(1)\n","        sum_mask = torch.clamp(sum_mask, min=1e-9)\n","        mean_embeddings = sum_embeddings / sum_mask\n","        return mean_embeddings\n","\n","class WeightedLayerPooling(nn.Module):\n","    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n","        super(WeightedLayerPooling, self).__init__()\n","        self.layer_start = layer_start\n","        self.num_hidden_layers = num_hidden_layers\n","        self.layer_weights = layer_weights if layer_weights is not None \\\n","            else nn.Parameter(\n","                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n","            )\n","\n","    def forward(self, features):\n","\n","        all_layer_embedding = torch.stack(features)\n","        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n","\n","        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n","        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n","\n","        return weighted_average\n","\n","\n","class AttentionHead(nn.Module):\n","    def __init__(self, in_features, hidden_dim):\n","        super().__init__()\n","        self.in_features = in_features\n","        self.middle_features = hidden_dim\n","        self.W = nn.Linear(in_features, hidden_dim)\n","        self.V = nn.Linear(hidden_dim, 1)\n","        self.out_features = hidden_dim\n","\n","    def forward(self, features):\n","        att = torch.tanh(self.W(features))\n","        score = self.V(att)\n","        attention_weights = torch.softmax(score, dim=1)\n","        context_vector = attention_weights * features\n","        context_vector = torch.sum(context_vector, dim=1)\n","\n","        return context_vector"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-09T07:54:57.807816Z","iopub.status.busy":"2022-10-09T07:54:57.807445Z","iopub.status.idle":"2022-10-09T07:54:57.823651Z","shell.execute_reply":"2022-10-09T07:54:57.822443Z","shell.execute_reply.started":"2022-10-09T07:54:57.807783Z"},"id":"BS58rpxCbEKA","trusted":true},"outputs":[],"source":["#AWP\n","class AWP:\n","    def __init__(\n","        self,\n","        model,\n","        optimizer,\n","        adv_param=\"weight\",\n","        adv_lr=1,\n","        adv_eps=0.2,\n","        start_epoch=0,\n","        adv_step=1,\n","        scaler=None\n","    ):\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.adv_param = adv_param\n","        self.adv_lr = adv_lr\n","        self.adv_eps = adv_eps\n","        self.start_epoch = start_epoch\n","        self.adv_step = adv_step\n","        self.backup = {}\n","        self.backup_eps = {}\n","        self.scaler = scaler\n","\n","    def attack_backward(self, x, y, attention_mask,epoch):\n","        if (self.adv_lr == 0) or (epoch < self.start_epoch):\n","            return None\n","\n","        self._save() \n","        for i in range(self.adv_step):\n","            self._attack_step() \n","            with torch.cuda.amp.autocast():\n","                adv_loss, tr_logits = self.model(ids=x, mask=attention_mask, targets=y)\n","                adv_loss = adv_loss.mean()\n","            self.optimizer.zero_grad()\n","            self.scaler.scale(adv_loss).backward()\n","            \n","        self._restore()\n","\n","    def _attack_step(self):\n","        e = 1e-6\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad and param.grad is not None and self.adv_param in name:\n","                norm1 = torch.norm(param.grad)\n","                norm2 = torch.norm(param.data.detach())\n","                if norm1 != 0 and not torch.isnan(norm1):\n","                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n","                    param.data.add_(r_at)\n","                    param.data = torch.min(\n","                        torch.max(param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n","                    )\n","                # param.data.clamp_(*self.backup_eps[name])\n","\n","    def _save(self):\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad and param.grad is not None and self.adv_param in name:\n","                if name not in self.backup:\n","                    self.backup[name] = param.data.clone()\n","                    grad_eps = self.adv_eps * param.abs().detach()\n","                    self.backup_eps[name] = (\n","                        self.backup[name] - grad_eps,\n","                        self.backup[name] + grad_eps,\n","                    )\n","\n","    def _restore(self,):\n","        for name, param in self.model.named_parameters():\n","            if name in self.backup:\n","                param.data = self.backup[name]\n","        self.backup = {}\n","        self.backup_eps = {}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-09T07:54:57.825715Z","iopub.status.busy":"2022-10-09T07:54:57.825371Z","iopub.status.idle":"2022-10-09T07:54:57.839141Z","shell.execute_reply":"2022-10-09T07:54:57.838217Z","shell.execute_reply.started":"2022-10-09T07:54:57.825682Z"},"id":"O5F8bmMAOGpe","trusted":true},"outputs":[],"source":["#Getting the prompt tuning soft embeddings\n","class SoftEmbedding(nn.Module):\n","    def __init__(self, \n","                wte: nn.Embedding,\n","                n_tokens: int = 10, \n","                random_range: float = 0.5,\n","                initialize_from_vocab: bool = True):\n","        \"\"\"appends learned embedding to \n","        Args:\n","            wte (nn.Embedding): original transformer word embedding\n","            n_tokens (int, optional): number of tokens for task. Defaults to 10.\n","            random_range (float, optional): range to init embedding (if not initialize from vocab). Defaults to 0.5.\n","            initialize_from_vocab (bool, optional): initalizes from default vocab. Defaults to True.\n","        \"\"\"\n","        super(SoftEmbedding, self).__init__()\n","        self.wte = wte\n","        self.n_tokens = n_tokens\n","        self.learned_embedding = nn.parameter.Parameter(self.initialize_embedding(wte,\n","                                                                               n_tokens, \n","                                                                               random_range, \n","                                                                               initialize_from_vocab))\n","            \n","    def initialize_embedding(self, \n","                             wte: nn.Embedding,\n","                             n_tokens: int = 10, \n","                             random_range: float = 0.5, \n","                             initialize_from_vocab: bool = True):\n","        \"\"\"initializes learned embedding\n","        Args:\n","            same as __init__\n","        Returns:\n","            torch.float: initialized using original schemes\n","        \"\"\"\n","        if initialize_from_vocab:\n","            return self.wte.weight[:n_tokens].clone().detach()\n","        return torch.FloatTensor(n_tokens, wte.weight.size(1)).uniform_(-random_range, random_range)\n","            \n","    def forward(self, tokens):\n","        \"\"\"run forward pass\n","        Args:\n","            tokens (torch.long): input tokens before encoding\n","        Returns:\n","            torch.float: encoding of text concatenated with learned task specifc embedding\n","        \"\"\"\n","        input_embedding = self.wte(tokens[:, self.n_tokens:])\n","        learned_embedding = self.learned_embedding.repeat(input_embedding.size(0), 1, 1)\n","        return torch.cat([learned_embedding, input_embedding], 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-09T07:54:57.844448Z","iopub.status.busy":"2022-10-09T07:54:57.844167Z","iopub.status.idle":"2022-10-09T07:54:57.871716Z","shell.execute_reply":"2022-10-09T07:54:57.870723Z","shell.execute_reply.started":"2022-10-09T07:54:57.844411Z"},"id":"8lkexNsJDtCV","trusted":true},"outputs":[],"source":["\n","class Model(nn.Module):\n","    def __init__(self, model_name):\n","        super(Model, self).__init__()\n","        \n","        self.model_name = model_name\n","\n","        hidden_dropout_prob: float = CFG.hidden_dropout_prob\n","        layer_norm_eps: float = 1e-7\n","\n","        config = AutoConfig.from_pretrained(model_name)\n","\n","        config.update(\n","            {\n","                \"output_hidden_states\": True,\n","                \"hidden_dropout_prob\": hidden_dropout_prob,\n","                \"attention_probs_dropout_prob\" : hidden_dropout_prob,\n","                \"layer_norm_eps\": layer_norm_eps,\n","                \"add_pooling_layer\": False,\n","                \"num_labels\": 6,\n","            }\n","        )\n","        \n","        self.config = config\n","        \n","        #Using Prompt Tuning\n","        self.transformer = AutoModel.from_pretrained(model_name, config=config)\n","        s_wte = SoftEmbedding(self.transformer.get_input_embeddings(), \n","                      n_tokens=CFG.n_tokens, \n","                      initialize_from_vocab=True)\n","        self.transformer.set_input_embeddings(s_wte)\n","        \n","        if CFG.gradient_checkpointing_enable:\n","            self.transformer.gradient_checkpointing_enable()\n","        \n","        self.layer_nums = list(range(self.config.num_hidden_layers - 6, self.config.num_hidden_layers))\n","            \n","        self.freeze()\n","        \n","        self.output = nn.Linear(config.hidden_size, 6)\n","        self.loss = nn.SmoothL1Loss(reduction = \"mean\")\n","\n","        if CFG.reinit_weights:\n","            self.init_weights_(CFG.reinit_layers)\n","\n","    def init_weights_(self, reinit_layers):\n","        for layer in self.transformer.encoder.layer[-reinit_layers:]:\n","            for module in layer.modules():\n","                if isinstance(module, nn.Linear):\n","                    module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","                    if module.bias is not None:\n","                        module.bias.data.zero_()\n","                elif isinstance(module, nn.Embedding):\n","                    module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","                    if module.padding_idx is not None:\n","                        module.weight.data[module.padding_idx].zero_()\n","                elif isinstance(module, nn.LayerNorm):\n","                    module.bias.data.zero_()\n","                    module.weight.data.fill_(1.0)\n","\n","    def get_grouped_llrd_optimizer_scheduler(self, num_train_steps):\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","        top_params = [('learned_embedding', self.transformer.embeddings.word_embeddings.learned_embedding)] #+ list(self.output.named_parameters())\n","      # initialize lr for task specific layer\n","        optimizer_grouped_parameters = [\n","              {\n","                  \"params\": [p for n, p in top_params if not any(nd in n for nd in no_decay)],\n","                  \"weight_decay\": 0.001,\n","                  \"lr\": CFG.lr,\n","              },\n","              {\n","                  \"params\": [p for n, p in top_params if any(nd in n for nd in no_decay)],\n","                  \"weight_decay\": 0.0,\n","                  \"lr\": CFG.lr,\n","              },\n","          ]\n","      # initialize lrs for every layer\n","        num_layers = self.config.num_hidden_layers\n","        layers = [self.transformer.embeddings] + list(self.transformer.encoder.layer)\n","        layers.reverse()\n","        layers = layers[:6]\n","        lr = 2e-5\n","        for layer in layers:\n","            lr *= CFG.llrd\n","            optimizer_grouped_parameters += [\n","              {\n","                  \"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n","                  \"weight_decay\": 0.001,\n","                  \"lr\": lr,\n","              },\n","              {\n","                  \"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n","                  \"weight_decay\": 0.0,\n","                  \"lr\": lr,\n","              },\n","          ]\n","        opt = torch.optim.AdamW(optimizer_grouped_parameters)\n","        sch = get_polynomial_decay_schedule_with_warmup(\n","          opt,\n","          num_warmup_steps=int(num_train_steps * CFG.warmup_ratio),\n","          num_training_steps=num_train_steps,\n","          last_epoch=-1,\n","          )\n","        return opt, sch\n","    \n","    \n","    def freeze(self):\n","        for n,param in self.transformer.named_parameters():\n","            if \"learned_embedding\" not in n and not any([str(ln) in n for ln in self.layer_nums]):\n","                param.requires_grad = False\n","        \n","    \n","    def get_optimizer_scheduler(self, num_train_steps):\n","        param_optimizer = [('learned_embedding', self.transformer.embeddings.word_embeddings.learned_embedding)] + list(self.output.named_parameters())\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","        optimizer_parameters = [\n","            {\n","                \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.001,\n","            },\n","            {\n","                \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","            },\n","        ]\n","\n","        transformer_parms = [(n,p) for n,p in self.transformer.named_parameters() if any([str(ln) in n for ln in self.layer_nums])] #+ [(n,p) for n,p in self.named_parameters() if n in [\"transformer.encoder.LayerNorm.weight\",\"transformer.encoder.LayerNorm.bias\"]]\n","        optimizer_parameters += [\n","            {\n","                \"params\": [p for n, p in transformer_parms if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.001,\n","                \"lr\" : 2e-5\n","            },\n","            {\n","                \"params\": [p for n, p in transformer_parms if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","                \"lr\" : 2e-5\n","            },\n","        ]\n","        opt = torch.optim.AdamW(optimizer_parameters, lr=CFG.lr)\n","\n","        sch = get_polynomial_decay_schedule_with_warmup(\n","          opt,\n","          num_warmup_steps=int(num_train_steps * CFG.warmup_ratio),\n","          num_training_steps=num_train_steps,\n","          last_epoch=-1,\n","          )\n","        return opt, sch\n","\n","    def forward(self, ids, mask, token_type_ids=None, targets=None):\n","        if token_type_ids is not None:\n","            transformer_out = self.transformer( ids, mask, token_type_ids )\n","        else:\n","            transformer_out = self.transformer(ids, mask)\n","        sequence_output = transformer_out.last_hidden_state[:,0,:]\n","        logits = self.output(sequence_output)\n","        loss = self.loss(logits, targets)\n","\n","        return loss, logits"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-09T07:54:57.875016Z","iopub.status.busy":"2022-10-09T07:54:57.874709Z","iopub.status.idle":"2022-10-09T07:54:57.894435Z","shell.execute_reply":"2022-10-09T07:54:57.893622Z","shell.execute_reply.started":"2022-10-09T07:54:57.874986Z"},"id":"PojCbT3GDtCW","trusted":true},"outputs":[],"source":["def train(epoch, model, train_loader, valid_loader, optimizer, scheduler, device, awp, scaler, best_loss, fold):\n","    model.train()\n","    losses = AverageMeter()\n","    start = end = time.time()\n","    val_steps = len(train_loader) // 1\n","    for step, x in enumerate(train_loader):\n","        for k,v in x.items():\n","            x[k] = v.to(device)\n","        \n","        with autocast():    \n","            loss, logits = model(**x)\n","            \n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        \n","        losses.update(loss.item() * CFG.gradient_accumulation_steps, CFG.batch_size)\n","        scaler.scale(loss).backward(create_graph = True)\n","        if CFG.use_awp:\n","            awp.attack_backward(x[\"ids\"],x[\"targets\"],x[\"mask\"],epoch)\n","        \n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.unscale_(optimizer)\n","            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","            if scheduler is not None:\n","                scheduler.step()\n","        end = time.time()\n","        \n","        if ((step + 1) % CFG.print_freq == 0) or (step == (len(train_loader)-1)):\n","            print('Epoch: [{0}][{1}/{2}] '\n","                  'Elapsed {remain:s} '\n","                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                  'Grad: {grad_norm:.4f}  '\n","                  'LR: {lr:.8f}  '\n","                  .format(epoch+1, step + 1, len(train_loader), \n","                          remain=timeSince(start, float(step+1)/len(train_loader)),\n","                          loss=losses,\n","                          grad_norm= grad_norm,\n","                          lr=scheduler.get_lr()[0] if scheduler is not None else CFG.lr))\n","        \n","        if ((step + 1) % val_steps == 0) or ((step + 1) == len(train_loader)):\n","            print(\"\\nVALID LOOP\\n\")\n","            valid_loss = valid(epoch, model, valid_loader, device)\n","            print(f\"\\nThe valid loss for the current epoch is {valid_loss}\\n\")\n","            torch.cuda.empty_cache()\n","            gc.collect()\n","            if valid_loss < best_loss:\n","                best_loss = valid_loss\n","                if CFG.save_dir is not None:\n","                    if not os.path.exists(CFG.save_dir):\n","                        os.mkdir(CFG.save_dir)\n","                    save_path = os.path.join(CFG.save_dir, f\"{CFG.save_model_name}_fold_{fold}.pth\")\n","                else:\n","                    save_path = f\"{CFG.save_model_name}_fold_{fold}.pth\"\n","                torch.save(model.state_dict(), save_path)\n","        model.train()\n","\n","def valid(epoch, model, valid_loader, device):\n","    model.eval()\n","    all_targets = []\n","    all_outputs = []\n","    losses = AverageMeter()\n","    with torch.no_grad():\n","        for step, x in enumerate(valid_loader):\n","\n","            for k, v in x.items():\n","                x[k] = v.to(device)\n","          \n","            loss, logits = model(**x)\n","\n","            losses.update(loss.item(), CFG.batch_size)\n","            targets = x[\"targets\"].cpu().numpy()\n","            outputs = logits.cpu().numpy()\n","\n","            all_targets.append(targets)\n","            all_outputs.append(np.clip(outputs, 1.0, 5.0))\n","\n","            if ((step + 1) % CFG.print_freq == 0) or (step == (len(valid_loader)-1)):\n","                print('Epoch: [{0}][{1}/{2}] '\n","                      'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                      .format(epoch+1, step + 1, len(valid_loader), loss=losses))\n","        \n","    \n","    all_targets = np.vstack(all_targets)\n","    all_outputs = np.vstack(all_outputs)\n","    loss = get_score(all_targets, all_outputs)[0]\n","    \n","    del all_targets, all_outputs;\n","    return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-09T07:54:57.896531Z","iopub.status.busy":"2022-10-09T07:54:57.895862Z","iopub.status.idle":"2022-10-09T07:54:57.908384Z","shell.execute_reply":"2022-10-09T07:54:57.907507Z","shell.execute_reply.started":"2022-10-09T07:54:57.896495Z"},"trusted":true},"outputs":[],"source":["def create_pl_df( fold):\n","    df = pd.read_csv('../input/pseudo-label-data/pl_df.csv')\n","    indexes = joblib.load('../input/pseudo-filtered-jash/indexes.pkl')\n","    \n","#     x = 0\n","    \n","#     for fold in range(5):\n","#         x += torch.load(f'../input/generate-pseudo-labels/fold_{fold}/valid/model_outputs.pth')\n","    \n","#     x /= 5\n","    df = df[indexes]\n","    df[ CFG.target_columns ] = np.clip(torch.load(f'../input/ensemble-pseudo-labels-jash/model_outputs_{fold}.pth'),1.0,5.0)\n","    \n","    \n","    \n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-09T07:54:57.910341Z","iopub.status.busy":"2022-10-09T07:54:57.909990Z","iopub.status.idle":"2022-10-09T07:54:57.924778Z","shell.execute_reply":"2022-10-09T07:54:57.923983Z","shell.execute_reply.started":"2022-10-09T07:54:57.910306Z"},"id":"zovUkUouDtCW","trusted":true},"outputs":[],"source":["def main(fold):\n","    torch.cuda.empty_cache()\n","    df = pd.read_csv(CFG.train_file)\n","    pl_df = create_pl_df( fold)\n","    \n","    train_df = df.loc[df.kfold != fold]\n","    valid_df = df.loc[df.kfold == fold]\n","    \n","    common_ids = list(set(pl_df.ids.to_list()).intersection(df.text_id.to_list()))\n","    pl_df = pl_df.loc[~pl_df.ids.isin(common_ids)]\n","    \n","    pl_texts = pl_df.texts.apply(resolve_encodings_and_normalize).to_list()\n","    train_texts = train_df[\"full_text\"].apply(resolve_encodings_and_normalize).to_list()\n","    valid_texts = valid_df[\"full_text\"].apply(resolve_encodings_and_normalize).to_list()\n","    \n","    pl_targets = pl_df[CFG.target_columns].values.tolist()\n","    train_targets = train_df[CFG.target_columns].values.tolist()\n","    valid_targets = valid_df[CFG.target_columns].values.tolist()\n","    \n","    train_ds = Dataset(train_texts + pl_texts, train_targets + pl_targets, CFG.tokenizer)\n","    valid_ds = Dataset(valid_texts, valid_targets, CFG.tokenizer, is_train = False)\n","\n","    collate_fn = Collate(CFG.tokenizer)\n","    train_loader = torch.utils.data.DataLoader(train_ds, batch_size = CFG.batch_size, shuffle = True, collate_fn = collate_fn, num_workers = CFG.num_workers)\n","    valid_loader = torch.utils.data.DataLoader(valid_ds, batch_size = CFG.batch_size, shuffle = False, collate_fn = collate_fn, num_workers = CFG.num_workers)\n","    \n","    model = Model(CFG.model_name)\n","    num_train_steps = int(len(train_ds) / CFG.batch_size / CFG.gradient_accumulation_steps * CFG.epochs)\n","    optimizer, scheduler = model.get_optimizer_scheduler(num_train_steps)\n","    \n","    model = model.to(CFG.device)\n","    best_loss = np.inf\n","    scaler = GradScaler()\n","    if CFG.use_awp:\n","        print('ENABLE AWP')\n","        awp = AWP(model,\n","          optimizer,\n","          adv_lr=CFG.adv_lr,\n","          adv_eps=CFG.adv_eps,\n","          start_epoch=3,\n","          scaler=scaler)\n","    else:\n","        awp = None\n","    for epoch in range(CFG.epochs):\n","        print(\"\\nTRAIN LOOP\\n\")\n","        train(epoch, model, train_loader, valid_loader, optimizer, scheduler, CFG.device, awp, scaler, best_loss, fold) # None for the scaler parameter\n","        # print(\"\\nVALID LOOP\\n\")\n","        # valid_loss = valid(epoch, model, valid_loader, CFG.device)\n","        \n","        # print(f\"\\nThe valid loss for the current epoch is {valid_loss}\\n\")\n","        # torch.cuda.empty_cache()\n","        # gc.collect()\n","        # if valid_loss < best_loss:\n","        #     best_loss = valid_loss\n","        #     if CFG.save_dir is not None:\n","        #       if not os.path.exists(CFG.save_dir):\n","        #         os.mkdir(CFG.save_dir)\n","        #       save_path = os.path.join(CFG.save_dir, f\"{CFG.save_model_name}_fold_{fold}.pth\")\n","        #     else:\n","        #       save_path = f\"{CFG.save_model_name}_fold_{fold}.pth\"\n","        #     torch.save(model.state_dict(), save_path)\n","            \n","    del model, optimizer, scheduler, train_loader, valid_loader, train_df, valid_df;\n","    gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-09T07:54:57.927627Z","iopub.status.busy":"2022-10-09T07:54:57.927376Z","iopub.status.idle":"2022-10-09T07:58:01.214774Z","shell.execute_reply":"2022-10-09T07:58:01.213017Z","shell.execute_reply.started":"2022-10-09T07:54:57.927604Z"},"id":"ndUmqkakDtCW","outputId":"ef3dfcac-ecb5-4002-ae1b-e6ae57a73aa1","trusted":true},"outputs":[],"source":["for fold in range(5):\n","    print(\"-----\"*20)\n","    print(f\"\\nRUNNING FOLD {fold}\\n\")\n","    print(\"-----\"*20)\n","    main(fold)\n","    gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-q1YivBNTeOa"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":4}
