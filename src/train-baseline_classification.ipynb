{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10543,"status":"ok","timestamp":1666192763562,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"},"user_tz":-330},"id":"1ujo2PRQEEtA","outputId":"0e8102ef-8e96-4b02-bed1-ce74ab70086f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n","\u001b[K     |████████████████████████████████| 5.3 MB 15.6 MB/s \n","\u001b[?25hCollecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 72.7 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n","\u001b[K     |████████████████████████████████| 163 kB 70.2 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 52.2 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Installing collected packages: tokenizers, huggingface-hub, transformers, sentencepiece\n","Successfully installed huggingface-hub-0.10.1 sentencepiece-0.1.97 tokenizers-0.13.1 transformers-4.23.1\n"]}],"source":["!pip install transformers sentencepiece"]},{"cell_type":"code","source":["!pip install optuna"],"metadata":{"id":"6r2H4fC3ygKZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666192772269,"user_tz":-330,"elapsed":8719,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}},"outputId":"253fb648-e07a-4eea-8a4d-634d26f1e000"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting optuna\n","  Downloading optuna-3.0.3-py3-none-any.whl (348 kB)\n","\u001b[K     |████████████████████████████████| 348 kB 13.7 MB/s \n","\u001b[?25hCollecting cmaes>=0.8.2\n","  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n","Requirement already satisfied: importlib-metadata<5.0.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (4.13.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n","Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.41)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.64.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (6.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.6)\n","Collecting colorlog\n","  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied: scipy<1.9.0,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.7.3)\n","Collecting cliff\n","  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n","\u001b[K     |████████████████████████████████| 81 kB 10.0 MB/s \n","\u001b[?25hCollecting alembic>=1.5.0\n","  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n","\u001b[K     |████████████████████████████████| 209 kB 86.5 MB/s \n","\u001b[?25hCollecting Mako\n","  Downloading Mako-1.2.3-py3-none-any.whl (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 7.2 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic>=1.5.0->optuna) (5.10.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5.0.0->optuna) (4.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5.0.0->optuna) (3.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.3.0->optuna) (1.1.3.post0)\n","Collecting stevedore>=2.0.1\n","  Downloading stevedore-3.5.1-py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 7.0 MB/s \n","\u001b[?25hCollecting cmd2>=1.0.0\n","  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n","\u001b[K     |████████████████████████████████| 147 kB 74.9 MB/s \n","\u001b[?25hCollecting autopage>=0.4.0\n","  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n","Collecting pbr!=2.1.0,>=2.0.0\n","  Downloading pbr-5.10.0-py2.py3-none-any.whl (112 kB)\n","\u001b[K     |████████████████████████████████| 112 kB 70.8 MB/s \n","\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.4.1)\n","Collecting pyperclip>=1.6\n","  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n","Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (22.1.0)\n","Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n","Building wheels for collected packages: pyperclip\n","  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=a20cf3c764415646ee30c9d5eca363dfc54ae0b8309fc94861eb137d67cd6f5a\n","  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n","Successfully built pyperclip\n","Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n","Successfully installed Mako-1.2.3 alembic-1.8.1 autopage-0.5.1 cliff-3.10.1 cmaes-0.8.2 cmd2-2.4.2 colorlog-6.7.0 optuna-3.0.3 pbr-5.10.0 pyperclip-1.8.2 stevedore-3.5.1\n"]}]},{"cell_type":"code","execution_count":1,"metadata":{"id":"xAya0Z97DtCR","executionInfo":{"status":"ok","timestamp":1666193387555,"user_tz":-330,"elapsed":3607,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}},"colab":{"base_uri":"https://localhost:8080/","height":105,"referenced_widgets":["78b3947044744aef8bc663e4a8459a6e","3899339690284465afd9c4c6a8a3d898","2253ade1fe794643a0f05b5125768206","15f749cb359d48c69ecd38b72c3e9bb4","086a3aa51f024b67b7f5679e0f7783e8","dc7f10702c73405d92715da7a167a1d4","554866f838d242f7987def818c5463e7","c06fbc7ab3754edba4935ad0830aaab1","426520ce2a184502be147cb494300c1c","05318d4db2834c8f8b17235554d3781c","01f0baa5ceec4f4baaa97c0024d8b9e3"]},"outputId":"4cd6f96d-21a9-4728-ce58-f7f811c71bcc"},"outputs":[{"output_type":"stream","name":"stderr","text":["The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"]},{"output_type":"stream","name":"stdout","text":["Moving 0 files to the new cache system\n"]},{"output_type":"display_data","data":{"text/plain":["0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78b3947044744aef8bc663e4a8459a6e"}},"metadata":{}}],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import os\n","from transformers import AutoModel, AutoConfig, AutoTokenizer, get_cosine_schedule_with_warmup, get_polynomial_decay_schedule_with_warmup, get_linear_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup\n","import pandas as pd\n","from torch.cuda.amp import autocast, GradScaler\n","from sklearn.metrics import mean_squared_error\n","import random\n","import time\n","from torch.utils import checkpoint\n","import math\n","import gc\n","from text_unidecode import unidecode\n","from typing import Dict, List, Tuple\n","import codecs\n","import warnings\n","import torch.nn.functional as F\n","import optuna"]},{"cell_type":"code","source":["#######\n","#### BEST PARAMS FOR THE DEBERTA V3 LARGE MODEL\n","# {'learning_rate': 1.7027701828938127e-05,\n","#  'layer_wise_learning_rate_decay': 0.8780935762069765,\n","#  'learning_rate_schduler': 'polynomial',\n","#  'reinit_layers': 1}\n","#######\n","\n","\n","#####\n","#### BEST PARAMS FOR THE DEBERTA V3 BASE MODEL\n","# {'learning_rate': 0.0002634969863920811, \n","#  'layer_wise_learning_rate_decay': 0.7867664854455205, \n","#  'learning_rate_schduler': 'polynomial', \n","#  'reinit_layers': 3}\n","#####"],"metadata":{"id":"G5YXOkv0HY79","executionInfo":{"status":"ok","timestamp":1666193387556,"user_tz":-330,"elapsed":6,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"1wMfVtAqDtCT","executionInfo":{"status":"ok","timestamp":1666193387556,"user_tz":-330,"elapsed":5,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}}},"outputs":[],"source":["import transformers\n","transformers.logging.set_verbosity_error()"]},{"cell_type":"code","source":["warnings.simplefilter('ignore')"],"metadata":{"id":"PQy468aCeRZj","executionInfo":{"status":"ok","timestamp":1666193387557,"user_tz":-330,"elapsed":6,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5128,"status":"ok","timestamp":1666193392680,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"},"user_tz":-330},"id":"j2qok14QDua8","outputId":"7ca4fc28-4d4f-461f-87f6-dde7e938e343"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"P6go0UzRDtCT","executionInfo":{"status":"ok","timestamp":1666193392680,"user_tz":-330,"elapsed":4,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}}},"outputs":[],"source":["def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    \n","seed_everything(seed=42)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":959,"status":"ok","timestamp":1666193393636,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"},"user_tz":-330},"id":"_L1WdNpRDtCT"},"outputs":[],"source":["class CFG:\n","    train_file = \"/content/drive/MyDrive/Kaggle/FeedbackPrize3/train_folds.csv\"\n","    fold = 0\n","    batch_size = 8\n","    num_workers = 4\n","    target_columns = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n","    hidden_dropout_prob = 0.0\n","    reinit_weights = False\n","    reinit_layers = 1\n","    lr = 2e-5\n","    llrd = 0.9\n","    warmup_ratio = 0.0\n","    use_awp = False\n","    adv_lr = 0.0002\n","    adv_eps = 0.001\n","    model_name = \"microsoft/deberta-v3-large\"\n","    gradient_accumulation_steps = 2\n","    max_grad_norm = 10\n","    print_freq = 20\n","    epochs = 3\n","    specific_max_len = 512\n","    token_dropout = False\n","    token_dropout_prob = 0.1\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    gradient_checkpointing_enable = True\n","    save_dir = \"deberta-v3-large\"\n","    save_model_name = \"deberta-v3-large\""]},{"cell_type":"code","execution_count":8,"metadata":{"id":"f2V2bCiFe0ZJ","executionInfo":{"status":"ok","timestamp":1666193393636,"user_tz":-330,"elapsed":9,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}}},"outputs":[],"source":["#Preprocessing Functions\n","\n","def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n","    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n","\n","\n","def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n","    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n","\n","# Register the encoding and decoding error handlers for `utf-8` and `cp1252`.\n","codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n","codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n","\n","def resolve_encodings_and_normalize(text: str) -> str:\n","    \"\"\"Resolve the encoding problems and normalize the abnormal characters.\"\"\"\n","    text = (\n","        text.encode(\"raw_unicode_escape\")\n","        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n","        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n","        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n","    )\n","    text = unidecode(text)\n","    return text"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"UxvMRHfKDtCU","executionInfo":{"status":"ok","timestamp":1666193393637,"user_tz":-330,"elapsed":9,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}}},"outputs":[],"source":["#Utiliy functions \n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","        \n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n","        \n","def MCRMSE(y_trues, y_preds):\n","    ## Mapping real numbers to classes\n","    classes = [1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n","    class_mapping = dict()\n","    for i, class_ in enumerate(classes):\n","      class_mapping[class_] = i\n","    \n","    inv_class_mapping = {v : k for k, v in class_mapping.items()}\n","    scores = []\n","    idxes = y_trues.shape[1]\n","    for i in range(idxes):\n","        y_true = np.vectorize(inv_class_mapping.get)(y_trues[:,i])\n","        y_pred = np.vectorize(inv_class_mapping.get)(y_preds[:,i])\n","        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n","        scores.append(score)\n","    mcrmse_score = np.mean(scores)\n","    return mcrmse_score, scores\n","\n","\n","def get_score(y_trues, y_preds):\n","    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n","    return mcrmse_score, scores"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"jQ_1vncGDtCU","executionInfo":{"status":"ok","timestamp":1666193393637,"user_tz":-330,"elapsed":8,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}}},"outputs":[],"source":["class Collate:\n","    def __init__(self, tokenizer, max_length = CFG.specific_max_len):\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        \n","    def __call__(self, batch):\n","        \n","        batch_len = max([len(sample[\"ids\"]) for sample in batch])\n","        \n","        output = dict()\n","        output[\"ids\"] = [sample[\"ids\"] for sample in batch]\n","        output[\"mask\"] = [sample[\"mask\"] for sample in batch]\n","        output[\"targets\"] = [sample[\"targets\"] for sample in batch]\n","        \n","        if self.tokenizer.padding_side == \"right\":\n","            output[\"ids\"] = [s + [self.tokenizer.pad_token_id] * (batch_len - len(s)) for s in output[\"ids\"]]\n","            output[\"mask\"] = [s + [0] * (batch_len - len(s)) for s in output[\"mask\"]]\n","        else:\n","            output[\"ids\"] = [[self.tokenizer.pad_token_id] * (batch_len - len(s)) + s for s in output[\"ids\"]]\n","            output[\"mask\"] = [[0] * (batch_len - len(s)) + s for s in output[\"mask\"]]\n","            \n","            \n","        output[\"ids\"] = torch.tensor(output[\"ids\"], dtype = torch.long)\n","        output[\"mask\"] = torch.tensor(output[\"mask\"], dtype = torch.long)\n","        output[\"targets\"] = torch.tensor(output[\"targets\"], dtype = torch.long)\n","        \n","        return output"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"JghKVWQIDtCU","executionInfo":{"status":"ok","timestamp":1666193393637,"user_tz":-330,"elapsed":8,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}}},"outputs":[],"source":["class Dataset:\n","    def __init__(self, texts, targets, tokenizer, is_train = True):\n","        self.texts = texts\n","        self.targets = targets\n","        self.tokenizer = tokenizer\n","        self.is_train = is_train\n","        \n","    def __len__(self):\n","        return len(self.texts)\n","    \n","    def __getitem__(self, idx):\n","        \n","        text = self.texts[idx]\n","        targets = self.targets[idx]\n","        \n","        \n","        if CFG.specific_max_len is not None:\n","          encoding = self.tokenizer(text, add_special_tokens = True, max_length = CFG.specific_max_len, padding = False, truncation = 'longest_first')\n","        else:\n","          encoding = self.tokenizer(text, add_special_tokens = True)\n","        \n","        sample = dict()\n","\n","        if CFG.token_dropout and self.is_train:\n","          idxs = np.random.choice(np.arange(1, len(encoding[\"input_ids\"]) - 1), size = int(CFG.token_dropout_prob * len(encoding[\"input_ids\"])), replace = False)\n","          ids = np.array(encoding[\"input_ids\"])\n","          ids[idxs] = self.tokenizer.mask_token_id\n","          encoding[\"input_ids\"] = ids.tolist()\n","          \n","        \n","        sample[\"ids\"] = encoding[\"input_ids\"]\n","        sample[\"mask\"] = encoding[\"attention_mask\"]  \n","        sample[\"targets\"] = targets\n","        \n","        return sample"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"QOtpvTlNDtCV","executionInfo":{"status":"ok","timestamp":1666193393637,"user_tz":-330,"elapsed":8,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}}},"outputs":[],"source":["class RMSELoss(nn.Module):\n","    def __init__(self, reduction='mean', eps=1e-9):\n","        super().__init__()\n","        self.mse = nn.MSELoss(reduction='none')\n","        self.reduction = reduction\n","        self.eps = eps\n","\n","    def forward(self, y_pred, y_true):\n","        loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n","        if self.reduction == 'none':\n","            loss = loss\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","        elif self.reduction == 'mean':\n","            loss = loss.mean()\n","        return loss\n","\n","class SmoothRMSELoss(nn.Module):\n","    def __init__(self, reduction='mean', eps=1e-9):\n","        super().__init__()\n","        self.mse = nn.SmoothL1Loss(reduction='none')\n","        self.reduction = reduction\n","        self.eps = eps\n","\n","    def forward(self, y_pred, y_true):\n","        loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n","        if self.reduction == 'none':\n","            loss = loss\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","        elif self.reduction == 'mean':\n","            loss = loss.mean()\n","        return loss\n","\n","class SmoothRMSEComp(nn.Module):\n","  def __init__(self, reduction='mean', eps=1e-9):\n","        super().__init__()\n","        self.mse = nn.SmoothL1Loss(reduction='none')\n","        self.reduction = reduction\n","        self.eps = eps\n","\n","  def forward(self, y_pred, y_true):\n","    loss = torch.sqrt(self.mse(y_pred, y_true).mean(dim = 0)).mean(dim = 0)\n","    return loss  \n","\n","\n","class MeanPooling(nn.Module):\n","    def __init__(self):\n","        super(MeanPooling, self).__init__()\n","        \n","    def forward(self, last_hidden_state, attention_mask):\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","        sum_mask = input_mask_expanded.sum(1)\n","        sum_mask = torch.clamp(sum_mask, min=1e-9)\n","        mean_embeddings = sum_embeddings / sum_mask\n","        return mean_embeddings\n","\n","class WeightedLayerPooling(nn.Module):\n","    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n","        super(WeightedLayerPooling, self).__init__()\n","        self.layer_start = layer_start\n","        self.num_hidden_layers = num_hidden_layers\n","        self.layer_weights = layer_weights if layer_weights is not None \\\n","            else nn.Parameter(\n","                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n","            )\n","\n","    def forward(self, features):\n","\n","        all_layer_embedding = torch.stack(features)\n","        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n","\n","        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n","        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n","\n","        return weighted_average"]},{"cell_type":"code","source":["#AWP\n","class AWP:\n","    def __init__(\n","        self,\n","        model,\n","        optimizer,\n","        adv_param=\"weight\",\n","        adv_lr=1,\n","        adv_eps=0.2,\n","        start_epoch=0,\n","        adv_step=1,\n","        scaler=None\n","    ):\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.adv_param = adv_param\n","        self.adv_lr = adv_lr\n","        self.adv_eps = adv_eps\n","        self.start_epoch = start_epoch\n","        self.adv_step = adv_step\n","        self.backup = {}\n","        self.backup_eps = {}\n","        self.scaler = scaler\n","\n","    def attack_backward(self, x, y, attention_mask,epoch):\n","        if (self.adv_lr == 0) or (epoch < self.start_epoch):\n","            return None\n","\n","        self._save() \n","        for i in range(self.adv_step):\n","            self._attack_step() \n","            with torch.cuda.amp.autocast():\n","                adv_loss, tr_logits = self.model(ids=x, mask=attention_mask, targets=y)\n","                adv_loss = adv_loss.mean()\n","            self.optimizer.zero_grad()\n","            self.scaler.scale(adv_loss).backward()\n","            \n","        self._restore()\n","\n","    def _attack_step(self):\n","        e = 1e-6\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad and param.grad is not None and self.adv_param in name:\n","                norm1 = torch.norm(param.grad)\n","                norm2 = torch.norm(param.data.detach())\n","                if norm1 != 0 and not torch.isnan(norm1):\n","                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n","                    param.data.add_(r_at)\n","                    param.data = torch.min(\n","                        torch.max(param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n","                    )\n","                # param.data.clamp_(*self.backup_eps[name])\n","\n","    def _save(self):\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad and param.grad is not None and self.adv_param in name:\n","                if name not in self.backup:\n","                    self.backup[name] = param.data.clone()\n","                    grad_eps = self.adv_eps * param.abs().detach()\n","                    self.backup_eps[name] = (\n","                        self.backup[name] - grad_eps,\n","                        self.backup[name] + grad_eps,\n","                    )\n","\n","    def _restore(self,):\n","        for name, param in self.model.named_parameters():\n","            if name in self.backup:\n","                param.data = self.backup[name]\n","        self.backup = {}\n","        self.backup_eps = {}"],"metadata":{"id":"BS58rpxCbEKA","executionInfo":{"status":"ok","timestamp":1666193393638,"user_tz":-330,"elapsed":8,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","execution_count":14,"metadata":{"id":"8lkexNsJDtCV","executionInfo":{"status":"ok","timestamp":1666193393638,"user_tz":-330,"elapsed":8,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}}},"outputs":[],"source":["class Model(nn.Module):\n","    def __init__(self, model_name, params = None):\n","        super(Model, self).__init__()\n","        \n","        self.model_name = model_name\n","\n","        hidden_dropout_prob: float = CFG.hidden_dropout_prob\n","        layer_norm_eps: float = 1e-7\n","\n","        config = AutoConfig.from_pretrained(model_name)\n","\n","        config.update(\n","            {\n","                \"output_hidden_states\": True,\n","                \"hidden_dropout_prob\": hidden_dropout_prob,\n","                \"attention_probs_dropout_prob\" : hidden_dropout_prob,\n","                \"layer_norm_eps\": layer_norm_eps,\n","                \"add_pooling_layer\": False,\n","                \"num_labels\": 6,\n","            }\n","        )\n","        \n","        self.config = config\n","        self.params = params\n","        \n","        self.transformer = AutoModel.from_pretrained(model_name, config=config)\n","        if CFG.gradient_checkpointing_enable:\n","            self.transformer.gradient_checkpointing_enable()\n","        \n","        self.output1 = nn.Linear(config.hidden_size, 9)\n","        self.output2 = nn.Linear(config.hidden_size, 9)\n","        self.output3 = nn.Linear(config.hidden_size, 9)\n","        self.output4 = nn.Linear(config.hidden_size, 9)\n","        self.output5 = nn.Linear(config.hidden_size, 9)\n","        self.output6 = nn.Linear(config.hidden_size, 9)\n","        self.loss = nn.CrossEntropyLoss()\n","\n","        if CFG.reinit_weights:\n","          self.init_weights_(params[\"reinit_layers\"])\n","\n","    def init_weights_(self, reinit_layers):\n","      for layer in self.transformer.encoder.layer[-reinit_layers:]:\n","        for module in layer.modules():\n","            if isinstance(module, nn.Linear):\n","                module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","                if module.bias is not None:\n","                    module.bias.data.zero_()\n","            elif isinstance(module, nn.Embedding):\n","                module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","                if module.padding_idx is not None:\n","                    module.weight.data[module.padding_idx].zero_()\n","            elif isinstance(module, nn.LayerNorm):\n","                module.bias.data.zero_()\n","                module.weight.data.fill_(1.0)\n","\n","    def get_grouped_llrd_optimizer_scheduler(self, num_train_steps, params):\n","      no_decay = [\"bias\", \"LayerNorm.weight\"]\n","      top_params = list(self.output.named_parameters())\n","      # initialize lr for task specific layer\n","      optimizer_grouped_parameters = [\n","              {\n","                  \"params\": [p for n, p in top_params if not any(nd in n for nd in no_decay)],\n","                  \"weight_decay\": 0.001,\n","                  \"lr\": params[\"lr\"],\n","              },\n","              {\n","                  \"params\": [p for n, p in top_params if any(nd in n for nd in no_decay)],\n","                  \"weight_decay\": 0.0,\n","                  \"lr\": params[\"lr\"],\n","              },\n","          ]\n","\n","      bottom_params = [(n,p) for n,p in self.named_parameters() if n in [\"transformer.encoder.rel_embeddings.weight\",\"transformer.encoder.LayerNorm.weight\",\"transformer.encoder.LayerNorm.bias\"]]\n","\n","      optimizer_grouped_parameters += [\n","              {\n","                  \"params\": [p for n, p in bottom_params if not any(nd in n for nd in no_decay)],\n","                  \"weight_decay\": 0.001,\n","                  \"lr\": params[\"lr\"],\n","              },\n","              {\n","                  \"params\": [p for n, p in bottom_params if any(nd in n for nd in no_decay)],\n","                  \"weight_decay\": 0.0,\n","                  \"lr\": params[\"lr\"],\n","              },\n","          ]\n","      # initialize lrs for every layer\n","      num_layers = self.config.num_hidden_layers\n","      layers = [self.transformer.embeddings] + list(self.transformer.encoder.layer)\n","      layers.reverse()\n","      lr = params[\"lr\"]\n","      for i,layer in enumerate(layers):\n","        lr *= params[\"llrd\"]\n","\n","        optimizer_grouped_parameters += [\n","            {\n","                \"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.001,\n","                \"lr\": lr,\n","            },\n","            {\n","                \"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","                \"lr\": lr,\n","            },\n","        ]\n","      opt = torch.optim.AdamW(optimizer_grouped_parameters)\n","      if params[\"scheduler\"] == \"polynomial\":\n","        sch = get_polynomial_decay_schedule_with_warmup(\n","            opt,\n","            num_warmup_steps=int(num_train_steps * CFG.warmup_ratio),\n","            num_training_steps=num_train_steps,\n","            last_epoch=-1,\n","        )\n","      elif params[\"scheduler\"] == \"linear\":\n","        sch = get_linear_schedule_with_warmup(\n","            opt,\n","            num_warmup_steps=int(num_train_steps * CFG.warmup_ratio),\n","            num_training_steps=num_train_steps,\n","            last_epoch=-1,\n","        )\n","      else:\n","        sch = get_cosine_schedule_with_warmup(\n","            opt,\n","            num_warmup_steps=int(num_train_steps * CFG.warmup_ratio),\n","            num_training_steps=num_train_steps,\n","            last_epoch=-1,\n","        )\n","\n","      return opt, sch\n","        \n","    def get_optimizer_scheduler(self, num_train_steps):\n","        param_optimizer = list(self.named_parameters())\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","        optimizer_parameters = [\n","            {\n","                \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.001,\n","            },\n","            {\n","                \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","            },\n","        ]\n","        opt = torch.optim.AdamW(optimizer_parameters, lr=CFG.lr)\n","        sch = get_polynomial_decay_schedule_with_warmup(\n","            opt,\n","            num_warmup_steps=int(num_train_steps * CFG.warmup_ratio),\n","            num_training_steps=num_train_steps,\n","            last_epoch=-1,\n","        )\n","        return opt, sch\n","\n","    def forward(self, ids, mask, token_type_ids=None, targets=None):\n","        if token_type_ids is not None:\n","            transformer_out = self.transformer( ids, mask, token_type_ids )\n","        else:\n","            transformer_out = self.transformer(ids, mask)\n","        sequence_output = transformer_out.last_hidden_state[:,0,:]\n","    \n","        logits1 = self.output1(sequence_output)\n","        logits2 = self.output2(sequence_output)\n","        logits3 = self.output3(sequence_output)\n","        logits4 = self.output4(sequence_output)\n","        logits5 = self.output5(sequence_output)\n","        logits6 = self.output6(sequence_output)\n","\n","        \n","        loss1 = self.loss(logits1, targets[:,0])\n","        loss2 = self.loss(logits2, targets[:,1])\n","        loss3 = self.loss(logits3, targets[:,2])\n","        loss4 = self.loss(logits4, targets[:,3])\n","        loss5 = self.loss(logits5, targets[:,4])\n","        loss6 = self.loss(logits6, targets[:,5])\n","\n","        loss = (loss1 + loss2 + loss3 + loss4 + loss5 + loss6)/ 6\n","\n","        logits = torch.cat((logits1.unsqueeze(-1), logits2.unsqueeze(-1), logits3.unsqueeze(-1), logits4.unsqueeze(-1), logits5.unsqueeze(-1), logits6.unsqueeze(-1)), dim = -1)\n","\n","        return loss, logits"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"PojCbT3GDtCW","executionInfo":{"status":"ok","timestamp":1666193393638,"user_tz":-330,"elapsed":7,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}}},"outputs":[],"source":["def train(epoch, model, train_loader, valid_loader, optimizer, scheduler, device, awp, scaler, best_loss, fold):\n","    model.train()\n","    losses = AverageMeter()\n","    start = end = time.time()\n","    val_steps = len(train_loader) // 1\n","    for step, x in enumerate(train_loader):\n","        for k,v in x.items():\n","            x[k] = v.to(device)\n","        \n","        with autocast():    \n","            loss, logits = model(**x)\n","            \n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        \n","        losses.update(loss.item() * CFG.gradient_accumulation_steps , CFG.batch_size)\n","        scaler.scale(loss).backward()\n","        if CFG.use_awp:\n","          awp.attack_backward(x[\"ids\"],x[\"targets\"],x[\"mask\"],epoch) \n","        \n","        \n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.unscale_(optimizer)\n","            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","            if scheduler is not None:\n","                scheduler.step()\n","        end = time.time()\n","        \n","        if ((step + 1) % CFG.print_freq == 0) or (step == (len(train_loader)-1)):\n","            print('Epoch: [{0}][{1}/{2}] '\n","                  'Elapsed {remain:s} '\n","                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                  'Grad: {grad_norm:.4f}  '\n","                  'LR: {lr:.8f}  '\n","                  .format(epoch+1, step + 1, len(train_loader), \n","                          remain=timeSince(start, float(step+1)/len(train_loader)),\n","                          loss=losses,\n","                          grad_norm= grad_norm,\n","                          lr=scheduler.get_lr()[0]))\n","        \n","        if ((step + 1) % val_steps == 0) or ((step + 1) == len(train_loader)):\n","          print(\"\\nVALID LOOP\\n\")\n","          valid_loss = valid(epoch, model, valid_loader, device)\n","          print(f\"\\nThe valid loss for the current epoch is {valid_loss}\\n\")\n","          torch.cuda.empty_cache()\n","          gc.collect()\n","          if valid_loss < best_loss:\n","              best_loss = valid_loss\n","              if CFG.save_dir is not None:\n","                if not os.path.exists(CFG.save_dir):\n","                  os.mkdir(CFG.save_dir)\n","                save_path = os.path.join(CFG.save_dir, f\"{CFG.save_model_name}_fold_{fold}.pth\")\n","              else:\n","                save_path = f\"{CFG.save_model_name}_fold_{fold}.pth\"\n","              torch.save(model.state_dict(), save_path)\n","        model.train()\n","    return best_loss\n","\n","def valid(epoch, model, valid_loader, device):\n","    model.eval()\n","    all_targets = []\n","    all_outputs = []\n","    losses = AverageMeter()\n","    with torch.no_grad():\n","        for step, x in enumerate(valid_loader):\n","\n","            for k, v in x.items():\n","                x[k] = v.to(device)\n","            with autocast():\n","                loss, logits = model(**x)\n","\n","            \n","            losses.update(loss.item(), CFG.batch_size)\n","            targets = x[\"targets\"].cpu().numpy()\n","            outputs = torch.max(logits, dim = 1)[1].cpu().numpy()\n","\n","            all_targets.append(targets)\n","            all_outputs.append(outputs)\n","\n","            if ((step + 1) % CFG.print_freq == 0) or (step == (len(valid_loader)-1)):\n","                print('Epoch: [{0}][{1}/{2}] '\n","                      'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                      .format(epoch+1, step + 1, len(valid_loader), loss=losses))\n","        \n","    \n","    all_targets = np.vstack(all_targets)\n","    all_outputs = np.vstack(all_outputs)\n","    loss = get_score(all_targets, all_outputs)[0]\n","    \n","    del all_targets, all_outputs;\n","    return loss"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"zovUkUouDtCW","executionInfo":{"status":"ok","timestamp":1666193393639,"user_tz":-330,"elapsed":8,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}}},"outputs":[],"source":["def main(fold, params):\n","\n","    ## Mapping real numbers to classes\n","    classes = [1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n","    class_mapping = dict()\n","    for i, class_ in enumerate(classes):\n","      class_mapping[class_] = i\n","\n","    \n","    torch.cuda.empty_cache()\n","    df = pd.read_csv(CFG.train_file)\n","    \n","    train_df = df.loc[df.kfold != fold]\n","    valid_df = df.loc[df.kfold == fold]\n","    \n","    train_texts = train_df[\"full_text\"].apply(resolve_encodings_and_normalize).to_list()\n","    valid_texts = valid_df[\"full_text\"].apply(resolve_encodings_and_normalize).to_list()\n","    \n","    train_targets = train_df[CFG.target_columns].applymap(lambda x : class_mapping[float(x)]).values.tolist()\n","    \n","    valid_targets = valid_df[CFG.target_columns].applymap(lambda x : class_mapping[float(x)]).values.tolist()\n","    \n","    train_ds = Dataset(train_texts, train_targets, CFG.tokenizer)\n","    valid_ds = Dataset(valid_texts, valid_targets, CFG.tokenizer, is_train = False)\n","\n","    collate_fn = Collate(CFG.tokenizer)\n","    train_loader = torch.utils.data.DataLoader(train_ds, batch_size = CFG.batch_size, shuffle = True, collate_fn = collate_fn, num_workers = CFG.num_workers)\n","    valid_loader = torch.utils.data.DataLoader(valid_ds, batch_size = CFG.batch_size, shuffle = False, collate_fn = collate_fn, num_workers = CFG.num_workers)\n","    \n","    model = Model(CFG.model_name, params)\n","\n","    ## Adding noisy tune -> NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better\n","    # noise_lambda = 0.15\n","    # sd = model.state_dict()\n","    # for name, param in model.named_parameters():\n","    #   sd[name][:] += (torch.rand(param.size()) - 0.5) * noise_lambda * torch.std(param)\n","\n","    \n","    # model.load_state_dict(sd)\n","\n","    \n","    num_train_steps = int(len(train_ds) / CFG.batch_size / CFG.gradient_accumulation_steps * CFG.epochs)\n","    optimizer, scheduler = model.get_optimizer_scheduler(num_train_steps)\n","    \n","    model = model.to(CFG.device)\n","    best_loss = np.inf\n","    scaler = GradScaler()\n","    if CFG.use_awp:\n","      print('Enable AWP')\n","      awp = AWP(model,\n","          optimizer,\n","          adv_lr=CFG.adv_lr,\n","          adv_eps=CFG.adv_eps,\n","          start_epoch=2,\n","          scaler=scaler)\n","    else:\n","      awp = None\n","    for epoch in range(CFG.epochs):\n","        print(\"\\nTRAIN LOOP\\n\")\n","        best_loss = train(epoch, model, train_loader, valid_loader, optimizer, scheduler, CFG.device, awp, scaler, best_loss, fold)\n","        # print(\"\\nVALID LOOP\\n\")\n","        # valid_loss = valid(epoch, model, valid_loader, CFG.device)\n","        \n","        # print(f\"\\nThe valid loss for the current epoch is {valid_loss}\\n\")\n","        # torch.cuda.empty_cache()\n","        # gc.collect()\n","        # if valid_loss < best_loss:\n","        #     best_loss = valid_loss\n","        #     if CFG.save_dir is not None:\n","        #       if not os.path.exists(CFG.save_dir):\n","        #         os.mkdir(CFG.save_dir)\n","        #       save_path = os.path.join(CFG.save_dir, f\"{CFG.save_model_name}_fold_{fold}.pth\")\n","        #     else:\n","        #       save_path = f\"{CFG.save_model_name}_fold_{fold}.pth\"\n","        #     torch.save(model.state_dict(), save_path)\n","            \n","    del model, optimizer, scheduler, train_loader, valid_loader, train_df, valid_df;\n","    gc.collect()\n","\n","    return best_loss"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ndUmqkakDtCW","outputId":"ca294627-9ae5-463e-b5ae-b5858b1df609","executionInfo":{"status":"error","timestamp":1666195719839,"user_tz":-330,"elapsed":2326208,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","RUNNING FOLD 0\n","\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","TRAIN LOOP\n","\n","Epoch: [1][20/392] Elapsed 0m 35s (remain 10m 56s) Loss: 1.9588(2.1745) Grad: 6.7461  LR: 0.00001966  \n","Epoch: [1][40/392] Elapsed 1m 9s (remain 10m 13s) Loss: 1.6163(2.0151) Grad: 5.4931  LR: 0.00001932  \n","Epoch: [1][60/392] Elapsed 1m 44s (remain 9m 37s) Loss: 1.5365(1.9075) Grad: 4.7764  LR: 0.00001898  \n","Epoch: [1][80/392] Elapsed 2m 18s (remain 9m 1s) Loss: 1.4415(1.8316) Grad: 6.0434  LR: 0.00001864  \n","Epoch: [1][100/392] Elapsed 2m 53s (remain 8m 27s) Loss: 1.5514(1.7810) Grad: 5.2040  LR: 0.00001830  \n","Epoch: [1][120/392] Elapsed 3m 28s (remain 7m 53s) Loss: 1.3717(1.7397) Grad: 4.4591  LR: 0.00001796  \n","Epoch: [1][140/392] Elapsed 4m 4s (remain 7m 19s) Loss: 1.3503(1.7013) Grad: 4.4100  LR: 0.00001762  \n","Epoch: [1][160/392] Elapsed 4m 39s (remain 6m 45s) Loss: 1.4192(1.6777) Grad: 5.3578  LR: 0.00001728  \n","Epoch: [1][180/392] Elapsed 5m 15s (remain 6m 11s) Loss: 1.9186(1.6592) Grad: 5.9671  LR: 0.00001694  \n","Epoch: [1][200/392] Elapsed 5m 50s (remain 5m 36s) Loss: 1.5188(1.6407) Grad: 6.7867  LR: 0.00001660  \n","Epoch: [1][220/392] Elapsed 6m 25s (remain 5m 1s) Loss: 1.2237(1.6161) Grad: 3.9552  LR: 0.00001626  \n","Epoch: [1][240/392] Elapsed 7m 0s (remain 4m 26s) Loss: 1.5345(1.6026) Grad: 4.0349  LR: 0.00001592  \n","Epoch: [1][260/392] Elapsed 7m 36s (remain 3m 51s) Loss: 1.5059(1.5907) Grad: 6.1320  LR: 0.00001559  \n","Epoch: [1][280/392] Elapsed 8m 11s (remain 3m 16s) Loss: 1.9302(1.5781) Grad: 4.0319  LR: 0.00001525  \n","Epoch: [1][300/392] Elapsed 8m 46s (remain 2m 41s) Loss: 1.5557(1.5685) Grad: 3.9323  LR: 0.00001491  \n","Epoch: [1][320/392] Elapsed 9m 21s (remain 2m 6s) Loss: 1.2287(1.5590) Grad: 4.9849  LR: 0.00001457  \n","Epoch: [1][340/392] Elapsed 9m 56s (remain 1m 31s) Loss: 1.4951(1.5501) Grad: 5.5193  LR: 0.00001423  \n","Epoch: [1][360/392] Elapsed 10m 31s (remain 0m 56s) Loss: 1.3457(1.5417) Grad: 3.2468  LR: 0.00001389  \n","Epoch: [1][380/392] Elapsed 11m 6s (remain 0m 21s) Loss: 1.3264(1.5340) Grad: 3.8138  LR: 0.00001355  \n","Epoch: [1][392/392] Elapsed 11m 26s (remain 0m 0s) Loss: 1.3816(1.5322) Grad: 8.0728  LR: 0.00001334  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/98] Loss: 1.3690(1.4331) \n","Epoch: [1][40/98] Loss: 1.4933(1.4386) \n","Epoch: [1][60/98] Loss: 1.2044(1.4341) \n","Epoch: [1][80/98] Loss: 1.3814(1.4386) \n","Epoch: [1][98/98] Loss: 1.5172(1.4427) \n","\n","The valid loss for the current epoch is 0.5488053140375435\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/392] Elapsed 0m 35s (remain 10m 57s) Loss: 1.5258(1.4110) Grad: 4.4085  LR: 0.00001300  \n","Epoch: [2][40/392] Elapsed 1m 10s (remain 10m 21s) Loss: 1.3879(1.3923) Grad: 5.8568  LR: 0.00001266  \n","Epoch: [2][60/392] Elapsed 1m 45s (remain 9m 45s) Loss: 1.4097(1.3827) Grad: 5.1699  LR: 0.00001233  \n","Epoch: [2][80/392] Elapsed 2m 20s (remain 9m 9s) Loss: 1.2972(1.3792) Grad: 4.7896  LR: 0.00001199  \n","Epoch: [2][100/392] Elapsed 2m 56s (remain 8m 34s) Loss: 1.3458(1.3760) Grad: 4.8109  LR: 0.00001165  \n","Epoch: [2][120/392] Elapsed 3m 31s (remain 7m 59s) Loss: 1.4625(1.3709) Grad: 4.2200  LR: 0.00001131  \n","Epoch: [2][140/392] Elapsed 4m 6s (remain 7m 23s) Loss: 1.3397(1.3666) Grad: 3.4367  LR: 0.00001097  \n","Epoch: [2][160/392] Elapsed 4m 41s (remain 6m 48s) Loss: 1.2103(1.3581) Grad: 5.5261  LR: 0.00001063  \n","Epoch: [2][180/392] Elapsed 5m 17s (remain 6m 13s) Loss: 1.3879(1.3548) Grad: 5.3508  LR: 0.00001029  \n","Epoch: [2][200/392] Elapsed 5m 52s (remain 5m 38s) Loss: 1.1840(1.3486) Grad: 3.9996  LR: 0.00000995  \n","Epoch: [2][220/392] Elapsed 6m 27s (remain 5m 2s) Loss: 1.3591(1.3440) Grad: 4.0291  LR: 0.00000961  \n","Epoch: [2][240/392] Elapsed 7m 2s (remain 4m 27s) Loss: 1.2034(1.3390) Grad: 3.8824  LR: 0.00000927  \n","Epoch: [2][260/392] Elapsed 7m 37s (remain 3m 52s) Loss: 1.3684(1.3343) Grad: 5.6206  LR: 0.00000893  \n","Epoch: [2][280/392] Elapsed 8m 13s (remain 3m 17s) Loss: 1.1309(1.3273) Grad: 3.4286  LR: 0.00000859  \n","Epoch: [2][300/392] Elapsed 8m 48s (remain 2m 42s) Loss: 1.2868(1.3239) Grad: 4.1281  LR: 0.00000825  \n","Epoch: [2][320/392] Elapsed 9m 24s (remain 2m 6s) Loss: 1.1928(1.3204) Grad: 3.5957  LR: 0.00000791  \n","Epoch: [2][340/392] Elapsed 9m 59s (remain 1m 31s) Loss: 1.2439(1.3200) Grad: 5.1528  LR: 0.00000757  \n","Epoch: [2][360/392] Elapsed 10m 34s (remain 0m 56s) Loss: 1.4995(1.3176) Grad: 8.0641  LR: 0.00000723  \n","Epoch: [2][380/392] Elapsed 11m 9s (remain 0m 21s) Loss: 1.2277(1.3153) Grad: 3.6319  LR: 0.00000689  \n","Epoch: [2][392/392] Elapsed 11m 29s (remain 0m 0s) Loss: 1.4546(1.3136) Grad: 8.3380  LR: 0.00000669  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/98] Loss: 1.3128(1.3572) \n","Epoch: [2][40/98] Loss: 1.2609(1.3478) \n","Epoch: [2][60/98] Loss: 1.1223(1.3375) \n","Epoch: [2][80/98] Loss: 1.4363(1.3502) \n","Epoch: [2][98/98] Loss: 1.2828(1.3458) \n","\n","The valid loss for the current epoch is 0.4856829983216692\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/392] Elapsed 0m 35s (remain 10m 59s) Loss: 1.3762(1.2343) Grad: 4.2796  LR: 0.00000635  \n","Epoch: [3][40/392] Elapsed 1m 10s (remain 10m 21s) Loss: 1.1250(1.1934) Grad: 3.4918  LR: 0.00000601  \n","Epoch: [3][60/392] Elapsed 1m 46s (remain 9m 50s) Loss: 1.3487(1.1869) Grad: 4.9710  LR: 0.00000567  \n","Epoch: [3][80/392] Elapsed 2m 22s (remain 9m 13s) Loss: 1.1342(1.1849) Grad: 5.0272  LR: 0.00000533  \n","Epoch: [3][100/392] Elapsed 2m 57s (remain 8m 37s) Loss: 1.2022(1.1805) Grad: 4.6294  LR: 0.00000499  \n","Epoch: [3][120/392] Elapsed 3m 32s (remain 8m 1s) Loss: 1.1203(1.1732) Grad: 5.2864  LR: 0.00000465  \n","Epoch: [3][140/392] Elapsed 4m 7s (remain 7m 25s) Loss: 1.3229(1.1706) Grad: 6.3438  LR: 0.00000431  \n","Epoch: [3][160/392] Elapsed 4m 42s (remain 6m 50s) Loss: 1.1281(1.1645) Grad: 4.8253  LR: 0.00000397  \n","Epoch: [3][180/392] Elapsed 5m 18s (remain 6m 14s) Loss: 1.1510(1.1628) Grad: 4.2403  LR: 0.00000363  \n","Epoch: [3][200/392] Elapsed 5m 53s (remain 5m 39s) Loss: 1.1869(1.1624) Grad: 4.1700  LR: 0.00000329  \n","Epoch: [3][220/392] Elapsed 6m 28s (remain 5m 3s) Loss: 1.1794(1.1640) Grad: 3.9523  LR: 0.00000295  \n","Epoch: [3][240/392] Elapsed 7m 3s (remain 4m 28s) Loss: 1.0791(1.1626) Grad: 5.5843  LR: 0.00000261  \n","Epoch: [3][260/392] Elapsed 7m 39s (remain 3m 53s) Loss: 1.1764(1.1602) Grad: 4.6166  LR: 0.00000227  \n","Epoch: [3][280/392] Elapsed 8m 14s (remain 3m 17s) Loss: 1.1709(1.1606) Grad: 3.9313  LR: 0.00000193  \n","Epoch: [3][300/392] Elapsed 8m 49s (remain 2m 42s) Loss: 1.0580(1.1600) Grad: 4.2851  LR: 0.00000159  \n","Epoch: [3][320/392] Elapsed 9m 25s (remain 2m 7s) Loss: 1.2200(1.1588) Grad: 4.8410  LR: 0.00000125  \n","Epoch: [3][340/392] Elapsed 10m 0s (remain 1m 31s) Loss: 0.9912(1.1573) Grad: 4.0213  LR: 0.00000092  \n","Epoch: [3][360/392] Elapsed 10m 36s (remain 0m 56s) Loss: 1.0822(1.1563) Grad: 3.5799  LR: 0.00000058  \n","Epoch: [3][380/392] Elapsed 11m 11s (remain 0m 21s) Loss: 1.1994(1.1556) Grad: 5.6377  LR: 0.00000024  \n","Epoch: [3][392/392] Elapsed 11m 30s (remain 0m 0s) Loss: 1.0203(1.1549) Grad: 8.7926  LR: 0.00000010  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/98] Loss: 1.2670(1.3596) \n","Epoch: [3][40/98] Loss: 1.2765(1.3495) \n","Epoch: [3][60/98] Loss: 1.0759(1.3399) \n","Epoch: [3][80/98] Loss: 1.5003(1.3532) \n","Epoch: [3][98/98] Loss: 1.4003(1.3564) \n","\n","The valid loss for the current epoch is 0.4926783565784887\n","\n","######\n","The best loss for fold 0 is 0.4856829983216692\n","######\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","RUNNING FOLD 1\n","\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","TRAIN LOOP\n","\n","Epoch: [1][20/391] Elapsed 0m 35s (remain 11m 1s) Loss: 2.0046(2.1882) Grad: 5.9423  LR: 0.00001966  \n","Epoch: [1][40/391] Elapsed 1m 10s (remain 10m 21s) Loss: 1.6740(1.9959) Grad: 5.6115  LR: 0.00001932  \n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-2f484f0c0925>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;34m'scheduler'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'polynomial'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             'reinit_layers': 4}\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mbest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"######\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The best loss for fold {fold} is {best_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-6d329ab96e6c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(fold, params)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTRAIN LOOP\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mbest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mawp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;31m# print(\"\\nVALID LOOP\\n\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# valid_loss = valid(epoch, model, valid_loader, CFG.device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-a7698e5f0104>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, model, train_loader, valid_loader, optimizer, scheduler, device, awp, scaler, best_loss, fold)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-9d16a2fc6789>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ids, mask, token_type_ids, targets)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mtransformer_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mtransformer_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1104\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m         )\n\u001b[1;32m   1108\u001b[0m         \u001b[0mencoded_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    537\u001b[0m                     \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m                     \u001b[0mrelative_pos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m                     \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m                 )\n\u001b[1;32m    541\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, *args, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_reentrant\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         return _checkpoint_without_reentrant(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mcustom_forward\u001b[0;34m(*inputs)\u001b[0m\n\u001b[1;32m    527\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mcreate_custom_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mcustom_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0mquery_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0mrelative_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrelative_pos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrel_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         )\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mquery_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mrelative_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrelative_pos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrel_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         )\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mrel_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             rel_att = self.disentangled_attention_bias(\n\u001b[0;32m--> 751\u001b[0;31m                 \u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m             )\n\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mdisentangled_attention_bias\u001b[0;34m(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0matt_span\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_ebd_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m         \u001b[0mrelative_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelative_pos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0mrel_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0matt_span\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["for fold in range(5):\n","  print(\"-----\"*30)\n","  print(f\"\\nRUNNING FOLD {fold}\\n\")\n","  print(\"-----\"*30)\n","  params = {'lr': 2e-5,\n","            'llrd': 0.9,\n","            'scheduler': 'polynomial',\n","            'reinit_layers': 4}\n","  best_loss = main(fold, params)\n","  print(\"######\")\n","  print(f\"The best loss for fold {fold} is {best_loss}\")\n","  print(\"######\")\n","  gc.collect()"]},{"cell_type":"code","source":["#running hyperparameter tuning on first fold with optuna\n","def objective(trial):\n","  params = {\n","      \"lr\": trial.suggest_loguniform(\"learning_rate\", 1e-7, 1e-4),\n","      \"llrd\" : trial.suggest_uniform(\"layer_wise_learning_rate_decay\", 0.7, 1.0),\n","      \"scheduler\" : trial.suggest_categorical(\"learning_rate_schduler\",[\"polynomial\", \"cosine\",\"linear\"]),\n","      \"reinit_layers\": trial.suggest_int(\"reinit_layers\",0, 4)\n","  }\n","  best_loss= main(1,params)\n","  return best_loss\n","\n","\n","study = optuna.create_study(direction='minimize')\n","study.optimize(objective, n_trials=20)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vTU6r_sovsBJ","executionInfo":{"status":"ok","timestamp":1663311812699,"user_tz":-330,"elapsed":12567757,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}},"outputId":"5229c918-bbbb-48d6-c1d3-7784cf0e3b14"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-09-15 18:26:18,743]\u001b[0m A new study created in memory with name: no-name-d9193ad8-4b94-4d31-a28c-de8c5b753dcc\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 12s (remain 10m 33s) Loss: 1.3281(2.1507) Grad: 16.2040  LR: 0.00000612  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.2657(1.2626) Grad: 3.9198  LR: 0.00001225  \n","Epoch: [1][60/196] Elapsed 3m 35s (remain 8m 7s) Loss: 0.1242(0.8913) Grad: 3.8266  LR: 0.00001837  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 0.1186(0.6998) Grad: 4.7077  LR: 0.00002450  \n","Epoch: [1][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.0962(0.5867) Grad: 1.9993  LR: 0.00003062  \n","Epoch: [1][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 0.1393(0.5106) Grad: 2.8769  LR: 0.00003583  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1014(0.4560) Grad: 3.6234  LR: 0.00003562  \n","Epoch: [1][160/196] Elapsed 9m 32s (remain 2m 8s) Loss: 0.0952(0.4140) Grad: 2.8452  LR: 0.00003509  \n","Epoch: [1][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.1466(0.3822) Grad: 3.5936  LR: 0.00003426  \n","Epoch: [1][196/196] Elapsed 11m 39s (remain 0m 0s) Loss: 0.1468(0.3617) Grad: 1.6981  LR: 0.00003338  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.48067936301231384\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 12s (remain 10m 34s) Loss: 0.1198(0.1124) Grad: 4.5297  LR: 0.00003203  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.0867(0.1098) Grad: 2.9946  LR: 0.00003043  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.0721(0.1071) Grad: 1.0160  LR: 0.00002861  \n","Epoch: [2][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.1238(0.1053) Grad: 1.7685  LR: 0.00002659  \n","Epoch: [2][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.0893(0.1032) Grad: 2.4408  LR: 0.00002442  \n","Epoch: [2][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 0.1051(0.1027) Grad: 1.9590  LR: 0.00002213  \n","Epoch: [2][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1304(0.1035) Grad: 3.9332  LR: 0.00001977  \n","Epoch: [2][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.0849(0.1025) Grad: 2.3551  LR: 0.00001737  \n","Epoch: [2][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.0684(0.1009) Grad: 0.9413  LR: 0.00001499  \n","Epoch: [2][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.0733(0.1001) Grad: 1.6976  LR: 0.00001311  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.4583507478237152\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 12s (remain 10m 34s) Loss: 0.1027(0.0779) Grad: 2.0285  LR: 0.00001085  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.0564(0.0764) Grad: 2.0738  LR: 0.00000872  \n","Epoch: [3][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.0568(0.0725) Grad: 1.0257  LR: 0.00000674  \n","Epoch: [3][80/196] Elapsed 4m 45s (remain 6m 54s) Loss: 0.0621(0.0709) Grad: 1.6748  LR: 0.00000497  \n","Epoch: [3][100/196] Elapsed 5m 57s (remain 5m 42s) Loss: 0.0883(0.0699) Grad: 1.3882  LR: 0.00000344  \n","Epoch: [3][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.0591(0.0698) Grad: 1.6332  LR: 0.00000216  \n","Epoch: [3][140/196] Elapsed 8m 19s (remain 3m 19s) Loss: 0.0601(0.0694) Grad: 1.6902  LR: 0.00000116  \n","Epoch: [3][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.0631(0.0693) Grad: 0.7188  LR: 0.00000046  \n","Epoch: [3][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.0716(0.0694) Grad: 1.0313  LR: 0.00000008  \n","Epoch: [3][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.0824(0.0692) Grad: 2.5153  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.45831298828125\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-09-15 19:04:12,212]\u001b[0m Trial 0 finished with value: 0.45831298828125 and parameters: {'learning_rate': 3.5828994394331454e-05, 'layer_wise_learning_rate_decay': 0.9767672591137755, 'learning_rate_schduler': 'cosine', 'reinit_layers': 1}. Best is trial 0 with value: 0.45831298828125.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 11s (remain 10m 32s) Loss: 0.6627(1.9961) Grad: 12.6625  LR: 0.00001022  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 18s) Loss: 0.1617(1.1272) Grad: 5.9817  LR: 0.00002043  \n","Epoch: [1][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.1523(0.8059) Grad: 6.1189  LR: 0.00003065  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.1614(0.6430) Grad: 4.9825  LR: 0.00004086  \n","Epoch: [1][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.0882(0.5389) Grad: 2.2667  LR: 0.00005108  \n","Epoch: [1][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 0.1492(0.4709) Grad: 5.0348  LR: 0.00005975  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.0770(0.4227) Grad: 1.9766  LR: 0.00005941  \n","Epoch: [1][160/196] Elapsed 9m 32s (remain 2m 8s) Loss: 0.1981(0.3885) Grad: 4.4461  LR: 0.00005853  \n","Epoch: [1][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.1491(0.3605) Grad: 3.6420  LR: 0.00005714  \n","Epoch: [1][196/196] Elapsed 11m 39s (remain 0m 0s) Loss: 0.1232(0.3415) Grad: 4.7468  LR: 0.00005567  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.5392002463340759\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 11s (remain 10m 29s) Loss: 0.1147(0.1274) Grad: 3.8033  LR: 0.00005343  \n","Epoch: [2][40/196] Elapsed 2m 22s (remain 9m 17s) Loss: 0.1100(0.1139) Grad: 3.7908  LR: 0.00005076  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 5s) Loss: 0.0869(0.1118) Grad: 1.9651  LR: 0.00004772  \n","Epoch: [2][80/196] Elapsed 4m 45s (remain 6m 54s) Loss: 0.0801(0.1090) Grad: 2.1392  LR: 0.00004435  \n","Epoch: [2][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.0924(0.1070) Grad: 2.8385  LR: 0.00004073  \n","Epoch: [2][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1136(0.1055) Grad: 2.7469  LR: 0.00003692  \n","Epoch: [2][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.0665(0.1039) Grad: 1.7560  LR: 0.00003298  \n","Epoch: [2][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.0733(0.1047) Grad: 2.2175  LR: 0.00002898  \n","Epoch: [2][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.1089(0.1048) Grad: 3.1740  LR: 0.00002500  \n","Epoch: [2][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.0935(0.1038) Grad: 3.1952  LR: 0.00002187  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.4593140780925751\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 12s (remain 10m 33s) Loss: 0.0643(0.0777) Grad: 1.6479  LR: 0.00001810  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.0808(0.0774) Grad: 1.9345  LR: 0.00001454  \n","Epoch: [3][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.0554(0.0755) Grad: 1.2811  LR: 0.00001125  \n","Epoch: [3][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.0652(0.0734) Grad: 0.8362  LR: 0.00000830  \n","Epoch: [3][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.0702(0.0730) Grad: 2.1292  LR: 0.00000573  \n","Epoch: [3][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 0.0739(0.0727) Grad: 1.9861  LR: 0.00000360  \n","Epoch: [3][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.0500(0.0715) Grad: 1.7373  LR: 0.00000193  \n","Epoch: [3][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.0495(0.0710) Grad: 1.5459  LR: 0.00000077  \n","Epoch: [3][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.0791(0.0711) Grad: 0.7775  LR: 0.00000013  \n","Epoch: [3][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.0489(0.0703) Grad: 1.7691  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.45570382475852966\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-09-15 19:42:05,320]\u001b[0m Trial 1 finished with value: 0.45570382475852966 and parameters: {'learning_rate': 5.975913614980313e-05, 'layer_wise_learning_rate_decay': 0.9419119594261866, 'learning_rate_schduler': 'cosine', 'reinit_layers': 1}. Best is trial 1 with value: 0.45570382475852966.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 11s (remain 10m 26s) Loss: 3.0075(2.8926) Grad: 45.9391  LR: 0.00000002  \n","Epoch: [1][40/196] Elapsed 2m 22s (remain 9m 15s) Loss: 2.9578(2.8690) Grad: 46.0174  LR: 0.00000004  \n","Epoch: [1][60/196] Elapsed 3m 33s (remain 8m 4s) Loss: 2.8359(2.8478) Grad: 45.5045  LR: 0.00000007  \n","Epoch: [1][80/196] Elapsed 4m 45s (remain 6m 53s) Loss: 2.4535(2.8280) Grad: 45.7833  LR: 0.00000009  \n","Epoch: [1][100/196] Elapsed 5m 56s (remain 5m 42s) Loss: 2.6267(2.7785) Grad: 45.9548  LR: 0.00000011  \n","Epoch: [1][120/196] Elapsed 7m 7s (remain 4m 30s) Loss: 2.3203(2.7316) Grad: 45.5919  LR: 0.00000013  \n","Epoch: [1][140/196] Elapsed 8m 18s (remain 3m 19s) Loss: 2.2931(2.6779) Grad: 45.9473  LR: 0.00000013  \n","Epoch: [1][160/196] Elapsed 9m 30s (remain 2m 8s) Loss: 2.2342(2.6223) Grad: 45.3126  LR: 0.00000013  \n","Epoch: [1][180/196] Elapsed 10m 41s (remain 0m 57s) Loss: 1.8138(2.5486) Grad: 44.5934  LR: 0.00000013  \n","Epoch: [1][196/196] Elapsed 11m 37s (remain 0m 0s) Loss: 2.0883(2.4950) Grad: 45.1421  LR: 0.00000012  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 2.0875608921051025\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 11s (remain 10m 32s) Loss: 1.7726(1.6957) Grad: 45.7035  LR: 0.00000012  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 1.4059(1.6226) Grad: 44.1594  LR: 0.00000011  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 1.2418(1.5850) Grad: 42.4413  LR: 0.00000010  \n","Epoch: [2][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 1.3995(1.5321) Grad: 43.4033  LR: 0.00000010  \n","Epoch: [2][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 1.3439(1.4697) Grad: 43.2231  LR: 0.00000009  \n","Epoch: [2][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 1.1205(1.4049) Grad: 40.3951  LR: 0.00000008  \n","Epoch: [2][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 1.1597(1.3597) Grad: 40.4753  LR: 0.00000007  \n","Epoch: [2][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.9332(1.3135) Grad: 37.0475  LR: 0.00000006  \n","Epoch: [2][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.9381(1.2679) Grad: 37.3355  LR: 0.00000005  \n","Epoch: [2][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.9709(1.2376) Grad: 37.4341  LR: 0.00000005  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 1.4343982934951782\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 11s (remain 10m 26s) Loss: 0.5848(0.8464) Grad: 31.5245  LR: 0.00000004  \n","Epoch: [3][40/196] Elapsed 2m 22s (remain 9m 14s) Loss: 0.7681(0.7997) Grad: 36.3528  LR: 0.00000003  \n","Epoch: [3][60/196] Elapsed 3m 33s (remain 8m 3s) Loss: 1.0294(0.7980) Grad: 37.7590  LR: 0.00000002  \n","Epoch: [3][80/196] Elapsed 4m 44s (remain 6m 52s) Loss: 0.8957(0.7915) Grad: 34.5998  LR: 0.00000002  \n","Epoch: [3][100/196] Elapsed 5m 56s (remain 5m 41s) Loss: 0.7333(0.7815) Grad: 33.4935  LR: 0.00000001  \n","Epoch: [3][120/196] Elapsed 7m 7s (remain 4m 30s) Loss: 0.8519(0.7728) Grad: 34.2606  LR: 0.00000001  \n","Epoch: [3][140/196] Elapsed 8m 19s (remain 3m 19s) Loss: 0.5827(0.7666) Grad: 33.9058  LR: 0.00000000  \n","Epoch: [3][160/196] Elapsed 9m 30s (remain 2m 8s) Loss: 0.7709(0.7612) Grad: 33.7145  LR: 0.00000000  \n","Epoch: [3][180/196] Elapsed 10m 41s (remain 0m 57s) Loss: 0.7607(0.7557) Grad: 35.3653  LR: 0.00000000  \n","Epoch: [3][196/196] Elapsed 11m 37s (remain 0m 0s) Loss: 0.3462(0.7537) Grad: 19.5632  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 1.2976852655410767\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-09-15 20:19:57,130]\u001b[0m Trial 2 finished with value: 1.2976852655410767 and parameters: {'learning_rate': 1.3139112976803736e-07, 'layer_wise_learning_rate_decay': 0.7741365547424885, 'learning_rate_schduler': 'cosine', 'reinit_layers': 0}. Best is trial 1 with value: 0.45570382475852966.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 12s (remain 10m 36s) Loss: 1.6320(2.3574) Grad: 42.3980  LR: 0.00000258  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 20s) Loss: 0.2700(1.5041) Grad: 10.6155  LR: 0.00000515  \n","Epoch: [1][60/196] Elapsed 3m 35s (remain 8m 8s) Loss: 0.2152(1.0883) Grad: 5.6037  LR: 0.00000773  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 0.2636(0.8729) Grad: 4.9757  LR: 0.00001031  \n","Epoch: [1][100/196] Elapsed 5m 58s (remain 5m 43s) Loss: 0.2555(0.7395) Grad: 7.7593  LR: 0.00001288  \n","Epoch: [1][120/196] Elapsed 7m 9s (remain 4m 32s) Loss: 0.2169(0.6504) Grad: 7.0170  LR: 0.00001498  \n","Epoch: [1][140/196] Elapsed 8m 21s (remain 3m 20s) Loss: 0.3085(0.5887) Grad: 5.4635  LR: 0.00001433  \n","Epoch: [1][160/196] Elapsed 9m 32s (remain 2m 8s) Loss: 0.1893(0.5397) Grad: 5.4492  LR: 0.00001369  \n","Epoch: [1][180/196] Elapsed 10m 44s (remain 0m 57s) Loss: 0.1760(0.5016) Grad: 8.2680  LR: 0.00001305  \n","Epoch: [1][196/196] Elapsed 11m 39s (remain 0m 0s) Loss: 0.1654(0.4768) Grad: 14.1178  LR: 0.00001253  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.6211233139038086\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 11s (remain 10m 26s) Loss: 0.1472(0.1683) Grad: 5.9022  LR: 0.00001189  \n","Epoch: [2][40/196] Elapsed 2m 22s (remain 9m 14s) Loss: 0.1583(0.1824) Grad: 7.5168  LR: 0.00001125  \n","Epoch: [2][60/196] Elapsed 3m 32s (remain 8m 2s) Loss: 0.2062(0.1811) Grad: 5.8140  LR: 0.00001061  \n","Epoch: [2][80/196] Elapsed 4m 44s (remain 6m 51s) Loss: 0.1843(0.1825) Grad: 14.7255  LR: 0.00000996  \n","Epoch: [2][100/196] Elapsed 5m 55s (remain 5m 40s) Loss: 0.1779(0.1835) Grad: 11.2766  LR: 0.00000932  \n","Epoch: [2][120/196] Elapsed 7m 6s (remain 4m 29s) Loss: 0.2112(0.1822) Grad: 5.5069  LR: 0.00000868  \n","Epoch: [2][140/196] Elapsed 8m 17s (remain 3m 18s) Loss: 0.1693(0.1801) Grad: 14.3329  LR: 0.00000804  \n","Epoch: [2][160/196] Elapsed 9m 28s (remain 2m 7s) Loss: 0.1334(0.1775) Grad: 3.9538  LR: 0.00000739  \n","Epoch: [2][180/196] Elapsed 10m 38s (remain 0m 56s) Loss: 0.1755(0.1754) Grad: 7.8093  LR: 0.00000675  \n","Epoch: [2][196/196] Elapsed 11m 34s (remain 0m 0s) Loss: 0.1406(0.1746) Grad: 16.7026  LR: 0.00000624  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.5681373476982117\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 10s (remain 10m 24s) Loss: 0.1389(0.1561) Grad: 8.5847  LR: 0.00000559  \n","Epoch: [3][40/196] Elapsed 2m 21s (remain 9m 12s) Loss: 0.1095(0.1567) Grad: 21.5962  LR: 0.00000495  \n","Epoch: [3][60/196] Elapsed 3m 32s (remain 8m 1s) Loss: 0.1568(0.1540) Grad: 19.9403  LR: 0.00000431  \n","Epoch: [3][80/196] Elapsed 4m 43s (remain 6m 50s) Loss: 0.1314(0.1581) Grad: 17.7191  LR: 0.00000366  \n","Epoch: [3][100/196] Elapsed 5m 54s (remain 5m 40s) Loss: 0.2093(0.1606) Grad: 21.8581  LR: 0.00000302  \n","Epoch: [3][120/196] Elapsed 7m 5s (remain 4m 29s) Loss: 0.0953(0.1585) Grad: 6.4657  LR: 0.00000238  \n","Epoch: [3][140/196] Elapsed 8m 16s (remain 3m 18s) Loss: 0.2058(0.1565) Grad: 8.6993  LR: 0.00000174  \n","Epoch: [3][160/196] Elapsed 9m 28s (remain 2m 7s) Loss: 0.1531(0.1564) Grad: 13.7082  LR: 0.00000109  \n","Epoch: [3][180/196] Elapsed 10m 39s (remain 0m 56s) Loss: 0.1662(0.1559) Grad: 22.7087  LR: 0.00000045  \n","Epoch: [3][196/196] Elapsed 11m 35s (remain 0m 0s) Loss: 0.1353(0.1553) Grad: 9.6201  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.5547553300857544\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-09-15 20:57:45,530]\u001b[0m Trial 3 finished with value: 0.5547553300857544 and parameters: {'learning_rate': 1.5072591229583353e-05, 'layer_wise_learning_rate_decay': 0.735996067546315, 'learning_rate_schduler': 'polynomial', 'reinit_layers': 0}. Best is trial 1 with value: 0.45570382475852966.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 12s (remain 10m 34s) Loss: 2.9079(2.8963) Grad: 19.5235  LR: 0.00000007  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 3.0929(2.9001) Grad: 20.1187  LR: 0.00000013  \n","Epoch: [1][60/196] Elapsed 3m 34s (remain 8m 7s) Loss: 2.9493(2.8764) Grad: 19.9003  LR: 0.00000020  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 2.6498(2.8356) Grad: 19.7370  LR: 0.00000026  \n","Epoch: [1][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 2.4036(2.7994) Grad: 19.7950  LR: 0.00000033  \n","Epoch: [1][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 2.2564(2.7526) Grad: 20.0148  LR: 0.00000038  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 2.3973(2.6889) Grad: 19.9308  LR: 0.00000037  \n","Epoch: [1][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 2.2436(2.6279) Grad: 20.1031  LR: 0.00000035  \n","Epoch: [1][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 1.9258(2.5645) Grad: 19.6313  LR: 0.00000033  \n","Epoch: [1][196/196] Elapsed 11m 37s (remain 0m 0s) Loss: 1.9134(2.5094) Grad: 20.2492  LR: 0.00000032  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 2.106431245803833\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 11s (remain 10m 29s) Loss: 1.6808(1.7781) Grad: 19.8798  LR: 0.00000030  \n","Epoch: [2][40/196] Elapsed 2m 22s (remain 9m 14s) Loss: 1.7265(1.7097) Grad: 19.6421  LR: 0.00000029  \n","Epoch: [2][60/196] Elapsed 3m 32s (remain 8m 2s) Loss: 1.5776(1.6278) Grad: 19.8334  LR: 0.00000027  \n","Epoch: [2][80/196] Elapsed 4m 43s (remain 6m 51s) Loss: 1.2760(1.5567) Grad: 19.0275  LR: 0.00000025  \n","Epoch: [2][100/196] Elapsed 5m 54s (remain 5m 40s) Loss: 1.2371(1.4803) Grad: 18.6687  LR: 0.00000024  \n","Epoch: [2][120/196] Elapsed 7m 5s (remain 4m 29s) Loss: 1.0437(1.4296) Grad: 17.7560  LR: 0.00000022  \n","Epoch: [2][140/196] Elapsed 8m 16s (remain 3m 18s) Loss: 1.0396(1.3702) Grad: 18.1523  LR: 0.00000021  \n","Epoch: [2][160/196] Elapsed 9m 27s (remain 2m 7s) Loss: 1.0987(1.3190) Grad: 17.9505  LR: 0.00000019  \n","Epoch: [2][180/196] Elapsed 10m 38s (remain 0m 56s) Loss: 0.8633(1.2749) Grad: 17.9608  LR: 0.00000017  \n","Epoch: [2][196/196] Elapsed 11m 33s (remain 0m 0s) Loss: 0.9846(1.2453) Grad: 18.0435  LR: 0.00000016  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 1.4301539659500122\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 10s (remain 10m 23s) Loss: 0.7749(0.7907) Grad: 16.1755  LR: 0.00000014  \n","Epoch: [3][40/196] Elapsed 2m 21s (remain 9m 13s) Loss: 0.8135(0.7792) Grad: 17.3570  LR: 0.00000013  \n","Epoch: [3][60/196] Elapsed 3m 33s (remain 8m 3s) Loss: 0.6348(0.7681) Grad: 14.9275  LR: 0.00000011  \n","Epoch: [3][80/196] Elapsed 4m 44s (remain 6m 52s) Loss: 0.8876(0.7524) Grad: 17.1809  LR: 0.00000009  \n","Epoch: [3][100/196] Elapsed 5m 55s (remain 5m 41s) Loss: 0.5833(0.7282) Grad: 14.1860  LR: 0.00000008  \n","Epoch: [3][120/196] Elapsed 7m 6s (remain 4m 30s) Loss: 0.7840(0.7091) Grad: 16.1348  LR: 0.00000006  \n","Epoch: [3][140/196] Elapsed 8m 17s (remain 3m 19s) Loss: 0.5202(0.7009) Grad: 13.6946  LR: 0.00000004  \n","Epoch: [3][160/196] Elapsed 9m 29s (remain 2m 8s) Loss: 0.5489(0.6882) Grad: 13.4129  LR: 0.00000003  \n","Epoch: [3][180/196] Elapsed 10m 40s (remain 0m 56s) Loss: 0.5672(0.6770) Grad: 14.9448  LR: 0.00000001  \n","Epoch: [3][196/196] Elapsed 11m 36s (remain 0m 0s) Loss: 0.6055(0.6716) Grad: 15.2379  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 1.177595853805542\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-09-15 21:35:29,369]\u001b[0m Trial 4 finished with value: 1.177595853805542 and parameters: {'learning_rate': 3.8498404377033904e-07, 'layer_wise_learning_rate_decay': 0.7221476129529402, 'learning_rate_schduler': 'linear', 'reinit_layers': 2}. Best is trial 1 with value: 0.45570382475852966.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 12s (remain 10m 35s) Loss: 2.2792(2.4612) Grad: 21.3833  LR: 0.00000020  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 20s) Loss: 2.5460(2.4187) Grad: 21.7220  LR: 0.00000041  \n","Epoch: [1][60/196] Elapsed 3m 34s (remain 8m 7s) Loss: 1.9550(2.3407) Grad: 21.4315  LR: 0.00000061  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 1.6546(2.2281) Grad: 21.5187  LR: 0.00000081  \n","Epoch: [1][100/196] Elapsed 5m 58s (remain 5m 43s) Loss: 1.3316(2.0808) Grad: 20.3413  LR: 0.00000102  \n","Epoch: [1][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 0.8517(1.9025) Grad: 18.6450  LR: 0.00000119  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.4417(1.7069) Grad: 11.7520  LR: 0.00000118  \n","Epoch: [1][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.3019(1.5293) Grad: 8.5100  LR: 0.00000117  \n","Epoch: [1][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.2264(1.3873) Grad: 3.2246  LR: 0.00000114  \n","Epoch: [1][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1931(1.2923) Grad: 4.1717  LR: 0.00000111  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.6539306640625\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 11s (remain 10m 33s) Loss: 0.1993(0.2188) Grad: 2.1517  LR: 0.00000106  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 18s) Loss: 0.2048(0.2127) Grad: 2.7212  LR: 0.00000101  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.2201(0.2081) Grad: 4.2012  LR: 0.00000095  \n","Epoch: [2][80/196] Elapsed 4m 45s (remain 6m 54s) Loss: 0.1910(0.2014) Grad: 2.0642  LR: 0.00000088  \n","Epoch: [2][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1305(0.2016) Grad: 2.5815  LR: 0.00000081  \n","Epoch: [2][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.2225(0.1983) Grad: 4.5001  LR: 0.00000073  \n","Epoch: [2][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1875(0.1964) Grad: 4.1894  LR: 0.00000066  \n","Epoch: [2][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1341(0.1949) Grad: 3.0573  LR: 0.00000058  \n","Epoch: [2][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.1318(0.1931) Grad: 2.4290  LR: 0.00000050  \n","Epoch: [2][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1550(0.1929) Grad: 5.3703  LR: 0.00000044  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.605730414390564\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 12s (remain 10m 33s) Loss: 0.2002(0.1866) Grad: 4.3901  LR: 0.00000036  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.1505(0.1791) Grad: 1.7839  LR: 0.00000029  \n","Epoch: [3][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.1634(0.1839) Grad: 5.0811  LR: 0.00000022  \n","Epoch: [3][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.2117(0.1822) Grad: 3.3866  LR: 0.00000017  \n","Epoch: [3][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1285(0.1809) Grad: 2.6786  LR: 0.00000011  \n","Epoch: [3][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1569(0.1792) Grad: 1.8177  LR: 0.00000007  \n","Epoch: [3][140/196] Elapsed 8m 19s (remain 3m 19s) Loss: 0.1682(0.1791) Grad: 4.3127  LR: 0.00000004  \n","Epoch: [3][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.2089(0.1793) Grad: 3.6126  LR: 0.00000002  \n","Epoch: [3][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.2151(0.1797) Grad: 3.8528  LR: 0.00000000  \n","Epoch: [3][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1538(0.1791) Grad: 2.6972  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.5984465479850769\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-09-15 22:13:21,780]\u001b[0m Trial 5 finished with value: 0.5984465479850769 and parameters: {'learning_rate': 1.1896075168370157e-06, 'layer_wise_learning_rate_decay': 0.7212806415658086, 'learning_rate_schduler': 'cosine', 'reinit_layers': 3}. Best is trial 1 with value: 0.45570382475852966.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 12s (remain 10m 35s) Loss: 2.4248(2.5588) Grad: 22.4338  LR: 0.00000004  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 2.7910(2.5485) Grad: 22.1282  LR: 0.00000008  \n","Epoch: [1][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 2.4471(2.5460) Grad: 22.0588  LR: 0.00000012  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 2.1733(2.5048) Grad: 22.1435  LR: 0.00000016  \n","Epoch: [1][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 2.3596(2.4730) Grad: 22.2139  LR: 0.00000020  \n","Epoch: [1][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 2.0947(2.4229) Grad: 22.2017  LR: 0.00000023  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 1.9021(2.3670) Grad: 22.2261  LR: 0.00000023  \n","Epoch: [1][160/196] Elapsed 9m 32s (remain 2m 8s) Loss: 1.6768(2.3123) Grad: 21.6222  LR: 0.00000022  \n","Epoch: [1][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 1.4424(2.2496) Grad: 21.6740  LR: 0.00000022  \n","Epoch: [1][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 1.5421(2.2009) Grad: 22.2576  LR: 0.00000021  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 2.0373640060424805\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 12s (remain 10m 34s) Loss: 1.4612(1.5167) Grad: 21.3867  LR: 0.00000020  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 1.2499(1.4624) Grad: 21.7104  LR: 0.00000019  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 1.3014(1.3889) Grad: 21.0756  LR: 0.00000018  \n","Epoch: [2][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.9356(1.3179) Grad: 20.7848  LR: 0.00000017  \n","Epoch: [2][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 1.2706(1.2550) Grad: 21.6890  LR: 0.00000016  \n","Epoch: [2][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.8575(1.1991) Grad: 19.7470  LR: 0.00000014  \n","Epoch: [2][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.7635(1.1436) Grad: 17.6699  LR: 0.00000013  \n","Epoch: [2][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.7791(1.0976) Grad: 18.5902  LR: 0.00000011  \n","Epoch: [2][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.5726(1.0531) Grad: 15.3509  LR: 0.00000010  \n","Epoch: [2][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.6864(1.0204) Grad: 16.8994  LR: 0.00000008  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 1.2470184564590454\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 11s (remain 10m 33s) Loss: 0.6546(0.6254) Grad: 17.6682  LR: 0.00000007  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 18s) Loss: 0.4922(0.6111) Grad: 16.3427  LR: 0.00000006  \n","Epoch: [3][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.4986(0.5993) Grad: 14.5627  LR: 0.00000004  \n","Epoch: [3][80/196] Elapsed 4m 45s (remain 6m 54s) Loss: 0.5609(0.5900) Grad: 14.9508  LR: 0.00000003  \n","Epoch: [3][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.4689(0.5943) Grad: 14.2525  LR: 0.00000002  \n","Epoch: [3][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.5776(0.5850) Grad: 14.7592  LR: 0.00000001  \n","Epoch: [3][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.5983(0.5814) Grad: 15.3451  LR: 0.00000001  \n","Epoch: [3][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.4190(0.5756) Grad: 11.9156  LR: 0.00000000  \n","Epoch: [3][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.6357(0.5761) Grad: 14.9253  LR: 0.00000000  \n","Epoch: [3][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.3736(0.5732) Grad: 11.7782  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 1.1141407489776611\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-09-15 22:51:14,205]\u001b[0m Trial 6 finished with value: 1.1141407489776611 and parameters: {'learning_rate': 2.2922957603103856e-07, 'layer_wise_learning_rate_decay': 0.7872546732613366, 'learning_rate_schduler': 'cosine', 'reinit_layers': 3}. Best is trial 1 with value: 0.45570382475852966.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 12s (remain 10m 39s) Loss: 2.2866(2.2442) Grad: 25.0934  LR: 0.00000055  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 21s) Loss: 1.6449(2.0800) Grad: 24.1665  LR: 0.00000110  \n","Epoch: [1][60/196] Elapsed 3m 35s (remain 8m 7s) Loss: 0.8250(1.7940) Grad: 20.3056  LR: 0.00000166  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 0.2524(1.4649) Grad: 7.4933  LR: 0.00000221  \n","Epoch: [1][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1931(1.2125) Grad: 3.9911  LR: 0.00000276  \n","Epoch: [1][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 0.2164(1.0484) Grad: 2.5684  LR: 0.00000321  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.3320(0.9262) Grad: 3.9656  LR: 0.00000307  \n","Epoch: [1][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.2017(0.8324) Grad: 4.7449  LR: 0.00000293  \n","Epoch: [1][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.1928(0.7586) Grad: 3.0671  LR: 0.00000280  \n","Epoch: [1][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1260(0.7098) Grad: 4.6098  LR: 0.00000269  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.5778746008872986\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 12s (remain 10m 34s) Loss: 0.1259(0.1569) Grad: 2.6451  LR: 0.00000255  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 20s) Loss: 0.1729(0.1579) Grad: 5.7194  LR: 0.00000241  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 7s) Loss: 0.1050(0.1548) Grad: 2.0682  LR: 0.00000227  \n","Epoch: [2][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 0.1220(0.1517) Grad: 2.6985  LR: 0.00000213  \n","Epoch: [2][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1979(0.1542) Grad: 6.6124  LR: 0.00000200  \n","Epoch: [2][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1475(0.1518) Grad: 3.0242  LR: 0.00000186  \n","Epoch: [2][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1040(0.1514) Grad: 3.8692  LR: 0.00000172  \n","Epoch: [2][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1177(0.1508) Grad: 3.2345  LR: 0.00000158  \n","Epoch: [2][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.1099(0.1481) Grad: 4.0045  LR: 0.00000145  \n","Epoch: [2][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.0910(0.1469) Grad: 5.6647  LR: 0.00000134  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.5204089879989624\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 12s (remain 10m 34s) Loss: 0.1290(0.1317) Grad: 3.6755  LR: 0.00000120  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.1475(0.1279) Grad: 3.1740  LR: 0.00000106  \n","Epoch: [3][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.1996(0.1286) Grad: 2.5732  LR: 0.00000092  \n","Epoch: [3][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.1540(0.1282) Grad: 2.5498  LR: 0.00000078  \n","Epoch: [3][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.0768(0.1265) Grad: 1.9484  LR: 0.00000065  \n","Epoch: [3][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1413(0.1274) Grad: 6.7915  LR: 0.00000051  \n","Epoch: [3][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.2095(0.1267) Grad: 3.3926  LR: 0.00000037  \n","Epoch: [3][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1584(0.1272) Grad: 3.9916  LR: 0.00000023  \n","Epoch: [3][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.0828(0.1275) Grad: 3.9993  LR: 0.00000010  \n","Epoch: [3][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1093(0.1275) Grad: 4.5207  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.5056667923927307\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-09-15 23:29:06,692]\u001b[0m Trial 7 finished with value: 0.5056667923927307 and parameters: {'learning_rate': 3.2291716682826884e-06, 'layer_wise_learning_rate_decay': 0.7547056904289499, 'learning_rate_schduler': 'linear', 'reinit_layers': 4}. Best is trial 1 with value: 0.45570382475852966.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 12s (remain 10m 37s) Loss: 0.3207(1.3670) Grad: 10.5144  LR: 0.00000891  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 20s) Loss: 0.2283(0.8161) Grad: 2.8694  LR: 0.00001781  \n","Epoch: [1][60/196] Elapsed 3m 34s (remain 8m 7s) Loss: 0.2030(0.6093) Grad: 4.4690  LR: 0.00002672  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 0.1216(0.4948) Grad: 3.6565  LR: 0.00003563  \n","Epoch: [1][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1262(0.4214) Grad: 4.3316  LR: 0.00004453  \n","Epoch: [1][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 0.1467(0.3738) Grad: 3.4700  LR: 0.00005177  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1391(0.3393) Grad: 4.8795  LR: 0.00004955  \n","Epoch: [1][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1094(0.3154) Grad: 3.3148  LR: 0.00004733  \n","Epoch: [1][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.1142(0.2938) Grad: 2.3177  LR: 0.00004511  \n","Epoch: [1][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1448(0.2790) Grad: 2.5184  LR: 0.00004333  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.4658414423465729\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 12s (remain 10m 35s) Loss: 0.1412(0.1119) Grad: 2.9718  LR: 0.00004111  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.1538(0.1058) Grad: 5.0827  LR: 0.00003888  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.0673(0.1027) Grad: 1.0732  LR: 0.00003666  \n","Epoch: [2][80/196] Elapsed 4m 45s (remain 6m 54s) Loss: 0.1454(0.1046) Grad: 5.7957  LR: 0.00003444  \n","Epoch: [2][100/196] Elapsed 5m 56s (remain 5m 42s) Loss: 0.0906(0.1037) Grad: 2.1349  LR: 0.00003222  \n","Epoch: [2][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1177(0.1031) Grad: 2.8419  LR: 0.00003000  \n","Epoch: [2][140/196] Elapsed 8m 19s (remain 3m 19s) Loss: 0.0918(0.1032) Grad: 2.5247  LR: 0.00002777  \n","Epoch: [2][160/196] Elapsed 9m 30s (remain 2m 8s) Loss: 0.0955(0.1017) Grad: 2.9787  LR: 0.00002555  \n","Epoch: [2][180/196] Elapsed 10m 41s (remain 0m 57s) Loss: 0.1386(0.1013) Grad: 3.7996  LR: 0.00002333  \n","Epoch: [2][196/196] Elapsed 11m 37s (remain 0m 0s) Loss: 0.0761(0.1007) Grad: 2.3506  LR: 0.00002155  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.4809825122356415\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 11s (remain 10m 27s) Loss: 0.1238(0.0825) Grad: 3.0641  LR: 0.00001933  \n","Epoch: [3][40/196] Elapsed 2m 22s (remain 9m 16s) Loss: 0.0481(0.0750) Grad: 1.0917  LR: 0.00001711  \n","Epoch: [3][60/196] Elapsed 3m 33s (remain 8m 4s) Loss: 0.0764(0.0723) Grad: 13.2708  LR: 0.00001489  \n","Epoch: [3][80/196] Elapsed 4m 45s (remain 6m 53s) Loss: 0.0667(0.0711) Grad: 1.3312  LR: 0.00001267  \n","Epoch: [3][100/196] Elapsed 5m 56s (remain 5m 42s) Loss: 0.0566(0.0704) Grad: 1.6026  LR: 0.00001044  \n","Epoch: [3][120/196] Elapsed 7m 7s (remain 4m 30s) Loss: 0.0678(0.0705) Grad: 1.9761  LR: 0.00000822  \n","Epoch: [3][140/196] Elapsed 8m 19s (remain 3m 19s) Loss: 0.0629(0.0704) Grad: 1.0430  LR: 0.00000600  \n","Epoch: [3][160/196] Elapsed 9m 30s (remain 2m 8s) Loss: 0.0634(0.0694) Grad: 2.0001  LR: 0.00000378  \n","Epoch: [3][180/196] Elapsed 10m 41s (remain 0m 57s) Loss: 0.0580(0.0689) Grad: 1.1229  LR: 0.00000156  \n","Epoch: [3][196/196] Elapsed 11m 37s (remain 0m 0s) Loss: 0.0636(0.0685) Grad: 1.6951  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.46920451521873474\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-09-16 00:06:42,044]\u001b[0m Trial 8 finished with value: 0.4658414423465729 and parameters: {'learning_rate': 5.210470632097099e-05, 'layer_wise_learning_rate_decay': 0.9815766262345241, 'learning_rate_schduler': 'linear', 'reinit_layers': 3}. Best is trial 1 with value: 0.45570382475852966.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 11s (remain 10m 33s) Loss: 2.4134(2.3962) Grad: 42.6502  LR: 0.00000033  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 1.9811(2.2472) Grad: 41.9844  LR: 0.00000065  \n","Epoch: [1][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 1.2317(2.0337) Grad: 38.3061  LR: 0.00000098  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.4153(1.7435) Grad: 20.5296  LR: 0.00000130  \n","Epoch: [1][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.2168(1.4496) Grad: 4.8794  LR: 0.00000163  \n","Epoch: [1][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.2251(1.2432) Grad: 7.4450  LR: 0.00000189  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.3085(1.0970) Grad: 5.9484  LR: 0.00000181  \n","Epoch: [1][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1537(0.9886) Grad: 4.0654  LR: 0.00000173  \n","Epoch: [1][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.1554(0.8985) Grad: 4.1655  LR: 0.00000165  \n","Epoch: [1][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1577(0.8407) Grad: 9.5372  LR: 0.00000159  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.6422947645187378\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 12s (remain 10m 35s) Loss: 0.2311(0.2132) Grad: 8.4851  LR: 0.00000150  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 20s) Loss: 0.1534(0.2032) Grad: 6.1575  LR: 0.00000142  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 7s) Loss: 0.1873(0.2005) Grad: 3.4002  LR: 0.00000134  \n","Epoch: [2][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.1891(0.1982) Grad: 4.0219  LR: 0.00000126  \n","Epoch: [2][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1995(0.1948) Grad: 7.3894  LR: 0.00000118  \n","Epoch: [2][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 0.2233(0.1967) Grad: 6.1966  LR: 0.00000110  \n","Epoch: [2][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.2168(0.1968) Grad: 4.5611  LR: 0.00000102  \n","Epoch: [2][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.3456(0.1959) Grad: 6.2934  LR: 0.00000094  \n","Epoch: [2][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.1441(0.1973) Grad: 3.0419  LR: 0.00000085  \n","Epoch: [2][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1834(0.1985) Grad: 6.6247  LR: 0.00000079  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.6282711625099182\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 12s (remain 10m 35s) Loss: 0.1987(0.1992) Grad: 5.6614  LR: 0.00000071  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 20s) Loss: 0.1882(0.1972) Grad: 8.9606  LR: 0.00000063  \n","Epoch: [3][60/196] Elapsed 3m 35s (remain 8m 7s) Loss: 0.1728(0.1959) Grad: 4.6090  LR: 0.00000054  \n","Epoch: [3][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 0.1726(0.1911) Grad: 4.2487  LR: 0.00000046  \n","Epoch: [3][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.2255(0.1929) Grad: 10.1701  LR: 0.00000038  \n","Epoch: [3][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1595(0.1918) Grad: 5.8957  LR: 0.00000030  \n","Epoch: [3][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1906(0.1923) Grad: 3.3299  LR: 0.00000022  \n","Epoch: [3][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.2909(0.1951) Grad: 5.8572  LR: 0.00000014  \n","Epoch: [3][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.1992(0.1937) Grad: 7.1414  LR: 0.00000006  \n","Epoch: [3][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1868(0.1925) Grad: 4.9999  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.6222785711288452\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-09-16 00:44:35,900]\u001b[0m Trial 9 finished with value: 0.6222785711288452 and parameters: {'learning_rate': 1.906783363311194e-06, 'layer_wise_learning_rate_decay': 0.7418116385385011, 'learning_rate_schduler': 'linear', 'reinit_layers': 0}. Best is trial 1 with value: 0.45570382475852966.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 11s (remain 10m 33s) Loss: 2.6041(2.7005) Grad: 17.3163  LR: 0.00000175  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 18s) Loss: 1.3273(2.3319) Grad: 16.5500  LR: 0.00000350  \n","Epoch: [1][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.2629(1.7385) Grad: 4.1191  LR: 0.00000524  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.1850(1.3510) Grad: 2.8408  LR: 0.00000699  \n","Epoch: [1][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1194(1.1085) Grad: 4.0295  LR: 0.00000874  \n","Epoch: [1][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1170(0.9457) Grad: 4.0105  LR: 0.00001016  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.0956(0.8292) Grad: 1.9154  LR: 0.00000972  \n","Epoch: [1][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1318(0.7405) Grad: 3.9856  LR: 0.00000929  \n","Epoch: [1][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.0797(0.6709) Grad: 1.9032  LR: 0.00000885  \n","Epoch: [1][196/196] Elapsed 11m 37s (remain 0m 0s) Loss: 0.1035(0.6258) Grad: 3.6445  LR: 0.00000850  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.46816107630729675\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 11s (remain 10m 32s) Loss: 0.1045(0.1084) Grad: 2.9906  LR: 0.00000807  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 18s) Loss: 0.0956(0.1069) Grad: 2.4060  LR: 0.00000763  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 5s) Loss: 0.0850(0.1065) Grad: 2.0395  LR: 0.00000719  \n","Epoch: [2][80/196] Elapsed 4m 45s (remain 6m 54s) Loss: 0.0731(0.1066) Grad: 1.9639  LR: 0.00000676  \n","Epoch: [2][100/196] Elapsed 5m 57s (remain 5m 42s) Loss: 0.1191(0.1053) Grad: 3.0544  LR: 0.00000632  \n","Epoch: [2][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1163(0.1051) Grad: 5.3515  LR: 0.00000589  \n","Epoch: [2][140/196] Elapsed 8m 19s (remain 3m 19s) Loss: 0.0725(0.1045) Grad: 2.5590  LR: 0.00000545  \n","Epoch: [2][160/196] Elapsed 9m 30s (remain 2m 8s) Loss: 0.1170(0.1056) Grad: 4.4162  LR: 0.00000501  \n","Epoch: [2][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.1289(0.1050) Grad: 3.6293  LR: 0.00000458  \n","Epoch: [2][196/196] Elapsed 11m 37s (remain 0m 0s) Loss: 0.0936(0.1050) Grad: 3.2016  LR: 0.00000423  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.4653651714324951\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 11s (remain 10m 33s) Loss: 0.0870(0.0940) Grad: 3.3620  LR: 0.00000379  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 18s) Loss: 0.0854(0.0942) Grad: 1.7604  LR: 0.00000336  \n","Epoch: [3][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.0796(0.0952) Grad: 1.6269  LR: 0.00000292  \n","Epoch: [3][80/196] Elapsed 4m 45s (remain 6m 54s) Loss: 0.0869(0.0944) Grad: 6.0490  LR: 0.00000249  \n","Epoch: [3][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.0958(0.0936) Grad: 1.7152  LR: 0.00000205  \n","Epoch: [3][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1004(0.0949) Grad: 2.7248  LR: 0.00000161  \n","Epoch: [3][140/196] Elapsed 8m 19s (remain 3m 19s) Loss: 0.1171(0.0954) Grad: 2.8445  LR: 0.00000118  \n","Epoch: [3][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.0734(0.0950) Grad: 1.3139  LR: 0.00000074  \n","Epoch: [3][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.1056(0.0954) Grad: 1.4715  LR: 0.00000031  \n","Epoch: [3][196/196] Elapsed 11m 37s (remain 0m 0s) Loss: 0.0711(0.0946) Grad: 4.7139  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.4555629789829254\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-09-16 01:22:27,296]\u001b[0m Trial 10 finished with value: 0.4555629789829254 and parameters: {'learning_rate': 1.0223764573961562e-05, 'layer_wise_learning_rate_decay': 0.9089745817743605, 'learning_rate_schduler': 'polynomial', 'reinit_layers': 1}. Best is trial 10 with value: 0.4555629789829254.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 12s (remain 10m 34s) Loss: 2.3743(2.8769) Grad: 17.4244  LR: 0.00000177  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 1.2638(2.4915) Grad: 16.5317  LR: 0.00000354  \n","Epoch: [1][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.3078(1.8601) Grad: 5.0017  LR: 0.00000531  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 0.1489(1.4458) Grad: 5.7934  LR: 0.00000708  \n","Epoch: [1][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1392(1.1840) Grad: 3.4416  LR: 0.00000884  \n","Epoch: [1][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1509(1.0075) Grad: 6.5264  LR: 0.00001028  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1047(0.8811) Grad: 4.3910  LR: 0.00000984  \n","Epoch: [1][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1322(0.7849) Grad: 2.5689  LR: 0.00000940  \n","Epoch: [1][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.1091(0.7113) Grad: 2.3364  LR: 0.00000896  \n","Epoch: [1][196/196] Elapsed 11m 37s (remain 0m 0s) Loss: 0.1441(0.6635) Grad: 5.3982  LR: 0.00000861  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.47643914818763733\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 11s (remain 10m 32s) Loss: 0.1012(0.1081) Grad: 2.5777  LR: 0.00000816  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 18s) Loss: 0.0912(0.1041) Grad: 2.1873  LR: 0.00000772  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.0968(0.1061) Grad: 2.3527  LR: 0.00000728  \n","Epoch: [2][80/196] Elapsed 4m 45s (remain 6m 54s) Loss: 0.0991(0.1091) Grad: 6.3720  LR: 0.00000684  \n","Epoch: [2][100/196] Elapsed 5m 56s (remain 5m 42s) Loss: 0.1096(0.1072) Grad: 3.5011  LR: 0.00000640  \n","Epoch: [2][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1002(0.1060) Grad: 1.8169  LR: 0.00000596  \n","Epoch: [2][140/196] Elapsed 8m 19s (remain 3m 19s) Loss: 0.1067(0.1046) Grad: 3.3242  LR: 0.00000552  \n","Epoch: [2][160/196] Elapsed 9m 30s (remain 2m 8s) Loss: 0.1196(0.1045) Grad: 3.9553  LR: 0.00000508  \n","Epoch: [2][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.0960(0.1040) Grad: 2.9201  LR: 0.00000463  \n","Epoch: [2][196/196] Elapsed 11m 37s (remain 0m 0s) Loss: 0.1449(0.1046) Grad: 2.5175  LR: 0.00000428  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.46374818682670593\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 11s (remain 10m 33s) Loss: 0.0976(0.0933) Grad: 2.5091  LR: 0.00000384  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 18s) Loss: 0.0717(0.0953) Grad: 1.6668  LR: 0.00000340  \n","Epoch: [3][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.0905(0.0952) Grad: 2.0780  LR: 0.00000296  \n","Epoch: [3][80/196] Elapsed 4m 45s (remain 6m 54s) Loss: 0.0993(0.0943) Grad: 2.4717  LR: 0.00000252  \n","Epoch: [3][100/196] Elapsed 5m 57s (remain 5m 42s) Loss: 0.0463(0.0944) Grad: 1.1180  LR: 0.00000207  \n","Epoch: [3][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1152(0.0943) Grad: 2.5202  LR: 0.00000163  \n","Epoch: [3][140/196] Elapsed 8m 19s (remain 3m 19s) Loss: 0.1185(0.0944) Grad: 2.8980  LR: 0.00000119  \n","Epoch: [3][160/196] Elapsed 9m 30s (remain 2m 8s) Loss: 0.0831(0.0939) Grad: 1.7900  LR: 0.00000075  \n","Epoch: [3][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.0975(0.0935) Grad: 2.4463  LR: 0.00000031  \n","Epoch: [3][196/196] Elapsed 11m 37s (remain 0m 0s) Loss: 0.0905(0.0933) Grad: 3.1266  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.4552156627178192\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-09-16 02:00:17,411]\u001b[0m Trial 11 finished with value: 0.4552156627178192 and parameters: {'learning_rate': 1.0348080335764067e-05, 'layer_wise_learning_rate_decay': 0.9154759345739282, 'learning_rate_schduler': 'polynomial', 'reinit_layers': 1}. Best is trial 11 with value: 0.4552156627178192.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 11s (remain 10m 33s) Loss: 2.2095(2.6464) Grad: 17.1960  LR: 0.00000144  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 1.7338(2.3753) Grad: 16.3909  LR: 0.00000288  \n","Epoch: [1][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.2608(1.8709) Grad: 3.4001  LR: 0.00000432  \n","Epoch: [1][80/196] Elapsed 4m 45s (remain 6m 54s) Loss: 0.1422(1.4588) Grad: 2.9088  LR: 0.00000576  \n","Epoch: [1][100/196] Elapsed 5m 57s (remain 5m 42s) Loss: 0.1322(1.2014) Grad: 3.4412  LR: 0.00000721  \n","Epoch: [1][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1136(1.0239) Grad: 3.8666  LR: 0.00000838  \n","Epoch: [1][140/196] Elapsed 8m 19s (remain 3m 19s) Loss: 0.1123(0.8947) Grad: 2.4485  LR: 0.00000802  \n","Epoch: [1][160/196] Elapsed 9m 30s (remain 2m 8s) Loss: 0.0813(0.7970) Grad: 1.4254  LR: 0.00000766  \n","Epoch: [1][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.1009(0.7210) Grad: 2.8761  LR: 0.00000730  \n","Epoch: [1][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1665(0.6724) Grad: 5.7341  LR: 0.00000701  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.48826348781585693\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 12s (remain 10m 34s) Loss: 0.1128(0.1127) Grad: 4.0129  LR: 0.00000665  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.0990(0.1113) Grad: 2.1175  LR: 0.00000629  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.0855(0.1094) Grad: 4.3974  LR: 0.00000593  \n","Epoch: [2][80/196] Elapsed 4m 45s (remain 6m 54s) Loss: 0.1590(0.1092) Grad: 5.3067  LR: 0.00000557  \n","Epoch: [2][100/196] Elapsed 5m 56s (remain 5m 42s) Loss: 0.0753(0.1080) Grad: 3.6286  LR: 0.00000521  \n","Epoch: [2][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1043(0.1071) Grad: 1.5519  LR: 0.00000485  \n","Epoch: [2][140/196] Elapsed 8m 19s (remain 3m 19s) Loss: 0.1082(0.1069) Grad: 1.6616  LR: 0.00000449  \n","Epoch: [2][160/196] Elapsed 9m 30s (remain 2m 8s) Loss: 0.1240(0.1063) Grad: 2.0138  LR: 0.00000413  \n","Epoch: [2][180/196] Elapsed 10m 41s (remain 0m 57s) Loss: 0.1217(0.1055) Grad: 3.4128  LR: 0.00000378  \n","Epoch: [2][196/196] Elapsed 11m 37s (remain 0m 0s) Loss: 0.0975(0.1059) Grad: 4.5224  LR: 0.00000349  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.46488532423973083\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 11s (remain 10m 33s) Loss: 0.1062(0.0942) Grad: 2.8301  LR: 0.00000313  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 18s) Loss: 0.0912(0.0947) Grad: 1.8062  LR: 0.00000277  \n","Epoch: [3][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.0850(0.0965) Grad: 1.6934  LR: 0.00000241  \n","Epoch: [3][80/196] Elapsed 4m 45s (remain 6m 54s) Loss: 0.0704(0.0971) Grad: 2.5374  LR: 0.00000205  \n","Epoch: [3][100/196] Elapsed 5m 57s (remain 5m 42s) Loss: 0.1115(0.0989) Grad: 3.8503  LR: 0.00000169  \n","Epoch: [3][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1151(0.0976) Grad: 3.7103  LR: 0.00000133  \n","Epoch: [3][140/196] Elapsed 8m 19s (remain 3m 19s) Loss: 0.1007(0.0976) Grad: 3.6571  LR: 0.00000097  \n","Epoch: [3][160/196] Elapsed 9m 30s (remain 2m 8s) Loss: 0.0786(0.0969) Grad: 1.6071  LR: 0.00000061  \n","Epoch: [3][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.0859(0.0975) Grad: 2.3498  LR: 0.00000025  \n","Epoch: [3][196/196] Elapsed 11m 37s (remain 0m 0s) Loss: 0.1140(0.0972) Grad: 2.4598  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.4534367322921753\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-09-16 02:38:07,724]\u001b[0m Trial 12 finished with value: 0.4534367322921753 and parameters: {'learning_rate': 8.430514132964566e-06, 'layer_wise_learning_rate_decay': 0.8931288973170695, 'learning_rate_schduler': 'polynomial', 'reinit_layers': 1}. Best is trial 12 with value: 0.4534367322921753.\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 12s (remain 10m 35s) Loss: 2.2832(2.5220) Grad: 19.9522  LR: 0.00000120  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.9825(2.1572) Grad: 18.0501  LR: 0.00000240  \n","Epoch: [1][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.2484(1.6441) Grad: 6.3193  LR: 0.00000360  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.2434(1.2891) Grad: 7.0590  LR: 0.00000480  \n","Epoch: [1][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1364(1.0664) Grad: 1.7462  LR: 0.00000600  \n","Epoch: [1][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1184(0.9148) Grad: 2.1593  LR: 0.00000698  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1196(0.8032) Grad: 2.8526  LR: 0.00000668  \n","Epoch: [1][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1685(0.7180) Grad: 2.9121  LR: 0.00000638  \n","Epoch: [1][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.1005(0.6521) Grad: 2.3067  LR: 0.00000608  \n","Epoch: [1][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.0842(0.6079) Grad: 2.0194  LR: 0.00000584  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.47588440775871277\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 12s (remain 10m 34s) Loss: 0.1036(0.1088) Grad: 2.2729  LR: 0.00000554  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.1125(0.1112) Grad: 2.4259  LR: 0.00000524  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.1104(0.1098) Grad: 3.5255  LR: 0.00000494  \n","Epoch: [2][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.1016(0.1099) Grad: 2.0838  LR: 0.00000464  \n","Epoch: [2][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.0914(0.1091) Grad: 3.1226  LR: 0.00000434  \n","Epoch: [2][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1214(0.1096) Grad: 3.7896  LR: 0.00000404  \n","Epoch: [2][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1371(0.1100) Grad: 3.1533  LR: 0.00000374  \n","Epoch: [2][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1407(0.1101) Grad: 5.1700  LR: 0.00000345  \n","Epoch: [2][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.1098(0.1097) Grad: 3.6464  LR: 0.00000315  \n","Epoch: [2][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.0810(0.1093) Grad: 3.0437  LR: 0.00000291  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.4614075720310211\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 12s (remain 10m 36s) Loss: 0.0832(0.0988) Grad: 1.7497  LR: 0.00000261  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.0825(0.1018) Grad: 2.8538  LR: 0.00000231  \n","Epoch: [3][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.0850(0.1013) Grad: 2.5458  LR: 0.00000201  \n","Epoch: [3][80/196] Elapsed 4m 45s (remain 6m 54s) Loss: 0.1082(0.1031) Grad: 3.0985  LR: 0.00000171  \n","Epoch: [3][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.0800(0.1014) Grad: 2.0545  LR: 0.00000141  \n","Epoch: [3][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.0955(0.1010) Grad: 2.0914  LR: 0.00000111  \n","Epoch: [3][140/196] Elapsed 8m 19s (remain 3m 19s) Loss: 0.1327(0.1007) Grad: 3.5045  LR: 0.00000081  \n","Epoch: [3][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.0836(0.1007) Grad: 2.7945  LR: 0.00000051  \n","Epoch: [3][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.0766(0.1009) Grad: 1.3957  LR: 0.00000021  \n","Epoch: [3][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1292(0.1011) Grad: 3.2420  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.4558442533016205\n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-09-16 03:15:59,833]\u001b[0m Trial 13 finished with value: 0.4558442533016205 and parameters: {'learning_rate': 7.0242596937025054e-06, 'layer_wise_learning_rate_decay': 0.8529699795029209, 'learning_rate_schduler': 'polynomial', 'reinit_layers': 2}. Best is trial 12 with value: 0.4534367322921753.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 11s (remain 10m 33s) Loss: 2.1400(2.5713) Grad: 17.4885  LR: 0.00000291  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 18s) Loss: 0.6804(1.9668) Grad: 13.8776  LR: 0.00000582  \n","Epoch: [1][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.1551(1.3973) Grad: 3.0829  LR: 0.00000873  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.1315(1.0872) Grad: 2.0704  LR: 0.00001164  \n","Epoch: [1][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1088(0.8941) Grad: 3.2032  LR: 0.00001455  \n","Epoch: [1][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 0.1265(0.7645) Grad: 3.1294  LR: 0.00001692  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1299(0.6719) Grad: 3.3602  LR: 0.00001619  \n","Epoch: [1][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1288(0.6026) Grad: 5.2275  LR: 0.00001547  \n","Epoch: [1][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.1380(0.5489) Grad: 3.1907  LR: 0.00001474  \n","Epoch: [1][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1286(0.5138) Grad: 4.7090  LR: 0.00001416  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.4781307876110077\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 12s (remain 10m 34s) Loss: 0.1133(0.1156) Grad: 2.7888  LR: 0.00001343  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.1179(0.1092) Grad: 4.2042  LR: 0.00001271  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.1016(0.1091) Grad: 2.3963  LR: 0.00001198  \n","Epoch: [2][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.1279(0.1059) Grad: 4.1085  LR: 0.00001126  \n","Epoch: [2][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.0824(0.1051) Grad: 1.9376  LR: 0.00001053  \n","Epoch: [2][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.0853(0.1038) Grad: 2.1230  LR: 0.00000980  \n","Epoch: [2][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1026(0.1038) Grad: 2.5393  LR: 0.00000908  \n","Epoch: [2][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1151(0.1032) Grad: 1.9860  LR: 0.00000835  \n","Epoch: [2][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.1054(0.1032) Grad: 2.0212  LR: 0.00000763  \n","Epoch: [2][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1110(0.1031) Grad: 4.0881  LR: 0.00000704  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.4505164325237274\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 12s (remain 10m 33s) Loss: 0.0883(0.0911) Grad: 2.5097  LR: 0.00000632  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.0936(0.0934) Grad: 2.3094  LR: 0.00000559  \n","Epoch: [3][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.0893(0.0932) Grad: 2.4045  LR: 0.00000487  \n","Epoch: [3][80/196] Elapsed 4m 45s (remain 6m 54s) Loss: 0.0827(0.0925) Grad: 1.8601  LR: 0.00000414  \n","Epoch: [3][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.0927(0.0915) Grad: 2.7630  LR: 0.00000341  \n","Epoch: [3][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.0770(0.0912) Grad: 1.7004  LR: 0.00000269  \n","Epoch: [3][140/196] Elapsed 8m 19s (remain 3m 19s) Loss: 0.1044(0.0930) Grad: 1.9994  LR: 0.00000196  \n","Epoch: [3][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.0978(0.0923) Grad: 1.8730  LR: 0.00000124  \n","Epoch: [3][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.0600(0.0916) Grad: 2.0002  LR: 0.00000051  \n","Epoch: [3][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1216(0.0914) Grad: 2.3942  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.4489534795284271\n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-09-16 03:53:53,804]\u001b[0m Trial 14 finished with value: 0.4489534795284271 and parameters: {'learning_rate': 1.7027701828938127e-05, 'layer_wise_learning_rate_decay': 0.8780935762069765, 'learning_rate_schduler': 'polynomial', 'reinit_layers': 1}. Best is trial 14 with value: 0.4489534795284271.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 11s (remain 10m 33s) Loss: 1.9800(2.4633) Grad: 18.8299  LR: 0.00000371  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 18s) Loss: 0.2941(1.6175) Grad: 4.2475  LR: 0.00000741  \n","Epoch: [1][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.2280(1.1579) Grad: 6.2789  LR: 0.00001112  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.1413(0.9121) Grad: 2.9273  LR: 0.00001483  \n","Epoch: [1][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1081(0.7558) Grad: 4.2234  LR: 0.00001853  \n","Epoch: [1][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1060(0.6517) Grad: 1.9142  LR: 0.00002154  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1474(0.5752) Grad: 4.3052  LR: 0.00002062  \n","Epoch: [1][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1363(0.5175) Grad: 2.5704  LR: 0.00001970  \n","Epoch: [1][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.1022(0.4732) Grad: 2.9929  LR: 0.00001877  \n","Epoch: [1][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1170(0.4437) Grad: 3.3068  LR: 0.00001803  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.46865782141685486\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 11s (remain 10m 33s) Loss: 0.1177(0.1095) Grad: 2.9268  LR: 0.00001711  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.1114(0.1127) Grad: 3.5254  LR: 0.00001618  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.1159(0.1092) Grad: 4.7581  LR: 0.00001526  \n","Epoch: [2][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.1247(0.1060) Grad: 4.7952  LR: 0.00001433  \n","Epoch: [2][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.0920(0.1061) Grad: 3.2497  LR: 0.00001341  \n","Epoch: [2][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.0972(0.1048) Grad: 3.6228  LR: 0.00001248  \n","Epoch: [2][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.0883(0.1045) Grad: 4.4634  LR: 0.00001156  \n","Epoch: [2][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.0866(0.1044) Grad: 2.3811  LR: 0.00001063  \n","Epoch: [2][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.0850(0.1043) Grad: 3.1639  LR: 0.00000971  \n","Epoch: [2][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1086(0.1040) Grad: 4.3189  LR: 0.00000897  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.457609087228775\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 12s (remain 10m 33s) Loss: 0.0938(0.0909) Grad: 1.4267  LR: 0.00000805  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 18s) Loss: 0.1166(0.0944) Grad: 2.4728  LR: 0.00000712  \n","Epoch: [3][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.1084(0.0940) Grad: 2.5473  LR: 0.00000620  \n","Epoch: [3][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.1015(0.0940) Grad: 1.9624  LR: 0.00000527  \n","Epoch: [3][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1003(0.0935) Grad: 3.2686  LR: 0.00000435  \n","Epoch: [3][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 0.0894(0.0935) Grad: 3.6684  LR: 0.00000342  \n","Epoch: [3][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.0790(0.0925) Grad: 1.7721  LR: 0.00000250  \n","Epoch: [3][160/196] Elapsed 9m 32s (remain 2m 8s) Loss: 0.0855(0.0923) Grad: 1.9995  LR: 0.00000157  \n","Epoch: [3][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.0809(0.0919) Grad: 2.2571  LR: 0.00000065  \n","Epoch: [3][196/196] Elapsed 11m 39s (remain 0m 0s) Loss: 0.0944(0.0914) Grad: 1.9904  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.45275118947029114\n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-09-16 04:31:47,643]\u001b[0m Trial 15 finished with value: 0.45275118947029114 and parameters: {'learning_rate': 2.168343678854504e-05, 'layer_wise_learning_rate_decay': 0.8571961513207124, 'learning_rate_schduler': 'polynomial', 'reinit_layers': 2}. Best is trial 14 with value: 0.4489534795284271.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 11s (remain 10m 33s) Loss: 1.6403(2.2064) Grad: 18.6384  LR: 0.00000407  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.3045(1.4024) Grad: 6.8586  LR: 0.00000814  \n","Epoch: [1][60/196] Elapsed 3m 34s (remain 8m 7s) Loss: 0.2466(1.0238) Grad: 7.4947  LR: 0.00001221  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 0.1050(0.8146) Grad: 3.3463  LR: 0.00001627  \n","Epoch: [1][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.0963(0.6806) Grad: 2.7758  LR: 0.00002034  \n","Epoch: [1][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1422(0.5868) Grad: 7.0914  LR: 0.00002365  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.0890(0.5190) Grad: 2.9379  LR: 0.00002263  \n","Epoch: [1][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1512(0.4687) Grad: 6.9918  LR: 0.00002162  \n","Epoch: [1][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.1182(0.4297) Grad: 2.8159  LR: 0.00002060  \n","Epoch: [1][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1106(0.4046) Grad: 5.8287  LR: 0.00001979  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.4756709635257721\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 12s (remain 10m 33s) Loss: 0.1288(0.1025) Grad: 4.7647  LR: 0.00001878  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.0967(0.1105) Grad: 2.5080  LR: 0.00001776  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.1079(0.1107) Grad: 4.6046  LR: 0.00001675  \n","Epoch: [2][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.0812(0.1079) Grad: 2.7773  LR: 0.00001573  \n","Epoch: [2][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1002(0.1083) Grad: 2.1494  LR: 0.00001472  \n","Epoch: [2][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.0918(0.1081) Grad: 1.3823  LR: 0.00001370  \n","Epoch: [2][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.0927(0.1066) Grad: 1.7549  LR: 0.00001269  \n","Epoch: [2][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1210(0.1064) Grad: 2.9008  LR: 0.00001167  \n","Epoch: [2][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.0890(0.1057) Grad: 2.0320  LR: 0.00001066  \n","Epoch: [2][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.0949(0.1054) Grad: 2.4464  LR: 0.00000985  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.45744648575782776\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 12s (remain 10m 34s) Loss: 0.1002(0.0947) Grad: 4.4672  LR: 0.00000883  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.0852(0.0946) Grad: 1.9895  LR: 0.00000782  \n","Epoch: [3][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.0751(0.0922) Grad: 2.7943  LR: 0.00000680  \n","Epoch: [3][80/196] Elapsed 4m 46s (remain 6m 54s) Loss: 0.1031(0.0936) Grad: 3.1522  LR: 0.00000579  \n","Epoch: [3][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1003(0.0944) Grad: 2.0783  LR: 0.00000477  \n","Epoch: [3][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1017(0.0933) Grad: 2.4107  LR: 0.00000376  \n","Epoch: [3][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1102(0.0947) Grad: 1.6564  LR: 0.00000274  \n","Epoch: [3][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1093(0.0943) Grad: 3.4920  LR: 0.00000173  \n","Epoch: [3][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.0728(0.0944) Grad: 1.9756  LR: 0.00000071  \n","Epoch: [3][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.0853(0.0940) Grad: 2.7310  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.454280823469162\n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-09-16 05:09:42,784]\u001b[0m Trial 16 finished with value: 0.454280823469162 and parameters: {'learning_rate': 2.3800759968097607e-05, 'layer_wise_learning_rate_decay': 0.8363549913652786, 'learning_rate_schduler': 'polynomial', 'reinit_layers': 2}. Best is trial 14 with value: 0.4489534795284271.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 12s (remain 10m 37s) Loss: 0.3605(1.4878) Grad: 9.8687  LR: 0.00001317  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 21s) Loss: 0.2062(0.9130) Grad: 3.3334  LR: 0.00002635  \n","Epoch: [1][60/196] Elapsed 3m 35s (remain 8m 8s) Loss: 0.1867(0.6777) Grad: 5.4035  LR: 0.00003952  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 0.1797(0.5494) Grad: 4.0744  LR: 0.00005270  \n","Epoch: [1][100/196] Elapsed 5m 58s (remain 5m 43s) Loss: 0.1112(0.4646) Grad: 3.0191  LR: 0.00006587  \n","Epoch: [1][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 0.0880(0.4108) Grad: 4.0185  LR: 0.00007658  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1163(0.3732) Grad: 3.6453  LR: 0.00007329  \n","Epoch: [1][160/196] Elapsed 9m 32s (remain 2m 8s) Loss: 0.1144(0.3429) Grad: 3.1968  LR: 0.00007001  \n","Epoch: [1][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.1454(0.3208) Grad: 3.1167  LR: 0.00006672  \n","Epoch: [1][196/196] Elapsed 11m 39s (remain 0m 0s) Loss: 0.0859(0.3040) Grad: 2.0352  LR: 0.00006409  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.47545692324638367\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 12s (remain 10m 34s) Loss: 0.1142(0.1098) Grad: 2.4458  LR: 0.00006081  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.1124(0.1048) Grad: 2.4328  LR: 0.00005752  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 7s) Loss: 0.1339(0.1068) Grad: 2.4571  LR: 0.00005423  \n","Epoch: [2][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 0.1097(0.1059) Grad: 4.3693  LR: 0.00005095  \n","Epoch: [2][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1115(0.1057) Grad: 2.6906  LR: 0.00004766  \n","Epoch: [2][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1089(0.1064) Grad: 1.5993  LR: 0.00004437  \n","Epoch: [2][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.0943(0.1061) Grad: 2.8761  LR: 0.00004109  \n","Epoch: [2][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.1160(0.1071) Grad: 3.1131  LR: 0.00003780  \n","Epoch: [2][180/196] Elapsed 10m 42s (remain 0m 57s) Loss: 0.1117(0.1069) Grad: 4.1096  LR: 0.00003451  \n","Epoch: [2][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.0799(0.1060) Grad: 4.3520  LR: 0.00003189  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.47311124205589294\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 12s (remain 10m 34s) Loss: 0.0595(0.0928) Grad: 1.4375  LR: 0.00002860  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 20s) Loss: 0.0717(0.0908) Grad: 1.8806  LR: 0.00002531  \n","Epoch: [3][60/196] Elapsed 3m 35s (remain 8m 7s) Loss: 0.0850(0.0928) Grad: 1.1531  LR: 0.00002203  \n","Epoch: [3][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 0.0896(0.0922) Grad: 3.4903  LR: 0.00001874  \n","Epoch: [3][100/196] Elapsed 5m 58s (remain 5m 43s) Loss: 0.1080(0.0908) Grad: 2.9320  LR: 0.00001545  \n","Epoch: [3][120/196] Elapsed 7m 9s (remain 4m 32s) Loss: 0.0659(0.0899) Grad: 1.8172  LR: 0.00001217  \n","Epoch: [3][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.0794(0.0897) Grad: 1.8653  LR: 0.00000888  \n","Epoch: [3][160/196] Elapsed 9m 32s (remain 2m 8s) Loss: 0.0748(0.0893) Grad: 0.9444  LR: 0.00000559  \n","Epoch: [3][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.0754(0.0884) Grad: 1.9118  LR: 0.00000231  \n","Epoch: [3][196/196] Elapsed 11m 39s (remain 0m 0s) Loss: 0.0706(0.0877) Grad: 1.3742  LR: 0.00000001  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.45891880989074707\n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-09-16 05:47:37,855]\u001b[0m Trial 17 finished with value: 0.45891880989074707 and parameters: {'learning_rate': 7.707324684449108e-05, 'layer_wise_learning_rate_decay': 0.8339476407176097, 'learning_rate_schduler': 'polynomial', 'reinit_layers': 4}. Best is trial 14 with value: 0.4489534795284271.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 12s (remain 10m 36s) Loss: 2.6135(2.7590) Grad: 19.8303  LR: 0.00000063  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 20s) Loss: 2.0224(2.5872) Grad: 19.9114  LR: 0.00000127  \n","Epoch: [1][60/196] Elapsed 3m 34s (remain 8m 7s) Loss: 1.1023(2.2502) Grad: 18.6274  LR: 0.00000190  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 0.2280(1.8308) Grad: 6.7808  LR: 0.00000254  \n","Epoch: [1][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.2152(1.5102) Grad: 3.8052  LR: 0.00000317  \n","Epoch: [1][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 0.2582(1.2887) Grad: 2.1512  LR: 0.00000369  \n","Epoch: [1][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1642(1.1284) Grad: 2.3194  LR: 0.00000353  \n","Epoch: [1][160/196] Elapsed 9m 32s (remain 2m 8s) Loss: 0.1995(1.0067) Grad: 3.5046  LR: 0.00000337  \n","Epoch: [1][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.1004(0.9093) Grad: 2.2256  LR: 0.00000321  \n","Epoch: [1][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1101(0.8450) Grad: 3.2673  LR: 0.00000309  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.5040845274925232\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 11s (remain 10m 32s) Loss: 0.1302(0.1370) Grad: 4.6800  LR: 0.00000293  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 18s) Loss: 0.1029(0.1281) Grad: 2.6356  LR: 0.00000277  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 6s) Loss: 0.1094(0.1236) Grad: 3.4926  LR: 0.00000261  \n","Epoch: [2][80/196] Elapsed 4m 45s (remain 6m 54s) Loss: 0.1026(0.1220) Grad: 2.7578  LR: 0.00000245  \n","Epoch: [2][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.1127(0.1193) Grad: 1.9870  LR: 0.00000230  \n","Epoch: [2][120/196] Elapsed 7m 8s (remain 4m 31s) Loss: 0.1337(0.1185) Grad: 1.4891  LR: 0.00000214  \n","Epoch: [2][140/196] Elapsed 8m 19s (remain 3m 19s) Loss: 0.0926(0.1165) Grad: 1.7667  LR: 0.00000198  \n","Epoch: [2][160/196] Elapsed 9m 31s (remain 2m 8s) Loss: 0.0872(0.1157) Grad: 2.6406  LR: 0.00000182  \n","Epoch: [2][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.1168(0.1156) Grad: 3.2060  LR: 0.00000166  \n","Epoch: [2][196/196] Elapsed 11m 38s (remain 0m 0s) Loss: 0.1321(0.1152) Grad: 3.3444  LR: 0.00000154  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.47307947278022766\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 12s (remain 10m 35s) Loss: 0.1289(0.1152) Grad: 2.1874  LR: 0.00000138  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 20s) Loss: 0.1038(0.1116) Grad: 2.9944  LR: 0.00000122  \n","Epoch: [3][60/196] Elapsed 3m 35s (remain 8m 7s) Loss: 0.0955(0.1102) Grad: 2.1210  LR: 0.00000106  \n","Epoch: [3][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 0.1150(0.1095) Grad: 2.8214  LR: 0.00000090  \n","Epoch: [3][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.0709(0.1080) Grad: 2.8964  LR: 0.00000074  \n","Epoch: [3][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 0.0707(0.1064) Grad: 1.8118  LR: 0.00000059  \n","Epoch: [3][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.1164(0.1070) Grad: 3.5446  LR: 0.00000043  \n","Epoch: [3][160/196] Elapsed 9m 32s (remain 2m 8s) Loss: 0.1275(0.1066) Grad: 3.0602  LR: 0.00000027  \n","Epoch: [3][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.1059(0.1067) Grad: 3.2053  LR: 0.00000011  \n","Epoch: [3][196/196] Elapsed 11m 39s (remain 0m 0s) Loss: 0.1225(0.1070) Grad: 3.1221  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.46429410576820374\n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-09-16 06:25:32,764]\u001b[0m Trial 18 finished with value: 0.46429410576820374 and parameters: {'learning_rate': 3.712200194342814e-06, 'layer_wise_learning_rate_decay': 0.873797238234208, 'learning_rate_schduler': 'polynomial', 'reinit_layers': 2}. Best is trial 14 with value: 0.4489534795284271.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 1m 12s (remain 10m 36s) Loss: 1.6407(2.3579) Grad: 19.6849  LR: 0.00000342  \n","Epoch: [1][40/196] Elapsed 2m 23s (remain 9m 21s) Loss: 0.1692(1.6092) Grad: 2.6283  LR: 0.00000683  \n","Epoch: [1][60/196] Elapsed 3m 35s (remain 8m 8s) Loss: 0.1660(1.1529) Grad: 1.6743  LR: 0.00001025  \n","Epoch: [1][80/196] Elapsed 4m 46s (remain 6m 56s) Loss: 0.1442(0.9106) Grad: 2.9029  LR: 0.00001366  \n","Epoch: [1][100/196] Elapsed 5m 58s (remain 5m 44s) Loss: 0.1284(0.7553) Grad: 5.7241  LR: 0.00001708  \n","Epoch: [1][120/196] Elapsed 7m 10s (remain 4m 32s) Loss: 0.0961(0.6498) Grad: 6.2203  LR: 0.00001985  \n","Epoch: [1][140/196] Elapsed 8m 21s (remain 3m 20s) Loss: 0.0909(0.5737) Grad: 2.3681  LR: 0.00001900  \n","Epoch: [1][160/196] Elapsed 9m 33s (remain 2m 8s) Loss: 0.0931(0.5165) Grad: 2.3086  LR: 0.00001815  \n","Epoch: [1][180/196] Elapsed 10m 44s (remain 0m 57s) Loss: 0.1277(0.4712) Grad: 2.2002  LR: 0.00001730  \n","Epoch: [1][196/196] Elapsed 11m 40s (remain 0m 0s) Loss: 0.1350(0.4426) Grad: 7.0962  LR: 0.00001661  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49]\n","Epoch: [1][40/49]\n","Epoch: [1][49/49]\n","\n","The valid loss for the current epoch is 0.4734232425689697\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 1m 12s (remain 10m 34s) Loss: 0.1437(0.1073) Grad: 5.1518  LR: 0.00001576  \n","Epoch: [2][40/196] Elapsed 2m 23s (remain 9m 19s) Loss: 0.0989(0.1068) Grad: 2.0834  LR: 0.00001491  \n","Epoch: [2][60/196] Elapsed 3m 34s (remain 8m 7s) Loss: 0.1130(0.1084) Grad: 1.8458  LR: 0.00001406  \n","Epoch: [2][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 0.0601(0.1073) Grad: 2.7524  LR: 0.00001321  \n","Epoch: [2][100/196] Elapsed 5m 57s (remain 5m 43s) Loss: 0.0799(0.1081) Grad: 2.2620  LR: 0.00001235  \n","Epoch: [2][120/196] Elapsed 7m 9s (remain 4m 31s) Loss: 0.1161(0.1072) Grad: 3.9139  LR: 0.00001150  \n","Epoch: [2][140/196] Elapsed 8m 20s (remain 3m 20s) Loss: 0.0785(0.1064) Grad: 2.0786  LR: 0.00001065  \n","Epoch: [2][160/196] Elapsed 9m 32s (remain 2m 8s) Loss: 0.0992(0.1067) Grad: 1.8319  LR: 0.00000980  \n","Epoch: [2][180/196] Elapsed 10m 43s (remain 0m 57s) Loss: 0.1497(0.1064) Grad: 3.3159  LR: 0.00000895  \n","Epoch: [2][196/196] Elapsed 11m 39s (remain 0m 0s) Loss: 0.0971(0.1060) Grad: 5.0950  LR: 0.00000827  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49]\n","Epoch: [2][40/49]\n","Epoch: [2][49/49]\n","\n","The valid loss for the current epoch is 0.4604106843471527\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 1m 12s (remain 10m 37s) Loss: 0.0755(0.0964) Grad: 1.3559  LR: 0.00000741  \n","Epoch: [3][40/196] Elapsed 2m 23s (remain 9m 21s) Loss: 0.0718(0.0967) Grad: 2.7638  LR: 0.00000656  \n","Epoch: [3][60/196] Elapsed 3m 35s (remain 8m 8s) Loss: 0.1429(0.1007) Grad: 2.4926  LR: 0.00000571  \n","Epoch: [3][80/196] Elapsed 4m 46s (remain 6m 55s) Loss: 0.0777(0.1008) Grad: 1.8732  LR: 0.00000486  \n","Epoch: [3][100/196] Elapsed 5m 58s (remain 5m 44s) Loss: 0.0910(0.0993) Grad: 2.6933  LR: 0.00000401  \n","Epoch: [3][120/196] Elapsed 7m 9s (remain 4m 32s) Loss: 0.0817(0.0985) Grad: 1.7557  LR: 0.00000315  \n","Epoch: [3][140/196] Elapsed 8m 21s (remain 3m 20s) Loss: 0.0957(0.0987) Grad: 1.7157  LR: 0.00000230  \n","Epoch: [3][160/196] Elapsed 9m 32s (remain 2m 8s) Loss: 0.0693(0.0981) Grad: 1.8602  LR: 0.00000145  \n","Epoch: [3][180/196] Elapsed 10m 44s (remain 0m 57s) Loss: 0.0841(0.0967) Grad: 2.0103  LR: 0.00000060  \n","Epoch: [3][196/196] Elapsed 11m 39s (remain 0m 0s) Loss: 0.0785(0.0962) Grad: 2.0554  LR: 0.00000000  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49]\n","Epoch: [3][40/49]\n","Epoch: [3][49/49]\n","\n","The valid loss for the current epoch is 0.45386430621147156\n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-09-16 07:03:31,355]\u001b[0m Trial 19 finished with value: 0.45386430621147156 and parameters: {'learning_rate': 1.9979077386798717e-05, 'layer_wise_learning_rate_decay': 0.8086618256858717, 'learning_rate_schduler': 'polynomial', 'reinit_layers': 2}. Best is trial 14 with value: 0.4489534795284271.\u001b[0m\n"]}]},{"cell_type":"code","source":["study.best_trial.params"],"metadata":{"id":"DJAJ-NHo2pB6","executionInfo":{"status":"ok","timestamp":1663311812700,"user_tz":-330,"elapsed":4,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"48b3c9ea-8def-4030-a3f6-15dd3a45bd4a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'learning_rate': 1.7027701828938127e-05,\n"," 'layer_wise_learning_rate_decay': 0.8780935762069765,\n"," 'learning_rate_schduler': 'polynomial',\n"," 'reinit_layers': 1}"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dpq4wadpEuWB"},"outputs":[],"source":["!cp -r deberta-v3-large/ /content/drive/MyDrive/Kaggle/FeedbackPrize3/"]},{"cell_type":"code","source":[],"metadata":{"id":"4ipyl8HZ1_QD"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"collapsed_sections":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"78b3947044744aef8bc663e4a8459a6e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3899339690284465afd9c4c6a8a3d898","IPY_MODEL_2253ade1fe794643a0f05b5125768206","IPY_MODEL_15f749cb359d48c69ecd38b72c3e9bb4"],"layout":"IPY_MODEL_086a3aa51f024b67b7f5679e0f7783e8"}},"3899339690284465afd9c4c6a8a3d898":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc7f10702c73405d92715da7a167a1d4","placeholder":"​","style":"IPY_MODEL_554866f838d242f7987def818c5463e7","value":""}},"2253ade1fe794643a0f05b5125768206":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c06fbc7ab3754edba4935ad0830aaab1","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_426520ce2a184502be147cb494300c1c","value":0}},"15f749cb359d48c69ecd38b72c3e9bb4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_05318d4db2834c8f8b17235554d3781c","placeholder":"​","style":"IPY_MODEL_01f0baa5ceec4f4baaa97c0024d8b9e3","value":" 0/0 [00:00&lt;?, ?it/s]"}},"086a3aa51f024b67b7f5679e0f7783e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc7f10702c73405d92715da7a167a1d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"554866f838d242f7987def818c5463e7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c06fbc7ab3754edba4935ad0830aaab1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"426520ce2a184502be147cb494300c1c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"05318d4db2834c8f8b17235554d3781c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01f0baa5ceec4f4baaa97c0024d8b9e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}