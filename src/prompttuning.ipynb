{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10731,"status":"ok","timestamp":1664732771475,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"},"user_tz":-330},"id":"wyNXCUpeOJ-r","outputId":"2e47ec79-ab8d-4797-9a21-24e0e14d9594"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.22.2-py3-none-any.whl (4.9 MB)\n","\u001b[K     |████████████████████████████████| 4.9 MB 31.0 MB/s \n","\u001b[?25hCollecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 78.8 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 95.2 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Collecting huggingface-hub<1.0,>=0.9.0\n","  Downloading huggingface_hub-0.10.0-py3-none-any.whl (163 kB)\n","\u001b[K     |████████████████████████████████| 163 kB 86.6 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Installing collected packages: tokenizers, huggingface-hub, transformers, sentencepiece\n","Successfully installed huggingface-hub-0.10.0 sentencepiece-0.1.97 tokenizers-0.12.1 transformers-4.22.2\n"]}],"source":["!pip install transformers sentencepiece"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":5151,"status":"ok","timestamp":1664732776624,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"},"user_tz":-330},"id":"xAya0Z97DtCR"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import os\n","from transformers import AutoModel, AutoConfig, AutoTokenizer, get_polynomial_decay_schedule_with_warmup, get_linear_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup\n","import pandas as pd\n","from transformers.optimization import Adafactor, AdafactorSchedule\n","from torch.cuda.amp import autocast, GradScaler\n","from sklearn.metrics import mean_squared_error\n","import random\n","import time\n","from torch.utils import checkpoint\n","import math\n","import gc\n","from text_unidecode import unidecode\n","from typing import Dict, List, Tuple\n","import codecs\n","import warnings\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":741,"status":"ok","timestamp":1664732891796,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"},"user_tz":-330},"id":"1wMfVtAqDtCT"},"outputs":[],"source":["import transformers\n","transformers.logging.set_verbosity_error()"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1664732892473,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"},"user_tz":-330},"id":"PQy468aCeRZj"},"outputs":[],"source":["warnings.simplefilter('ignore')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26628,"status":"ok","timestamp":1664732919096,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"},"user_tz":-330},"id":"XFRwjZi6OM3D","outputId":"d2a06ce0-df9c-40c3-989d-d7a172e38ced"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"guVaXLpzuJ-Q","executionInfo":{"status":"ok","timestamp":1664732919097,"user_tz":-330,"elapsed":6,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}},"outputId":"23095a30-1a1d-46b5-d2ad-0979f1794d72"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Oct  2 17:48:38 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   41C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1664732919097,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"},"user_tz":-330},"id":"P6go0UzRDtCT"},"outputs":[],"source":["def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    \n","seed_everything(seed=42)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1664732919097,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"},"user_tz":-330},"id":"m7F59_xBA8lY"},"outputs":[],"source":["#####\n","#### BEST PARAMS FOR THE DEBERTA V3 BASE MODEL\n","# params = {'learning_rate': 0.0002634969863920811, \n","#             'layer_wise_learning_rate_decay': 0.7867664854455205, \n","#             'learning_rate_schduler': 'polynomial', \n","#             'reinit_layers': 3}\n","#####"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":9030,"status":"ok","timestamp":1664732928124,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"},"user_tz":-330},"id":"_L1WdNpRDtCT","colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["734db5eedea642e7ad396eef1e863bc7","3e2cbd85f02546a9b7b1340a79801525","dfc58e738947458ab27b9a3df33486ba","a09b2ea88e724332952ef81f928865f0","95de51e446ca48e089fb2797826d9b49","cddf9355c4804c41a956e67d37fa8984","399f7aca8c5843a7a75cf9263420c039","7c2b7f42527844ed9b99578b24714359","6b6013e2541440ed8834a19e56d58890","a1b8afc91ad74acda40bed06db3d314c","33645c6cd7c6408c8d28793a963e74b5","261ba8ea966a4944987daab0caf7b1ac","814e1b20889c4ba0b39563334a668a22","7699ea2e82304354986e68297b5d8ffa","921547ff600e4096b8f1bc22e010d6ac","458f10ed9a174a3aa08a42ce8ff2f480","dfd73246bf4a451f80c85c6286a9bd00","ae20fd51fbb442a1b348d65db7d107bf","c1a1ca44bb1342748929c69ac05873cd","a8c618114abb47b0a9e0d2f44fa77c9f","86a4dba27d92482cb437454933f89995","c3ded0898c0a45c6bbd623d480e58daf","5f8a870112604dcd9f4e44a064b913fc","1854af30e1b744d9b8bd0c3019e92ab5","ed979a351d2443c6aba89a7c69bc78e6","db2a0965ebbb4b1694314caa3cc39e0b","bbb7fcd3ffa44292a262193b10046065","e695cd8987eb4406be8a9bde95f2b1fa","11f63e25611f47a8953c3cf23086084f","3244b6b29eb5484b94ee1f63e3d47a40","2713e32048df4e0fbd95e4e8016ac71b","e62abff7aff04902b3d585e4743746a0","18578bc64f9743f8a4624eb82a3d905b"]},"outputId":"45e5cad5-9be5-48e4-e36a-2e73a075fdec"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"734db5eedea642e7ad396eef1e863bc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/580 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"261ba8ea966a4944987daab0caf7b1ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f8a870112604dcd9f4e44a064b913fc"}},"metadata":{}}],"source":["class CFG:\n","    train_file = \"/content/drive/MyDrive/Kaggle/FeedbackPrize3/train_folds.csv\"\n","    fold = 0\n","    batch_size = 16\n","    num_workers = 4\n","    target_columns = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n","    hidden_dropout_prob = 0.0\n","    reinit_weights = False\n","    reinit_layers = 1\n","    lr = 0.001\n","    llrd = 0.9\n","    warmup_ratio = 0\n","    use_awp = False\n","    adv_lr = 0.0002\n","    adv_eps = 0.001\n","    model_name = \"microsoft/deberta-v3-large\"\n","    gradient_accumulation_steps = 1\n","    max_grad_norm = 10\n","    print_freq = 20\n","    epochs = 3\n","    n_tokens = 40\n","    specific_max_len = 768 - n_tokens\n","    token_dropout = False\n","    token_dropout_prob = 0.15\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    gradient_checkpointing_enable = True\n","    save_dir = \"deberta-v3-large\"\n","    save_model_name = \"deberta-v3-large\""]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1664732928124,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"},"user_tz":-330},"id":"f2V2bCiFe0ZJ"},"outputs":[],"source":["#Preprocessing Functions\n","\n","def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n","    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n","\n","\n","def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n","    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n","\n","# Register the encoding and decoding error handlers for `utf-8` and `cp1252`.\n","codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n","codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n","\n","def resolve_encodings_and_normalize(text: str) -> str:\n","    \"\"\"Resolve the encoding problems and normalize the abnormal characters.\"\"\"\n","    text = (\n","        text.encode(\"raw_unicode_escape\")\n","        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n","        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n","        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n","    )\n","    text = unidecode(text)\n","    return text"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1664732928124,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"},"user_tz":-330},"id":"UxvMRHfKDtCU"},"outputs":[],"source":["#Utiliy functions \n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","        \n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n","        \n","def MCRMSE(y_trues, y_preds):\n","    scores = []\n","    idxes = y_trues.shape[1]\n","    for i in range(idxes):\n","        y_true = y_trues[:,i]\n","        y_pred = y_preds[:,i]\n","        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n","        scores.append(score)\n","    mcrmse_score = np.mean(scores)\n","    return mcrmse_score, scores\n","\n","\n","def get_score(y_trues, y_preds):\n","    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n","    return mcrmse_score, scores"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1664732928125,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"},"user_tz":-330},"id":"jQ_1vncGDtCU"},"outputs":[],"source":["class Collate:\n","    def __init__(self, tokenizer, max_length = CFG.specific_max_len):\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        \n","    def __call__(self, batch):\n","        \n","        batch_len = max([len(sample[\"ids\"]) for sample in batch])\n","        \n","        output = dict()\n","        output[\"ids\"] = [sample[\"ids\"] for sample in batch]\n","        output[\"mask\"] = [sample[\"mask\"] for sample in batch]\n","        output[\"targets\"] = [sample[\"targets\"] for sample in batch]\n","        \n","        if self.tokenizer.padding_side == \"right\":\n","            output[\"ids\"] = [s + [self.tokenizer.pad_token_id] * (batch_len - len(s)) for s in output[\"ids\"]]\n","            output[\"mask\"] = [s + [0] * (batch_len - len(s)) for s in output[\"mask\"]]\n","        else:\n","            output[\"ids\"] = [[self.tokenizer.pad_token_id] * (batch_len - len(s)) + s for s in output[\"ids\"]]\n","            output[\"mask\"] = [[0] * (batch_len - len(s)) + s for s in output[\"mask\"]]\n","            \n","            \n","        output[\"ids\"] = torch.tensor(output[\"ids\"], dtype = torch.long)\n","        output[\"mask\"] = torch.tensor(output[\"mask\"], dtype = torch.long)\n","        output[\"targets\"] = torch.tensor(output[\"targets\"], dtype = torch.float32)\n","        \n","        return output"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1664732928125,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"},"user_tz":-330},"id":"JghKVWQIDtCU"},"outputs":[],"source":["class Dataset:\n","    def __init__(self, texts, targets, tokenizer, is_train = True):\n","        self.texts = texts\n","        self.targets = targets\n","        self.tokenizer = tokenizer\n","        self.is_train = is_train\n","        \n","    def __len__(self):\n","        return len(self.texts)\n","    \n","    def __getitem__(self, idx):\n","        \n","        text = self.texts[idx]\n","        targets = self.targets[idx]\n","        \n","        \n","        if CFG.specific_max_len is not None:\n","            encoding = self.tokenizer(text, add_special_tokens = True, max_length = CFG.specific_max_len, padding = False, truncation = 'longest_first')\n","        else:\n","            encoding = self.tokenizer(text, add_special_tokens = True)\n","        \n","        sample = dict()\n","\n","        if CFG.token_dropout and self.is_train:\n","            print(\"Running token dropout\")\n","            idxs = np.random.choice(np.arange(1, len(encoding[\"input_ids\"]) - 1), size = int(CFG.token_dropout_prob * len(encoding[\"input_ids\"])), replace = False)\n","            ids = np.array(encoding[\"input_ids\"])\n","            ids[idxs] = self.tokenizer.mask_token_id\n","            encoding[\"input_ids\"] = ids.tolist()\n","          \n","        sample[\"ids\"] = [50256] * CFG.n_tokens + encoding[\"input_ids\"]\n","        sample[\"mask\"] = [1] * CFG.n_tokens + encoding[\"attention_mask\"]  \n","        sample[\"targets\"] = targets\n","        \n","        return sample"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1664732928125,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"},"user_tz":-330},"id":"QOtpvTlNDtCV"},"outputs":[],"source":["class RMSELoss(nn.Module):\n","    def __init__(self, reduction='mean', eps=1e-9):\n","        super().__init__()\n","        self.mse = nn.MSELoss(reduction='none')\n","        self.reduction = reduction\n","        self.eps = eps\n","\n","    def forward(self, y_pred, y_true):\n","        loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n","        if self.reduction == 'none':\n","            loss = loss\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","        elif self.reduction == 'mean':\n","            loss = loss.mean()\n","        return loss\n","\n","class SmoothRMSELoss(nn.Module):\n","    def __init__(self, reduction='mean', eps=1e-9):\n","        super().__init__()\n","        self.mse = nn.SmoothL1Loss(reduction='none')\n","        self.reduction = reduction\n","        self.eps = eps\n","\n","    def forward(self, y_pred, y_true):\n","        loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n","        if self.reduction == 'none':\n","            loss = loss\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","        elif self.reduction == 'mean':\n","            loss = loss.mean()\n","        return loss\n","\n","class SmoothRMSEComp(nn.Module):\n","    def __init__(self, reduction='mean', eps=1e-9):\n","        super().__init__()\n","        self.mse = nn.SmoothL1Loss(reduction='none')\n","        self.reduction = reduction\n","        self.eps = eps\n","\n","    def forward(self, y_pred, y_true):\n","        loss = torch.sqrt(self.mse(y_pred, y_true).mean(dim = 0)).mean(dim = 0)\n","        return loss  \n","\n","\n","class MeanPooling(nn.Module):\n","    def __init__(self):\n","        super(MeanPooling, self).__init__()\n","        \n","    def forward(self, last_hidden_state, attention_mask):\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","        sum_mask = input_mask_expanded.sum(1)\n","        sum_mask = torch.clamp(sum_mask, min=1e-9)\n","        mean_embeddings = sum_embeddings / sum_mask\n","        return mean_embeddings\n","\n","class WeightedLayerPooling(nn.Module):\n","    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n","        super(WeightedLayerPooling, self).__init__()\n","        self.layer_start = layer_start\n","        self.num_hidden_layers = num_hidden_layers\n","        self.layer_weights = layer_weights if layer_weights is not None \\\n","            else nn.Parameter(\n","                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n","            )\n","\n","    def forward(self, features):\n","\n","        all_layer_embedding = torch.stack(features)\n","        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n","\n","        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n","        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n","\n","        return weighted_average\n","\n","\n","class AttentionHead(nn.Module):\n","    def __init__(self, in_features, hidden_dim):\n","        super().__init__()\n","        self.in_features = in_features\n","        self.middle_features = hidden_dim\n","        self.W = nn.Linear(in_features, hidden_dim)\n","        self.V = nn.Linear(hidden_dim, 1)\n","        self.out_features = hidden_dim\n","\n","    def forward(self, features):\n","        att = torch.tanh(self.W(features))\n","        score = self.V(att)\n","        attention_weights = torch.softmax(score, dim=1)\n","        context_vector = attention_weights * features\n","        context_vector = torch.sum(context_vector, dim=1)\n","\n","        return context_vector"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1664732928125,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"},"user_tz":-330},"id":"BS58rpxCbEKA"},"outputs":[],"source":["#AWP\n","class AWP:\n","    def __init__(\n","        self,\n","        model,\n","        optimizer,\n","        adv_param=\"weight\",\n","        adv_lr=1,\n","        adv_eps=0.2,\n","        start_epoch=0,\n","        adv_step=1,\n","        scaler=None\n","    ):\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.adv_param = adv_param\n","        self.adv_lr = adv_lr\n","        self.adv_eps = adv_eps\n","        self.start_epoch = start_epoch\n","        self.adv_step = adv_step\n","        self.backup = {}\n","        self.backup_eps = {}\n","        self.scaler = scaler\n","\n","    def attack_backward(self, x, y, attention_mask,epoch):\n","        if (self.adv_lr == 0) or (epoch < self.start_epoch):\n","            return None\n","\n","        self._save() \n","        for i in range(self.adv_step):\n","            self._attack_step() \n","            with torch.cuda.amp.autocast():\n","                adv_loss, tr_logits = self.model(ids=x, mask=attention_mask, targets=y)\n","                adv_loss = adv_loss.mean()\n","            self.optimizer.zero_grad()\n","            self.scaler.scale(adv_loss).backward()\n","            \n","        self._restore()\n","\n","    def _attack_step(self):\n","        e = 1e-6\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad and param.grad is not None and self.adv_param in name:\n","                norm1 = torch.norm(param.grad)\n","                norm2 = torch.norm(param.data.detach())\n","                if norm1 != 0 and not torch.isnan(norm1):\n","                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n","                    param.data.add_(r_at)\n","                    param.data = torch.min(\n","                        torch.max(param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n","                    )\n","                # param.data.clamp_(*self.backup_eps[name])\n","\n","    def _save(self):\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad and param.grad is not None and self.adv_param in name:\n","                if name not in self.backup:\n","                    self.backup[name] = param.data.clone()\n","                    grad_eps = self.adv_eps * param.abs().detach()\n","                    self.backup_eps[name] = (\n","                        self.backup[name] - grad_eps,\n","                        self.backup[name] + grad_eps,\n","                    )\n","\n","    def _restore(self,):\n","        for name, param in self.model.named_parameters():\n","            if name in self.backup:\n","                param.data = self.backup[name]\n","        self.backup = {}\n","        self.backup_eps = {}"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1664732928125,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"},"user_tz":-330},"id":"O5F8bmMAOGpe"},"outputs":[],"source":["#Getting the prompt tuning soft embeddings\n","class SoftEmbedding(nn.Module):\n","    def __init__(self, \n","                wte: nn.Embedding,\n","                n_tokens: int = 10, \n","                random_range: float = 0.5,\n","                initialize_from_vocab: bool = True):\n","        \"\"\"appends learned embedding to \n","        Args:\n","            wte (nn.Embedding): original transformer word embedding\n","            n_tokens (int, optional): number of tokens for task. Defaults to 10.\n","            random_range (float, optional): range to init embedding (if not initialize from vocab). Defaults to 0.5.\n","            initialize_from_vocab (bool, optional): initalizes from default vocab. Defaults to True.\n","        \"\"\"\n","        super(SoftEmbedding, self).__init__()\n","        self.wte = wte\n","        self.n_tokens = n_tokens\n","        self.learned_embedding = nn.parameter.Parameter(self.initialize_embedding(wte,\n","                                                                               n_tokens, \n","                                                                               random_range, \n","                                                                               initialize_from_vocab))\n","            \n","    def initialize_embedding(self, \n","                             wte: nn.Embedding,\n","                             n_tokens: int = 10, \n","                             random_range: float = 0.5, \n","                             initialize_from_vocab: bool = True):\n","        \"\"\"initializes learned embedding\n","        Args:\n","            same as __init__\n","        Returns:\n","            torch.float: initialized using original schemes\n","        \"\"\"\n","        if initialize_from_vocab:\n","            return self.wte.weight[:n_tokens].clone().detach()\n","        return torch.FloatTensor(n_tokens, wte.weight.size(1)).uniform_(-random_range, random_range)\n","            \n","    def forward(self, tokens):\n","        \"\"\"run forward pass\n","        Args:\n","            tokens (torch.long): input tokens before encoding\n","        Returns:\n","            torch.float: encoding of text concatenated with learned task specifc embedding\n","        \"\"\"\n","        input_embedding = self.wte(tokens[:, self.n_tokens:])\n","        learned_embedding = self.learned_embedding.repeat(input_embedding.size(0), 1, 1)\n","        return torch.cat([learned_embedding, input_embedding], 1)"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1664732928125,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"},"user_tz":-330},"id":"8lkexNsJDtCV"},"outputs":[],"source":["class Model(nn.Module):\n","    def __init__(self, model_name):\n","        super(Model, self).__init__()\n","        \n","        self.model_name = model_name\n","\n","        hidden_dropout_prob: float = CFG.hidden_dropout_prob\n","        layer_norm_eps: float = 1e-7\n","\n","        config = AutoConfig.from_pretrained(model_name)\n","\n","        config.update(\n","            {\n","                \"output_hidden_states\": True,\n","                \"hidden_dropout_prob\": hidden_dropout_prob,\n","                \"attention_probs_dropout_prob\" : hidden_dropout_prob,\n","                \"layer_norm_eps\": layer_norm_eps,\n","                \"add_pooling_layer\": False,\n","                \"num_labels\": 6,\n","            }\n","        )\n","        \n","        self.config = config\n","        \n","        #Using Prompt Tuning\n","        self.transformer = AutoModel.from_pretrained(model_name, config=config)\n","        s_wte = SoftEmbedding(self.transformer.get_input_embeddings(), \n","                      n_tokens=CFG.n_tokens, \n","                      initialize_from_vocab=True)\n","        self.transformer.set_input_embeddings(s_wte)\n","        \n","        if CFG.gradient_checkpointing_enable:\n","            self.transformer.gradient_checkpointing_enable()\n","        \n","        self.layer_nums = list(range(self.config.num_hidden_layers - 6, self.config.num_hidden_layers))\n","            \n","        self.freeze()\n","        \n","        self.output = nn.Linear(config.hidden_size, 6)\n","        self.loss = nn.SmoothL1Loss(reduction = \"mean\")\n","\n","        if CFG.reinit_weights:\n","            self.init_weights_(CFG.reinit_layers)\n","\n","    def init_weights_(self, reinit_layers):\n","        for layer in self.transformer.encoder.layer[-reinit_layers:]:\n","            for module in layer.modules():\n","                if isinstance(module, nn.Linear):\n","                    module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","                    if module.bias is not None:\n","                        module.bias.data.zero_()\n","                elif isinstance(module, nn.Embedding):\n","                    module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","                    if module.padding_idx is not None:\n","                        module.weight.data[module.padding_idx].zero_()\n","                elif isinstance(module, nn.LayerNorm):\n","                    module.bias.data.zero_()\n","                    module.weight.data.fill_(1.0)\n","\n","    def get_grouped_llrd_optimizer_scheduler(self, num_train_steps):\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","        top_params = [('learned_embedding', self.transformer.embeddings.word_embeddings.learned_embedding)] #+ list(self.output.named_parameters())\n","      # initialize lr for task specific layer\n","        optimizer_grouped_parameters = [\n","              {\n","                  \"params\": [p for n, p in top_params if not any(nd in n for nd in no_decay)],\n","                  \"weight_decay\": 0.001,\n","                  \"lr\": CFG.lr,\n","              },\n","              {\n","                  \"params\": [p for n, p in top_params if any(nd in n for nd in no_decay)],\n","                  \"weight_decay\": 0.0,\n","                  \"lr\": CFG.lr,\n","              },\n","          ]\n","      # initialize lrs for every layer\n","        num_layers = self.config.num_hidden_layers\n","        layers = [self.transformer.embeddings] + list(self.transformer.encoder.layer)\n","        layers.reverse()\n","        layers = layers[:6]\n","        lr = 2e-5\n","        for layer in layers:\n","            lr *= CFG.llrd\n","            optimizer_grouped_parameters += [\n","              {\n","                  \"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n","                  \"weight_decay\": 0.001,\n","                  \"lr\": lr,\n","              },\n","              {\n","                  \"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n","                  \"weight_decay\": 0.0,\n","                  \"lr\": lr,\n","              },\n","          ]\n","        opt = torch.optim.AdamW(optimizer_grouped_parameters)\n","        sch = get_polynomial_decay_schedule_with_warmup(\n","          opt,\n","          num_warmup_steps=int(num_train_steps * CFG.warmup_ratio),\n","          num_training_steps=num_train_steps,\n","          last_epoch=-1,\n","          )\n","        return opt, sch\n","    \n","    \n","    def freeze(self):\n","        for n,param in self.transformer.named_parameters():\n","            if \"learned_embedding\" not in n and not any([str(ln) in n for ln in self.layer_nums]):\n","                param.requires_grad = False\n","        \n","    \n","    def get_optimizer_scheduler(self, num_train_steps):\n","        param_optimizer = [('learned_embedding', self.transformer.embeddings.word_embeddings.learned_embedding)] + list(self.output.named_parameters())\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","        optimizer_parameters = [\n","            {\n","                \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.001,\n","            },\n","            {\n","                \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","            },\n","        ]\n","\n","        transformer_parms = [(n,p) for n,p in self.transformer.named_parameters() if any([str(ln) in n for ln in self.layer_nums])] #+ [(n,p) for n,p in self.named_parameters() if n in [\"transformer.encoder.LayerNorm.weight\",\"transformer.encoder.LayerNorm.bias\"]]\n","        optimizer_parameters += [\n","            {\n","                \"params\": [p for n, p in transformer_parms if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.001,\n","                \"lr\" : 2e-5\n","            },\n","            {\n","                \"params\": [p for n, p in transformer_parms if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","                \"lr\" : 2e-5\n","            },\n","        ]\n","        opt = torch.optim.AdamW(optimizer_parameters, lr=CFG.lr)\n","\n","        sch = get_polynomial_decay_schedule_with_warmup(\n","          opt,\n","          num_warmup_steps=int(num_train_steps * CFG.warmup_ratio),\n","          num_training_steps=num_train_steps,\n","          last_epoch=-1,\n","          )\n","        return opt, sch\n","\n","    def forward(self, ids, mask, token_type_ids=None, targets=None):\n","        if token_type_ids is not None:\n","            transformer_out = self.transformer( ids, mask, token_type_ids )\n","        else:\n","            transformer_out = self.transformer(ids, mask)\n","        sequence_output = transformer_out.last_hidden_state[:,0,:]\n","        logits = self.output(sequence_output)\n","        loss = self.loss(logits, targets)\n","\n","        return loss, logits"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1664732928125,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"},"user_tz":-330},"id":"PojCbT3GDtCW"},"outputs":[],"source":["def train(epoch, model, train_loader, valid_loader, optimizer, scheduler, device, awp, scaler, best_loss, fold):\n","    model.train()\n","    losses = AverageMeter()\n","    start = end = time.time()\n","    val_steps = len(train_loader) // 1\n","    for step, x in enumerate(train_loader):\n","        for k,v in x.items():\n","            x[k] = v.to(device)\n","        \n","        with autocast():    \n","            loss, logits = model(**x)\n","            \n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        \n","        losses.update(loss.item() * CFG.gradient_accumulation_steps, CFG.batch_size)\n","        scaler.scale(loss).backward()\n","        if CFG.use_awp:\n","            awp.attack_backward(x[\"ids\"],x[\"targets\"],x[\"mask\"],epoch)\n","        \n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.unscale_(optimizer)\n","            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","            if scheduler is not None:\n","                scheduler.step()\n","        end = time.time()\n","        \n","        if ((step + 1) % CFG.print_freq == 0) or (step == (len(train_loader)-1)):\n","            print('Epoch: [{0}][{1}/{2}] '\n","                  'Elapsed {remain:s} '\n","                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                  'Grad: {grad_norm:.4f}  '\n","                  'LR: {lr:.8f}  '\n","                  .format(epoch+1, step + 1, len(train_loader), \n","                          remain=timeSince(start, float(step+1)/len(train_loader)),\n","                          loss=losses,\n","                          grad_norm= grad_norm,\n","                          lr=scheduler.get_lr()[0] if scheduler is not None else CFG.lr))\n","        \n","        if ((step + 1) % val_steps == 0) or ((step + 1) == len(train_loader)):\n","            print(\"\\nVALID LOOP\\n\")\n","            valid_loss = valid(epoch, model, valid_loader, device)\n","            print(f\"\\nThe valid loss for the current epoch is {valid_loss}\\n\")\n","            torch.cuda.empty_cache()\n","            gc.collect()\n","            if valid_loss < best_loss:\n","                best_loss = valid_loss\n","                if CFG.save_dir is not None:\n","                    if not os.path.exists(CFG.save_dir):\n","                        os.mkdir(CFG.save_dir)\n","                    save_path = os.path.join(CFG.save_dir, f\"{CFG.save_model_name}_fold_{fold}.pth\")\n","                else:\n","                    save_path = f\"{CFG.save_model_name}_fold_{fold}.pth\"\n","                torch.save(model.state_dict(), save_path)\n","        model.train()\n","\n","def valid(epoch, model, valid_loader, device):\n","    model.eval()\n","    all_targets = []\n","    all_outputs = []\n","    losses = AverageMeter()\n","    with torch.no_grad():\n","        for step, x in enumerate(valid_loader):\n","\n","            for k, v in x.items():\n","                x[k] = v.to(device)\n","          \n","            loss, logits = model(**x)\n","\n","            losses.update(loss.item(), CFG.batch_size)\n","            targets = x[\"targets\"].cpu().numpy()\n","            outputs = logits.cpu().numpy()\n","\n","            all_targets.append(targets)\n","            all_outputs.append(np.clip(outputs, 1.0, 5.0))\n","\n","            if ((step + 1) % CFG.print_freq == 0) or (step == (len(valid_loader)-1)):\n","                print('Epoch: [{0}][{1}/{2}] '\n","                      'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                      .format(epoch+1, step + 1, len(valid_loader), loss=losses))\n","        \n","    \n","    all_targets = np.vstack(all_targets)\n","    all_outputs = np.vstack(all_outputs)\n","    loss = get_score(all_targets, all_outputs)[0]\n","    \n","    del all_targets, all_outputs;\n","    return loss"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1664732928125,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"},"user_tz":-330},"id":"zovUkUouDtCW"},"outputs":[],"source":["def main(fold):\n","    torch.cuda.empty_cache()\n","    df = pd.read_csv(CFG.train_file)\n","    \n","    train_df = df.loc[df.kfold != fold]\n","    valid_df = df.loc[df.kfold == fold]\n","    \n","    train_texts = train_df[\"full_text\"].apply(resolve_encodings_and_normalize).to_list()\n","    valid_texts = valid_df[\"full_text\"].apply(resolve_encodings_and_normalize).to_list()\n","    \n","    train_targets = train_df[CFG.target_columns].values.tolist()\n","    \n","    valid_targets = valid_df[CFG.target_columns].values.tolist()\n","    \n","    train_ds = Dataset(train_texts, train_targets, CFG.tokenizer)\n","    valid_ds = Dataset(valid_texts, valid_targets, CFG.tokenizer, is_train = False)\n","\n","    collate_fn = Collate(CFG.tokenizer)\n","    train_loader = torch.utils.data.DataLoader(train_ds, batch_size = CFG.batch_size, shuffle = True, collate_fn = collate_fn, num_workers = CFG.num_workers)\n","    valid_loader = torch.utils.data.DataLoader(valid_ds, batch_size = CFG.batch_size, shuffle = False, collate_fn = collate_fn, num_workers = CFG.num_workers)\n","    \n","    model = Model(CFG.model_name)\n","    num_train_steps = int(len(train_ds) / CFG.batch_size / CFG.gradient_accumulation_steps * CFG.epochs)\n","    optimizer, scheduler = model.get_optimizer_scheduler(num_train_steps)\n","    \n","    model = model.to(CFG.device)\n","    best_loss = np.inf\n","    scaler = GradScaler()\n","    if CFG.use_awp:\n","        print('ENABLE AWP')\n","        awp = AWP(model,\n","          optimizer,\n","          adv_lr=CFG.adv_lr,\n","          adv_eps=CFG.adv_eps,\n","          start_epoch=3,\n","          scaler=scaler)\n","    else:\n","        awp = None\n","    for epoch in range(CFG.epochs):\n","        print(\"\\nTRAIN LOOP\\n\")\n","        train(epoch, model, train_loader, valid_loader, optimizer, scheduler, CFG.device, awp, scaler, best_loss, fold) # None for the scaler parameter\n","        # print(\"\\nVALID LOOP\\n\")\n","        # valid_loss = valid(epoch, model, valid_loader, CFG.device)\n","        \n","        # print(f\"\\nThe valid loss for the current epoch is {valid_loss}\\n\")\n","        # torch.cuda.empty_cache()\n","        # gc.collect()\n","        # if valid_loss < best_loss:\n","        #     best_loss = valid_loss\n","        #     if CFG.save_dir is not None:\n","        #       if not os.path.exists(CFG.save_dir):\n","        #         os.mkdir(CFG.save_dir)\n","        #       save_path = os.path.join(CFG.save_dir, f\"{CFG.save_model_name}_fold_{fold}.pth\")\n","        #     else:\n","        #       save_path = f\"{CFG.save_model_name}_fold_{fold}.pth\"\n","        #     torch.save(model.state_dict(), save_path)\n","            \n","    del model, optimizer, scheduler, train_loader, valid_loader, train_df, valid_df;\n","    gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["dbed710744414288bcedf9e3d37e70b8","98351fd5734042029e54b602eb23f727","95978307ac3d490f8a7de4bd2555ebd6","3840ca9bd28a43638ae651e1a913b0ba","51f46d0da94b4f8fb3ad1c289c235273","a94b668a5c924a6381d341561aac8029","d83c610dce914e93b1c847b64d2e549e","fae91debcdb94179aa262bac23cf1867","61c6bf68330b4b8db72a2007cb54a20e","9b056308ca9548dda7274f0a5a3f85f2","ff4ec6c753114f8c82df2cabebfae19b"]},"id":"ndUmqkakDtCW","outputId":"6596d893-8acc-4cc1-a8a6-22ec4fcfa480"},"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------------------------------------------\n","\n","RUNNING FOLD 0\n","\n","----------------------------------------------------------------------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/874M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbed710744414288bcedf9e3d37e70b8"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 2m 2s (remain 17m 54s) Loss: 0.2197(0.5643) Grad: 6.4194  LR: 0.00096587  \n","Epoch: [1][40/196] Elapsed 4m 5s (remain 15m 59s) Loss: 0.0952(0.3805) Grad: 3.4756  LR: 0.00093175  \n","Epoch: [1][60/196] Elapsed 6m 7s (remain 13m 53s) Loss: 0.0882(0.2973) Grad: 1.7023  LR: 0.00089762  \n","Epoch: [1][80/196] Elapsed 8m 15s (remain 11m 57s) Loss: 0.1489(0.2547) Grad: 2.5205  LR: 0.00086349  \n","Epoch: [1][100/196] Elapsed 10m 23s (remain 9m 58s) Loss: 0.1246(0.2273) Grad: 1.8776  LR: 0.00082937  \n","Epoch: [1][120/196] Elapsed 12m 28s (remain 7m 53s) Loss: 0.0842(0.2093) Grad: 3.1683  LR: 0.00079524  \n","Epoch: [1][140/196] Elapsed 14m 33s (remain 5m 49s) Loss: 0.1000(0.1963) Grad: 2.4861  LR: 0.00076112  \n","Epoch: [1][160/196] Elapsed 16m 36s (remain 3m 44s) Loss: 0.0910(0.1867) Grad: 1.8027  LR: 0.00072699  \n","Epoch: [1][180/196] Elapsed 18m 43s (remain 1m 39s) Loss: 0.1283(0.1770) Grad: 1.9926  LR: 0.00069286  \n","Epoch: [1][196/196] Elapsed 20m 19s (remain 0m 0s) Loss: 0.1411(0.1727) Grad: 4.9098  LR: 0.00066556  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49] Loss: 0.1086(0.1146) \n","Epoch: [1][40/49] Loss: 0.1472(0.1177) \n","Epoch: [1][49/49] Loss: 0.1202(0.1169) \n","\n","The valid loss for the current epoch is 0.484760046005249\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 2m 6s (remain 18m 31s) Loss: 0.1037(0.1122) Grad: 4.6130  LR: 0.00063144  \n","Epoch: [2][40/196] Elapsed 4m 9s (remain 16m 11s) Loss: 0.1072(0.1128) Grad: 1.8200  LR: 0.00059731  \n","Epoch: [2][60/196] Elapsed 6m 15s (remain 14m 10s) Loss: 0.0779(0.1103) Grad: 2.4963  LR: 0.00056318  \n","Epoch: [2][80/196] Elapsed 8m 20s (remain 12m 5s) Loss: 0.0860(0.1110) Grad: 2.4179  LR: 0.00052906  \n","Epoch: [2][100/196] Elapsed 10m 23s (remain 9m 58s) Loss: 0.1224(0.1114) Grad: 3.7411  LR: 0.00049493  \n","Epoch: [2][120/196] Elapsed 12m 29s (remain 7m 54s) Loss: 0.0761(0.1098) Grad: 1.6075  LR: 0.00046080  \n","Epoch: [2][140/196] Elapsed 14m 32s (remain 5m 48s) Loss: 0.1094(0.1087) Grad: 2.4384  LR: 0.00042668  \n","Epoch: [2][160/196] Elapsed 16m 35s (remain 3m 43s) Loss: 0.0759(0.1072) Grad: 1.5732  LR: 0.00039255  \n","Epoch: [2][180/196] Elapsed 18m 34s (remain 1m 39s) Loss: 0.0907(0.1066) Grad: 2.7429  LR: 0.00035843  \n","Epoch: [2][196/196] Elapsed 20m 13s (remain 0m 0s) Loss: 0.0927(0.1061) Grad: 2.4784  LR: 0.00033112  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49] Loss: 0.0929(0.1013) \n","Epoch: [2][40/49] Loss: 0.1331(0.1028) \n","Epoch: [2][49/49] Loss: 0.1105(0.1023) \n","\n","The valid loss for the current epoch is 0.45269355177879333\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 2m 4s (remain 18m 14s) Loss: 0.1296(0.0995) Grad: 1.1694  LR: 0.00029700  \n","Epoch: [3][40/196] Elapsed 4m 7s (remain 16m 5s) Loss: 0.0966(0.0978) Grad: 3.8834  LR: 0.00026287  \n","Epoch: [3][60/196] Elapsed 6m 13s (remain 14m 6s) Loss: 0.0777(0.0970) Grad: 1.9955  LR: 0.00022875  \n","Epoch: [3][80/196] Elapsed 8m 19s (remain 12m 4s) Loss: 0.1147(0.0974) Grad: 3.0665  LR: 0.00019462  \n","Epoch: [3][100/196] Elapsed 10m 21s (remain 9m 56s) Loss: 0.1187(0.0962) Grad: 2.0556  LR: 0.00016049  \n","Epoch: [3][120/196] Elapsed 12m 23s (remain 7m 50s) Loss: 0.0915(0.0960) Grad: 2.0187  LR: 0.00012637  \n","Epoch: [3][140/196] Elapsed 14m 25s (remain 5m 46s) Loss: 0.0934(0.0954) Grad: 2.5462  LR: 0.00009224  \n","Epoch: [3][160/196] Elapsed 16m 31s (remain 3m 42s) Loss: 0.1077(0.0955) Grad: 2.9863  LR: 0.00005811  \n","Epoch: [3][180/196] Elapsed 18m 34s (remain 1m 39s) Loss: 0.1507(0.0959) Grad: 1.4952  LR: 0.00002399  \n","Epoch: [3][196/196] Elapsed 20m 11s (remain 0m 0s) Loss: 0.0705(0.0950) Grad: 2.0127  LR: 0.00000010  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49] Loss: 0.0947(0.0978) \n","Epoch: [3][40/49] Loss: 0.1375(0.0999) \n","Epoch: [3][49/49] Loss: 0.1112(0.0990) \n","\n","The valid loss for the current epoch is 0.4451328217983246\n","\n","----------------------------------------------------------------------------------------------------\n","\n","RUNNING FOLD 1\n","\n","----------------------------------------------------------------------------------------------------\n","\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 2m 9s (remain 18m 57s) Loss: 0.2093(0.5888) Grad: 4.2994  LR: 0.00096587  \n","Epoch: [1][40/196] Elapsed 4m 11s (remain 16m 20s) Loss: 0.0973(0.3751) Grad: 2.6112  LR: 0.00093175  \n","Epoch: [1][60/196] Elapsed 6m 15s (remain 14m 10s) Loss: 0.1104(0.2901) Grad: 4.9931  LR: 0.00089762  \n","Epoch: [1][80/196] Elapsed 8m 15s (remain 11m 59s) Loss: 0.1204(0.2489) Grad: 1.9873  LR: 0.00086349  \n","Epoch: [1][100/196] Elapsed 10m 20s (remain 9m 55s) Loss: 0.1014(0.2252) Grad: 2.1737  LR: 0.00082937  \n","Epoch: [1][120/196] Elapsed 12m 26s (remain 7m 52s) Loss: 0.0776(0.2068) Grad: 2.9515  LR: 0.00079524  \n","Epoch: [1][140/196] Elapsed 14m 31s (remain 5m 48s) Loss: 0.1314(0.1944) Grad: 1.0541  LR: 0.00076112  \n","Epoch: [1][160/196] Elapsed 16m 33s (remain 3m 43s) Loss: 0.1283(0.1836) Grad: 5.2119  LR: 0.00072699  \n","Epoch: [1][180/196] Elapsed 18m 35s (remain 1m 39s) Loss: 0.0951(0.1759) Grad: 1.7619  LR: 0.00069286  \n","Epoch: [1][196/196] Elapsed 20m 13s (remain 0m 0s) Loss: 0.1241(0.1716) Grad: 4.0494  LR: 0.00066556  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49] Loss: 0.1076(0.1093) \n","Epoch: [1][40/49] Loss: 0.0815(0.1084) \n","Epoch: [1][49/49] Loss: 0.1067(0.1089) \n","\n","The valid loss for the current epoch is 0.46768856048583984\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 2m 8s (remain 18m 52s) Loss: 0.1043(0.1065) Grad: 2.3463  LR: 0.00063144  \n","Epoch: [2][40/196] Elapsed 4m 11s (remain 16m 22s) Loss: 0.0750(0.1067) Grad: 1.1636  LR: 0.00059731  \n","Epoch: [2][60/196] Elapsed 6m 16s (remain 14m 13s) Loss: 0.1136(0.1061) Grad: 1.1961  LR: 0.00056318  \n","Epoch: [2][80/196] Elapsed 8m 20s (remain 12m 6s) Loss: 0.0862(0.1039) Grad: 2.2703  LR: 0.00052906  \n","Epoch: [2][100/196] Elapsed 10m 22s (remain 9m 57s) Loss: 0.0978(0.1033) Grad: 1.6030  LR: 0.00049493  \n","Epoch: [2][120/196] Elapsed 12m 25s (remain 7m 52s) Loss: 0.1060(0.1049) Grad: 1.9532  LR: 0.00046080  \n","Epoch: [2][140/196] Elapsed 14m 28s (remain 5m 47s) Loss: 0.0984(0.1045) Grad: 2.9239  LR: 0.00042668  \n","Epoch: [2][160/196] Elapsed 16m 33s (remain 3m 43s) Loss: 0.0878(0.1041) Grad: 1.5874  LR: 0.00039255  \n","Epoch: [2][180/196] Elapsed 18m 39s (remain 1m 39s) Loss: 0.1091(0.1049) Grad: 2.4831  LR: 0.00035843  \n","Epoch: [2][196/196] Elapsed 20m 14s (remain 0m 0s) Loss: 0.1026(0.1051) Grad: 2.6338  LR: 0.00033112  \n","\n","VALID LOOP\n","\n","Epoch: [2][20/49] Loss: 0.1183(0.1011) \n","Epoch: [2][40/49] Loss: 0.0852(0.1029) \n","Epoch: [2][49/49] Loss: 0.1038(0.1039) \n","\n","The valid loss for the current epoch is 0.45684614777565\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [3][20/196] Elapsed 2m 7s (remain 18m 44s) Loss: 0.0884(0.0928) Grad: 2.0579  LR: 0.00029700  \n","Epoch: [3][40/196] Elapsed 4m 11s (remain 16m 20s) Loss: 0.0958(0.0937) Grad: 4.0322  LR: 0.00026287  \n","Epoch: [3][60/196] Elapsed 6m 19s (remain 14m 19s) Loss: 0.0806(0.0965) Grad: 2.3147  LR: 0.00022875  \n","Epoch: [3][80/196] Elapsed 8m 20s (remain 12m 5s) Loss: 0.1025(0.0961) Grad: 1.5882  LR: 0.00019462  \n","Epoch: [3][100/196] Elapsed 10m 25s (remain 10m 0s) Loss: 0.1023(0.0946) Grad: 2.5889  LR: 0.00016049  \n","Epoch: [3][120/196] Elapsed 12m 26s (remain 7m 52s) Loss: 0.0842(0.0949) Grad: 1.1724  LR: 0.00012637  \n","Epoch: [3][140/196] Elapsed 14m 31s (remain 5m 48s) Loss: 0.0967(0.0959) Grad: 0.9100  LR: 0.00009224  \n","Epoch: [3][160/196] Elapsed 16m 39s (remain 3m 44s) Loss: 0.0762(0.0947) Grad: 2.5126  LR: 0.00005811  \n","Epoch: [3][180/196] Elapsed 18m 47s (remain 1m 40s) Loss: 0.0930(0.0945) Grad: 0.9555  LR: 0.00002399  \n","Epoch: [3][196/196] Elapsed 20m 22s (remain 0m 0s) Loss: 0.0778(0.0945) Grad: 2.3293  LR: 0.00000010  \n","\n","VALID LOOP\n","\n","Epoch: [3][20/49] Loss: 0.1081(0.0984) \n","Epoch: [3][40/49] Loss: 0.0742(0.0994) \n","Epoch: [3][49/49] Loss: 0.1045(0.1007) \n","\n","The valid loss for the current epoch is 0.44963201880455017\n","\n","----------------------------------------------------------------------------------------------------\n","\n","RUNNING FOLD 2\n","\n","----------------------------------------------------------------------------------------------------\n","\n","TRAIN LOOP\n","\n","Epoch: [1][20/196] Elapsed 2m 6s (remain 18m 30s) Loss: 0.2394(0.4908) Grad: 6.0671  LR: 0.00096587  \n","Epoch: [1][40/196] Elapsed 4m 8s (remain 16m 11s) Loss: 0.1194(0.3198) Grad: 2.5907  LR: 0.00093175  \n","Epoch: [1][60/196] Elapsed 6m 14s (remain 14m 8s) Loss: 0.1335(0.2562) Grad: 2.8446  LR: 0.00089762  \n","Epoch: [1][80/196] Elapsed 8m 19s (remain 12m 4s) Loss: 0.0931(0.2229) Grad: 3.7739  LR: 0.00086349  \n","Epoch: [1][100/196] Elapsed 10m 23s (remain 9m 58s) Loss: 0.1475(0.2001) Grad: 2.9720  LR: 0.00082937  \n","Epoch: [1][120/196] Elapsed 12m 27s (remain 7m 53s) Loss: 0.1184(0.1869) Grad: 2.2873  LR: 0.00079524  \n","Epoch: [1][140/196] Elapsed 14m 33s (remain 5m 49s) Loss: 0.1553(0.1770) Grad: 2.7048  LR: 0.00076112  \n","Epoch: [1][160/196] Elapsed 16m 39s (remain 3m 44s) Loss: 0.1086(0.1692) Grad: 1.0850  LR: 0.00072699  \n","Epoch: [1][180/196] Elapsed 18m 39s (remain 1m 39s) Loss: 0.1138(0.1623) Grad: 3.6670  LR: 0.00069286  \n","Epoch: [1][196/196] Elapsed 20m 19s (remain 0m 0s) Loss: 0.1148(0.1585) Grad: 2.9208  LR: 0.00066556  \n","\n","VALID LOOP\n","\n","Epoch: [1][20/49] Loss: 0.1022(0.1150) \n","Epoch: [1][40/49] Loss: 0.0838(0.1125) \n","Epoch: [1][49/49] Loss: 0.0877(0.1141) \n","\n","The valid loss for the current epoch is 0.480231910943985\n","\n","\n","TRAIN LOOP\n","\n","Epoch: [2][20/196] Elapsed 2m 6s (remain 18m 32s) Loss: 0.0550(0.1051) Grad: 1.2846  LR: 0.00063144  \n","Epoch: [2][40/196] Elapsed 4m 9s (remain 16m 13s) Loss: 0.0909(0.1017) Grad: 1.5372  LR: 0.00059731  \n","Epoch: [2][60/196] Elapsed 6m 15s (remain 14m 10s) Loss: 0.0922(0.1035) Grad: 1.7383  LR: 0.00056318  \n","Epoch: [2][80/196] Elapsed 8m 20s (remain 12m 5s) Loss: 0.1104(0.1033) Grad: 1.6363  LR: 0.00052906  \n"]}],"source":["for fold in range(0,5):\n","    print(\"-----\"*20)\n","    print(f\"\\nRUNNING FOLD {fold}\\n\")\n","    print(\"-----\"*20)\n","    main(fold)\n","    gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iBFbGyLcOGph","executionInfo":{"status":"aborted","timestamp":1664570198298,"user_tz":-330,"elapsed":7,"user":{"displayName":"Jash Dalvi","userId":"05653555086574220679"}}},"outputs":[],"source":["!cp -r deberta-v3-large/ /content/drive/MyDrive/Kaggle/FeedbackPrize3/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-q1YivBNTeOa"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"widgets":{"application/vnd.jupyter.widget-state+json":{"734db5eedea642e7ad396eef1e863bc7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3e2cbd85f02546a9b7b1340a79801525","IPY_MODEL_dfc58e738947458ab27b9a3df33486ba","IPY_MODEL_a09b2ea88e724332952ef81f928865f0"],"layout":"IPY_MODEL_95de51e446ca48e089fb2797826d9b49"}},"3e2cbd85f02546a9b7b1340a79801525":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cddf9355c4804c41a956e67d37fa8984","placeholder":"​","style":"IPY_MODEL_399f7aca8c5843a7a75cf9263420c039","value":"Downloading: 100%"}},"dfc58e738947458ab27b9a3df33486ba":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c2b7f42527844ed9b99578b24714359","max":52,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6b6013e2541440ed8834a19e56d58890","value":52}},"a09b2ea88e724332952ef81f928865f0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1b8afc91ad74acda40bed06db3d314c","placeholder":"​","style":"IPY_MODEL_33645c6cd7c6408c8d28793a963e74b5","value":" 52.0/52.0 [00:00&lt;00:00, 1.80kB/s]"}},"95de51e446ca48e089fb2797826d9b49":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cddf9355c4804c41a956e67d37fa8984":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"399f7aca8c5843a7a75cf9263420c039":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7c2b7f42527844ed9b99578b24714359":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b6013e2541440ed8834a19e56d58890":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a1b8afc91ad74acda40bed06db3d314c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33645c6cd7c6408c8d28793a963e74b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"261ba8ea966a4944987daab0caf7b1ac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_814e1b20889c4ba0b39563334a668a22","IPY_MODEL_7699ea2e82304354986e68297b5d8ffa","IPY_MODEL_921547ff600e4096b8f1bc22e010d6ac"],"layout":"IPY_MODEL_458f10ed9a174a3aa08a42ce8ff2f480"}},"814e1b20889c4ba0b39563334a668a22":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfd73246bf4a451f80c85c6286a9bd00","placeholder":"​","style":"IPY_MODEL_ae20fd51fbb442a1b348d65db7d107bf","value":"Downloading: 100%"}},"7699ea2e82304354986e68297b5d8ffa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1a1ca44bb1342748929c69ac05873cd","max":580,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a8c618114abb47b0a9e0d2f44fa77c9f","value":580}},"921547ff600e4096b8f1bc22e010d6ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86a4dba27d92482cb437454933f89995","placeholder":"​","style":"IPY_MODEL_c3ded0898c0a45c6bbd623d480e58daf","value":" 580/580 [00:00&lt;00:00, 20.3kB/s]"}},"458f10ed9a174a3aa08a42ce8ff2f480":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfd73246bf4a451f80c85c6286a9bd00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae20fd51fbb442a1b348d65db7d107bf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c1a1ca44bb1342748929c69ac05873cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8c618114abb47b0a9e0d2f44fa77c9f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"86a4dba27d92482cb437454933f89995":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3ded0898c0a45c6bbd623d480e58daf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5f8a870112604dcd9f4e44a064b913fc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1854af30e1b744d9b8bd0c3019e92ab5","IPY_MODEL_ed979a351d2443c6aba89a7c69bc78e6","IPY_MODEL_db2a0965ebbb4b1694314caa3cc39e0b"],"layout":"IPY_MODEL_bbb7fcd3ffa44292a262193b10046065"}},"1854af30e1b744d9b8bd0c3019e92ab5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e695cd8987eb4406be8a9bde95f2b1fa","placeholder":"​","style":"IPY_MODEL_11f63e25611f47a8953c3cf23086084f","value":"Downloading: 100%"}},"ed979a351d2443c6aba89a7c69bc78e6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3244b6b29eb5484b94ee1f63e3d47a40","max":2464616,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2713e32048df4e0fbd95e4e8016ac71b","value":2464616}},"db2a0965ebbb4b1694314caa3cc39e0b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e62abff7aff04902b3d585e4743746a0","placeholder":"​","style":"IPY_MODEL_18578bc64f9743f8a4624eb82a3d905b","value":" 2.46M/2.46M [00:00&lt;00:00, 33.9MB/s]"}},"bbb7fcd3ffa44292a262193b10046065":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e695cd8987eb4406be8a9bde95f2b1fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11f63e25611f47a8953c3cf23086084f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3244b6b29eb5484b94ee1f63e3d47a40":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2713e32048df4e0fbd95e4e8016ac71b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e62abff7aff04902b3d585e4743746a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18578bc64f9743f8a4624eb82a3d905b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dbed710744414288bcedf9e3d37e70b8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_98351fd5734042029e54b602eb23f727","IPY_MODEL_95978307ac3d490f8a7de4bd2555ebd6","IPY_MODEL_3840ca9bd28a43638ae651e1a913b0ba"],"layout":"IPY_MODEL_51f46d0da94b4f8fb3ad1c289c235273"}},"98351fd5734042029e54b602eb23f727":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a94b668a5c924a6381d341561aac8029","placeholder":"​","style":"IPY_MODEL_d83c610dce914e93b1c847b64d2e549e","value":"Downloading: 100%"}},"95978307ac3d490f8a7de4bd2555ebd6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fae91debcdb94179aa262bac23cf1867","max":873673253,"min":0,"orientation":"horizontal","style":"IPY_MODEL_61c6bf68330b4b8db72a2007cb54a20e","value":873673253}},"3840ca9bd28a43638ae651e1a913b0ba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b056308ca9548dda7274f0a5a3f85f2","placeholder":"​","style":"IPY_MODEL_ff4ec6c753114f8c82df2cabebfae19b","value":" 874M/874M [00:13&lt;00:00, 62.1MB/s]"}},"51f46d0da94b4f8fb3ad1c289c235273":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a94b668a5c924a6381d341561aac8029":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d83c610dce914e93b1c847b64d2e549e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fae91debcdb94179aa262bac23cf1867":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61c6bf68330b4b8db72a2007cb54a20e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9b056308ca9548dda7274f0a5a3f85f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff4ec6c753114f8c82df2cabebfae19b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}